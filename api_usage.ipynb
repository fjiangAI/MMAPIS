{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# fastapi 请求方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 方法1：调用前端\n",
    "\n",
    "\n",
    "具体代码见[app.py](./app.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```bash\n",
    "    $ cd path/to/code_arxiv_summarizer\n",
    "    $ streamlit run app.py\n",
    "```\n",
    "进行可交互访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 方法2：request访问\n",
    "\n",
    "\n",
    "启动后端服务：\n",
    "```bash\n",
    "$ uvicorn backend:app --host 0.0.0.0 --port 8076 --reload\n",
    "```\n",
    "\n",
    "则端口设为8076，访问url(http://host_ip:port)为：\n",
    "```python\n",
    "req_url = \"http://61.241.103.32:8076\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### get_links\n",
    "\n",
    "具体接受格式如下（其中未给的参数会自动用default补全，因此实际什么都不输入也会有返回值）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "from fastapi import FastAPI,Body\n",
    "import json\n",
    "from typing import List,Union,Literal,Dict\n",
    "from pydantic import BaseModel,Field\n",
    "class LinkRequest(BaseModel):\n",
    "    key_word:Union[str,None] = None\n",
    "    proxies:Union[dict,None] = None\n",
    "    headers:Union[dict,None] = None\n",
    "    max_num:int = 5\n",
    "    line_length:int = 15\n",
    "    searchtype:str = \"all\"\n",
    "    abstracts:str = \"show\"\n",
    "    order:str = \"-announced_date_first\"\n",
    "    size:int = 50\n",
    "    show_meta_data:bool = True\n",
    "    daily_type:str = \"cs\"\n",
    "    max_retry:int = 3\n",
    "    wait_fixed:int = 1000\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "若访问正常，访问的结果为：\n",
    "```json\n",
    "{\n",
    "  \"links\": [\n",
    "    \"https://arxiv.org/pdf/xxx\",\n",
    "    \"https://arxiv.org/pdf/xxx\",\n",
    "  ],\n",
    "  \"titles\": [\n",
    "      \"xxx\",\n",
    "      \"xxx\",\n",
    "    ],\n",
    "  \"abstract\": [\n",
    "    \"xxx\",\n",
    "    \"xxx\",\n",
    "    ],\n",
    "  \"authors\":\n",
    "    [\n",
    "    \"xxx\",\n",
    "    \"xxx\",\n",
    "    ],\n",
    "  \"error\": null\n",
    "}\n",
    "```\n",
    "> noted that titles, abstract, authors are all in markdown format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "简易版params为\n",
    "```json\n",
    "params = {\n",
    "    \"key_word\":None\n",
    "}\n",
    "```\n",
    "标准版为：\n",
    "```json\n",
    "params = {\n",
    "    \"key_word\":None,\n",
    "    \"proxies\":None,\n",
    "    \"headers\":None,\n",
    "    \"max_num\":5,\n",
    "    \"line_length\":15,\n",
    "    \"searchtype\":\"all\",\n",
    "    \"abstracts\":\"show\",\n",
    "    \"order\":\"-announced_date_first\",\n",
    "    \"size\":50,\n",
    "    \"show_meta_data\":True,\n",
    "    \"daily_type\":\"cs\",\n",
    "    \"max_retry\":3,\n",
    "    \"wait_fixed\":1000\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:2023-11-22 11:19:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: process success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'links': ['https://arxiv.org/pdf/2311.12741', 'https://arxiv.org/pdf/2311.12657', 'https://arxiv.org/pdf/2311.12644', 'https://arxiv.org/pdf/2311.12630', 'https://arxiv.org/pdf/2311.12616'], 'titles': ['[$$~~~~Content~Augmented~Graph~Neural~Networks~$$](https://arxiv.org/pdf/2311.12741)', '[$$~~~~Carbohydrate~NMR~chemical~shift~predictions~using~E(3)~equivariant~graph~neural~networks\\\\\\\\~$$](https://arxiv.org/pdf/2311.12657)', '[$$~~~~Careful~Selection~and~Thoughtful~Discarding:~Graph~Explicit~Pooling~Utilizing~Discarded~Nodes\\\\\\\\$$](https://arxiv.org/pdf/2311.12644)', '[$$~~~~Hierarchical~Joint~Graph~Learning~and~Multivariate~Time~Series~Forecasting$$](https://arxiv.org/pdf/2311.12630)', '[$$~~~~DeepTreeGAN:~Fast~Generation~of~High~Dimensional~Point~Clouds$$](https://arxiv.org/pdf/2311.12616)'], 'abstract': ['$$~Abstract:~~~~In~recent~years,~graph…~~~$$', '$$~Abstract:~~~~…An~important~part~of~this~process~is~to~predict~the\\\\\\\\~NMR~chemical~shift~from~the~molecular~structure.~This~work~introduces~a~novel~approach~that~leverages\\\\\\\\~E(3)~equivariant~graph~neural~networks~to~predict~carbohydrate~NMR~spectra.~Notably,~our~model~achieves~a\\\\\\\\~substantial~reducti…~~~$$', '$$~Abstract:~~Graph~pooling~has~been~increasingly~recognized~as~crucial~for…~~~\\\\\\\\$$', '$$~Abstract:~~~~…dependencies~and~intricate~interactions––both~direct~and~indirect.~To~confront~these\\\\\\\\~complexities,~we~introduce~a~method~of~representing~multivariate~signals~as~nodes~in~a~graph~with\\\\\\\\~edges~indicating~interdependency~between~them.~Specifically,~we~leverage…~~~$$', '$$~Abstract:~~~~…are~inherently~tree–based~processes,~as~each~particle~is~produced~by\\\\\\\\~the~decay~or~detector~interaction~of~a~particle~of~the~previous~generation.~In~this~work,\\\\\\\\~we~present~a~novel~Graph~Neural~Network~model~(DeepTreeGAN)~that~is~able~to~generate~such\\\\\\\\~point~clouds~in~a~tree–based~ma…~~~$$'], 'authors': ['$$~Authors:~Fatemeh~Gholamzadeh~Nasrabadi,~~~AmirHossein~Kashani,~~~Pegah~Zahedi,~~\\\\\\\\~Mostafa~Haghir~Chehreghani~$$', '$$~Authors:~Maria~Bånkestad,~~~Keven~M.~Dorst,~~~Göran~Widmalm,~~\\\\\\\\~Jerk~Rönnols~$$', '$$~Authors:~Chuang~Liu,~~~Wenhang~Yu,~~~Kuang~Gao,~~~Xueqi\\\\\\\\~Ma,~~~Yibing~Zhan,~~~Jia~Wu,~~~Bo~Du,~~\\\\\\\\~Wenbin~Hu~$$', '$$~Authors:~Juhyeon~Kim,~~~Hyungeun~Lee,~~~Seungwon~Yu,~~~Ung\\\\\\\\~Hwang,~~~Wooyul~Jung,~~~Miseon~Park,~~~Kijung~Yoon~$$', '$$~Authors:~Moritz~Alfons~Wilhelm~Scham,~~~Dirk~Krücker,~~~Benno~Käch,~\\\\\\\\~~Kerstin~Borras~$$']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from submodule.my_utils import custom_response_handler\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def init_config(set_none:bool = False):\n",
    "    yaml_path = './config.yaml'\n",
    "    if os.path.exists(yaml_path):\n",
    "        with open(yaml_path, 'r') as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "    else:\n",
    "        logging.error(f'Config file not found at {yaml_path}')\n",
    "        sys.exit(1)\n",
    "    openai_info = config[\"openai\"]\n",
    "    with open(openai_info['prompts_path'], 'r') as f:\n",
    "        prompts = json.load(f)\n",
    "    arxiv_info = config['arxiv']\n",
    "    nougat_info = config[\"nougat\"]\n",
    "    proxy = arxiv_info['proxy']\n",
    "    headers = arxiv_info['headers']\n",
    "    base_url = openai_info['base_url']\n",
    "    if set_none:\n",
    "        proxy = headers = None\n",
    "    return openai_info, arxiv_info, nougat_info,prompts, proxy, headers,base_url\n",
    "openai_info, arxiv_info, nougat_info,prompts, proxy, headers,base_url = init_config(set_none=False)\n",
    "req_url = \"http://61.241.103.32:5010\"\n",
    "# req_url = \"http://127.0.0.1:8000\"\n",
    "link_url = req_url + '/get_links/'\n",
    "keyword = \"graph neural network\"\n",
    "params = {\n",
    "    \"key_word\": keyword,\n",
    "}\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(f'start time:{start_time}')\n",
    "now_time = time.time()\n",
    "response = requests.post(link_url, json=params)\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you post a request to the server with a wrong Body, you will get a response like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:request body error[], status: request error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request error input params error: \n",
      "error 1:type: type_error.str, location: request body, param key_word input:None, msg: str type expected\n",
      "error 2:type: type_error.list, location: request body, param key_word input:None, msg: value is not a valid list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong_params = {\n",
    "    \"key_word\": {\"sda\":111}\n",
    "}\n",
    "\n",
    "response = requests.post(link_url, json=wrong_params)\n",
    "\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you post some params that may cause the backend internal error,it will be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_params = {\n",
    "    \"key_word\": \"cv\",\n",
    "    \"proxies\":proxy,\n",
    "}\n",
    "\n",
    "response = requests.post(link_url, json=wrong_params)\n",
    "\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器接受参数形式如下\n",
    "\n",
    "```python\n",
    "class Args(BaseModel):\n",
    "    batchsize:int = get_batch_size()\n",
    "    checkpoint:Union[str,Path] = \"./pretrained_w\"\n",
    "    out:Union[str,Path] = './res'\n",
    "    recompute:bool = False\n",
    "    markdown:bool = True\n",
    "    pdf:Union[List[str],List[Path],str,Path] = None\n",
    "\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    args:Args = Field(...,arbitrary_types_allowed=True)\n",
    "    proxy:Union[Dict,None] = None\n",
    "    headers:Union[Dict,None] = None\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params标准形式为\n",
    "```json\n",
    "    params = {\n",
    "        \"args\": _args, \n",
    "        \"proxy\": _proxy,\n",
    "        \"headers\": _headers\n",
    "    }\n",
    "```\n",
    "\n",
    "其中args:\n",
    "```json\n",
    "    { \n",
    "    'recompute': True, \n",
    "    'markdown': True, \n",
    "    'kw': 'QA', \n",
    "    'pdf': ['https://arxiv.org/pdf/2311.01449'], \n",
    "    'num_process': 3\n",
    "    }\n",
    "\n",
    "```\n",
    "简易版：\n",
    "```json\n",
    "    { \n",
    "    'pdf': ['https://arxiv.org/pdf/2311.01449']\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "得到的output形式为：\n",
    "```json\n",
    "{\n",
    "    \"article_ls\":\n",
    "        [\n",
    "            \"article_text1\",\n",
    "            \"article_text2\"\n",
    "        ],\n",
    "    \"file_names\":\n",
    "        [\n",
    "            \"file_name1\",\n",
    "            \"file_name2\"\n",
    "        ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:2023-11-22 10:36:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: process success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_ls': ['# TopicGPT: A Prompt-based Topic Modeling Framework\\n\\nChau Minh Pham\\\\({}^{1}\\\\)   Alexander Hoyle\\\\({}^{2}\\\\)   Simeng Sun\\\\({}^{1}\\\\)   Mohit Iyyer\\\\({}^{1}\\\\)\\n\\n\\\\({}^{1}\\\\)University of Massachusetts Amherst  \\\\({}^{2}\\\\)University of Maryland\\n\\n{ctpham, simengsun, miyyer}@umass.edu,\\n\\nhoyle@umd.edu\\n\\n###### Abstract\\n\\nTopic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require \"reading the tea leaves\" to interpret; additionally, they offer users minimal semantic control over topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics within a provided text collection. TopicGPT produces topics that align better with human categorization compared to competing methods: for example, it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. TopicGPT can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, humancentered approach to topic modeling.1\\n\\nFootnote 1: Code at [https://github.com/chtmp223/topicGPT](https://github.com/chtmp223/topicGPT)\\n\\n## 1 Introduction\\n\\nTopic modeling is a commonly used technique for discovering latent thematic structures in extensive collections of text documents. Traditional topic models such as latent Dirichlet allocation Blei et al. (2003), LDA) represent documents as mixtures of topics, where each topic is a distribution over words.2 Topics are often represented with their most probable words, but this representation can contain incoherent or unrelated words that make topics difficult for users to interpret Chang et al. (2009); Newman et al. (2010). Although some models enable users to interactively guide topics based on needs and domain knowledge Hu et al. (2014); Nikolenko et al. (2017), their usability is constrained by the bag-of-words topic format.\\n\\nFootnote 2: “Word” may also refer to \\\\(n\\\\)-grams or other lexical items.\\n\\nTo address these limitations, we introduce TopicGPT (Figure 1), a human-centric approach to topic modeling that relies on prompting large language models to perform in-context topic generation (SS3.1) and assignment (SS3.2). First, we iteratively prompt an LLM to generate new topics given a sample of documents from an input dataset and a list of previously generated topics. The resulting set of topics is then further refined to consolidate redundant topics and eliminate infrequent ones. Finally, given a new document, an LLM assigns it to one or more of the generated topics, also providing a quotation from the document to support its assignment. These quotations make the method easily verifiable, addressing some of the validity concerns that plague traditional topic models.\\n\\nTopicGPT produces higher-quality topics than competing approaches.TopicGPT\\'s topics and assignments align significantly more closely with human-annotated ground truth topics than those from LDA and BERTopic Grootendorst (2022) on two datasets: Wikipedia articles Merity et al. (2018) and Congressional bills (as processed by Hoyle et al., 2022). We measure topical alignment using three external clustering metrics (harmonic mean purity, normalized mutual information, and adjusted Rand index) and find that TopicGPT improves substantially over baselines (e.g., absolute purity improves by 10 points over LDA on Wikipedia); furthermore, its topics are more semantically aligned with human-labeled topics (30.3% of TopicGPT\\'s topics are misaligned compared to 62.4% for LDA). Further analysis demonstrates the robustness of TopicGPT\\'s topic quality across various prompt and data settings.\\n\\n**TopicGPT produces more interpretable topics.** Topics generated by TopicGPT include natural language labels and descriptions that make them immediately interpretable without needing a separate labeling step. Furthermore, the framework provides informative document-topic associations along with contextual quoted evidence. By creating intuitive topic structures and understandable document-topic assignments, TopicGPT aims to make the overall topic modeling process more informative and interpretable.\\n\\n**TopicGPT is customizable to fit user needs:** Topic models should suit their application settings (Doogan and Buntine, 2021). In addition to interpretability, TopicGPT offers users the ability to guide the generated topics to their specific needs. Users initially provide seed topics to steer the scope and focus of the generated topics. After reviewing the initial results, users can then manually edit or remove any topics to curate a coherent and goal-oriented topic list. Notably, our approach distinguishes itself from a previous attempt in prompt-based topic mining (Wang et al., 2023) by using topic semantics to refine the resulting topic list.\\n\\n**Open-source LLMs are competent topic assigners but bad topic generators.** Most of our experiments implement topic generation with GPT-4 (OpenAI, 2023) and topic assignment with GPT-3.5-turbo, which costs around $100 per dataset (Table 6). Ideally, both stages could be implemented with open-source LLMs to remove the dependence on expensive APIs. To this end, we also experimented with the open-source Mistral-7B-Instruct model (Jiang et al., 2023), which performs fairly well for topic assignment but cannot competently follow instructions for generating topics. Improving topic generation capabilities for open-source LLMs is thus an exciting direction for future work.\\n\\n## 2 Related Work\\n\\nAutomated content analysis is the primary use case for our framework, as this is a dominant application of topic models (Hoyle et al., 2022). Inductive content analysis is a popular qualitative research method that involves deriving labels directly from the dataset without relying on preconceived categories or codes (Hsieh and Shannon, 2005; Kyngas, 2020; Vears and Gillam, 2022). In this methodology, which guided TopicGPT\\'s design, practitioners first engage in open coding of individual documents to establish initial labels. These initial codes are then carefully examined, reconfigured, and organized into a cohesive coding system.\\n\\nFigure 1: Overview of TopicGPT. 1) Topic Generation: Given a corpus and a list of manually-curated seed topics, TopicGPT identifies topics within each corpus document. The framework then refines the topic list by merging repeated topics and removing infrequent topics. 2) Topic Assignment: Given the generated topic list and a document, TopicGPT assigns the most relevant topic to the given document and provides a quote that supports this assignment.\\n\\n**Topic modeling for content analysis:** Traditional approaches to topic modeling, such as Latent Dirichlet Allocation (Blei et al., 2003, LDA), are parameterized by topic-word distributions (\\\\(\\\\phi_{k}\\\\)) and document-topic distributions (\\\\(\\\\theta_{d}\\\\)). Their inferred estimates can reveal latent thematic structures in a corpus. However, these representations are not straightforward to interpret (Mei et al., 2007; Chang et al., 2009; Ramage et al., 2011), requiring subjective manual effort that leads to issues of reliability and validity (Baden et al., 2021). Our work follows previous research that aims to produce more interpretable topics in natural language (Mei et al., 2007; Lau et al., 2011; Wan and Wang, 2016). Additionally, our design builds on both hierarchical topic models (Griffiths et al., 2003; Teh et al., 2006; Mimno et al., 2007; Paisley et al., 2014), as well as those that impose constraints on topics (Wallach et al., 2009; Hu et al., 2014).\\n\\n**LLM-based content analysis:** LLMs such as ChatGPT have enabled new prompting and embedding-based approaches to analyzing text. Researchers have used prompting techniques on these LLMs for related content analysis tasks including text clustering (Viswanathan et al., 2023; Zhang et al., 2023; Hoyle et al., 2023), abstractive summarization (Liu and Healey, 2023), and deductive qualitative coding (Tai et al., 2023; Chew et al., 2023). Prior work has also explored topic modeling using contextualized embeddings from LLMs (Sia et al., 2020; Thompson and Mimno, 2020; Bianchi et al., 2021; Grootendorst, 2022).\\n\\n**Comparison to GoalEx:** Our framework most closely resembles GoalEx (Wang et al., 2023), a goal-driven clustering approach, but is tailored specifically for topic modeling. GoalEx is focused on partitioning the corpus, and is not attuned to the overall coherence and organization of the set of topics, a fundamental need in content analysis settings. Rather than assigning each topic to individual documents, which becomes costlier as the number of clusters grows, TopicGPT\\'s topic assignment process is adaptable to the mixed membership assumption and more cost-effective by providing all topics in the prompt simultaneously. Additionally, while GoalEx was evaluated only on cluster recovery, TopicGPT is benchmarked on both stability and alignment with ground-truth topics, demonstrating its usefulness as a content analysis tool beyond just text clustering.\\n\\n## 3 Methodology\\n\\nTopicGPT consists of two main stages: topic generation (SS3.1) and topic assignment (SS3.2). Figure 1 provides an illustrative overview of our framework.\\n\\n### Stage 1: Topic Generation\\n\\nBroadly, we prompt an LLM to generate a set of topics given an input dataset, and then we further refine these topics to remove infrequently used ones and merge duplicates. The output of this step can optionally be fed to a hierarchical extension of TopicGPT that prompts the model to generate more fine-grained subtopics.\\n\\n**Generating new topics:** In the first stage, we iteratively prompt a large language model (LLM) to generate descriptive topics. Given a document \\\\(d\\\\) from the corpus and a set of seed topics \\\\(S\\\\), the model is instructed to either assign \\\\(d\\\\) to an existing topic in \\\\(S\\\\) or generate a new topic that better describes \\\\(d\\\\) and add it to \\\\(S\\\\). We define a _topic_ to be a concise label paired with a broad one-sentence description, as in\\n\\n**Trade**: Mentions the exchange of capital, goods, and services\\n\\nwhere \"Trade\" serves as the topic label. The initial seed set consists of a small number of human-written topics (our experiments use two seed topics) and does not need to be dataset-specific as we show in Appendix B (refer to Appendix A.1 for more instructions on creating seed topics). The intuition behind this iterative process is that it encourages newly generated topics to be distinctive and also match the specificity seen in other topics. Notably, instead of running topic generation over the entire corpus, which can be extremely costly, we apply the process to a carefully constructed sample from the dataset (see Section 4.3 for an in-depth discussion on the formation of this subset).\\n\\n**Refining generated topics:** We further refine the generated topics to ensure that the final topic list is coherent and non-redundant.3 We first use Sentence-Transformer embeddings (Reimers and Gurevych, 2019) to identify pairs of topics with cosine similarity \\\\(\\\\geq\\\\) 0.5. We then prompt the LLM with five such topic pairs, instructing it to merge relevant or near-duplicate pairs where appropriate. This merging step consolidates redundant topicsand aligns the specificity across topics, returning a coherent final list. To address any minor topics that may have been overlooked in the prior step, we eliminate topics with low frequency of occurrence. To do this, we keep track of how frequently each topic gets generated. If a topic occurs below a \"removal\" threshold frequency,4 we consider that topic to be minor and remove it from the final list.\\n\\nFootnote 4: We recommend trying different thresholds to make sure that important topics are not removed from the final list.\\n\\nGenerating a topic hierarchy:We further extend TopicGPT to construct a multi-level topic hierarchy. Specifically, we treat the generated topics that remain after the refinement stage as _top-level topics_ and prompt the LLM for more specific subtopics at subsequent levels. We then provide the model with a _topic branch_ that contains a top-level topic \\\\(t\\\\), a list of seed subtopics \\\\(S^{\\\\prime}\\\\), and the documents \\\\(d_{t}\\\\) associated with the top-level topic \\\\(t\\\\). With these inputs, we instruct the LLM to generate subtopics \\\\(t^{\\\\prime}\\\\) that capture common themes among the provided documents. To ensure subtopics are grounded in the documents rather than hallucinated, the model must also return specific documents supporting each subtopic. If the documents cannot fit into a single prompt, we divide them into different prompts and include subtopics generated by earlier prompts in subsequent ones. We qualitatively evaluate our hierarchical TopicGPT in Section 5.5.\\n\\n### Stage 2: Topic Assignment\\n\\nIn the assignment stage, we aim to establish a valid and interpretable association between the generated topic list and the documents in our datasets. To achieve this, we provide the LLM with our generated topic list, 2-3 examples and a document, the topic(s) of which we are interested in obtaining. We then instruct the model to assign one or more topics to the given document. The final output contains the assigned topic label, a document-specific topic description, and a quote extracted from the document to support this assignment. The quoted text improves the verifiability of TopicGPT\\'s assignments, which has been a long-standing concern with traditional methods such as LDA. A sample topic assignment is\\n\\n**Agriculture**: Mentions changes in agricultural export requirements (\"...repeal of the agricultural export requirements...\")\\n\\nwhere \"Agriculture\" is the assigned topic label, followed by the topic description and a quote from the document enclosed within parentheses.\\n\\nSelf-correction:To address topic assignments with incorrect formatting or low quality, we incorporate a self-correction step (Shinn et al., 2023; Sun et al., 2023). Specifically, we implement a parser to identify instances of hallucinated topic assignments or invalid responses (e.g. \"None\"/\"Error\"). Subsequently, we provide the LLM with the identified documents along with the error type, shuffle the topic list to add randomness, and prompt the model to reassign a valid topic.\\n\\n## 4 Experiments\\n\\nWe compare TopicGPT to two popular topic models using two labeled English-language datasets. Our goal is to assess whether TopicGPT\\'s outputs align with human-coded ground truth topics, as well as to test its robustness to various settings. Here, we describe our datasets, baseline methods, model configurations, and evaluation metrics.\\n\\n### Datasets\\n\\nWe used two English-language datasets for evaluation: Wiki and Bills. We adopt Hoyle et al.\\'s processing pipeline for both datasets. Table 1 shows high-level statistics for these datasets.\\n\\n**Wiki**(Merity et al., 2018) is a corpus of 14,290 Wikipedia articles that meet a core set of editorial standards (also known as \"good\" articles). This dataset comes with 15 high-level, 45 mid-level, and 279 low-level labels that were annotated by humans. Since the Wiki text-label mapping might have appeared in the pre-training corpus of LLMs, TopicGPT\\'s performance on this dataset reflects a best-case scenario for our algorithm (Thompson and Mimno, 2020).\\n\\n**Bills**(Adler and Wilkerson 2018, compiled by Hoyle et al. 2022) contains 32,661 bill summaries from the \\\\(110-114^{\\\\text{th}}\\\\) U.S. congresses. This dataset comes with 21 high-level and 114 low-level human-annotated labels. Bills is far less likely to appear in LLMs\\' training data because the label text is not directly associated with the corresponding documents.5\\n\\n### Baselines\\n\\nWe considered two popular topic models that follow different paradigms: LDA and BERTopic.\\n\\nLda:We use the MALLET McCallum (2002) implementation of LDA with Gibbs sampling Griffiths (2002). LDA is a well-established topic model that boasts strong alignment with human codes Srivastava and Sutton (2017); Hoyle et al. (2021) and high stability compared to neural topic models Hoyle et al. (2022). We set \\\\(|V|\\\\) = 15,000, \\\\(\\\\alpha=1.0\\\\), \\\\(\\\\beta=0.1\\\\), and run LDA for 2,000 iterations with optimization at every 10 intervals. For fair comparison, we control the number of topics \\\\(k\\\\) to be equal to the number of topics generated by TopicGPT.\\n\\nBERTopic:BERTopic is a popular neural topic model that obtains topics by performing clustering on Sentence-Transformer embeddings of documents Grootendorst (2022). We maintained all default hyperparameters except for the number of topics \\\\(k\\\\), which were controlled to be equal to the number of topics generated by TopicGPT.\\n\\n### Sampling documents for TopicGPT\\n\\nThe number of documents used during the topic generation phase is a critical parameter of TopicGPT. Given enough time and money (if using closed-source LLM APIs), we could certainly feed the entire training corpus into the LLM for topic generation. However, this is impractical given our current setup, which implements both topic generation and assignment with OpenAI\\'s GPT models, and it is also unnecessary as we confirm both in theory and practice below. We thus sample a subset of documents uniformly at random for topic generation, which results in 1,000 documents from Bills and 1,100 documents from Wiki.\\n\\nHow many documents should we sample?We recommend that users of TopicGPT either choose a sample size that fits their budget or run topic generation incrementally and stop when no new topics are generated for some threshold (e.g., 100 documents) -- a \"topic drought.\" To assess the effectiveness of this approach in achieving comprehensive topic coverage, we examine the topics generated after reaching this \"topic drought\" threshold and check whether our refinement process removed any of them. Figure 2 shows that on both datasets, the number of new topics that remain after refinement plateaus after around 500 documents have been generated.\\n\\nA probabilistic justification:We can also proceed probabilistically: the user sets a lower bound on the number of documents that should be assigned to the least-prevalent topic, \\\\(n_{d}\\\\), which induces an upper bound on the number of topics, \\\\(K_{u}=\\\\lfloor N/n_{d}\\\\rfloor\\\\). We can then model the sample of size \\\\(n_{s}\\\\) as a draw from a uniform7\\\\(\\\\mathbf{\\\\epsilon}\\\\sim\\\\text{Multi}(n_{s},K_{u})\\\\), where the expected number of zeros is then \\\\(\\\\frac{(K_{u}-1)^{n_{s}}}{K_{u}^{n_{s}-1}}\\\\). A straightforward simulation can be used to find the \\\\(n_{s}\\\\) that minimizes \\\\(p^{*}=\\\\lfloor P\\\\left(\\\\min_{k}c_{k}=0\\\\right)-\\\\varepsilon\\\\rfloor\\\\), where \\\\(\\\\varepsilon\\\\) is a user-defined acceptable minimum probability of failing to find the least-prevalent topic. Taking the Wiki dataset, if we set \\\\(n_{d}=140\\\\) (1% of the corpus) then \\\\(n_{s}=1,100\\\\) leads to \\\\(p^{*}\\\\approx 0.005\\\\).\\n\\nFootnote 7: If it were non-uniform, then we would have a topic with fewer than \\\\(n_{d}\\\\) documents.\\n\\n### TopicGPT implementation details\\n\\nOur default TopicGPT setting uses OpenAI\\'s GPT-4 to generate topics and GPT-3.5-turbo to assign topics to documents. To encourage deterministic outputs,8 we set max_tokens to 300 and temperature and top_p to 0. We truncate input documents that are too long to fit within the context window size of LLMs.9 Since Bills and Wiki have one-to-one mappings between documents and labels, we modify the assigner prompt to assign only one topic per document, although we emphasize that the method does not require a single assignment. To retain important topics in the final list, we set the removal frequency threshold to 10 and 5 for Bills and Wiki, respectively. We enable self-correction with a retry limit of 10 and end up resolving all topic hallucinations and formatting issues within this limit. For evaluation purposes, we sample 8,024 documents from Wiki\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l r r} \\\\hline \\\\hline  & Bills & Wiki \\\\\\\\ \\\\hline \\\\# docs & 32,661 & 14,290 \\\\\\\\ avg. \\\\# tokens / doc & 261 & 3,412 \\\\\\\\ \\\\# test examples & 15,242 & 8,024 \\\\\\\\ \\\\# topic generation docs & 1,000 & 1,100 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Dataset statistics of Bills and Wiki. The average number of tokens per document was calculated using tiktoken BPE tokenizer.6and 15,242 documents from Bills that are not included in the topic generation sample.\\n\\n### Evaluation Setup\\n\\nTo assess the usefulness of TopicGPT as an automated content analysis tool, we evaluate the topic alignment and stability of the generated topics, following (Hoyle et al., 2022).\\n\\n#### 4.5.1 Topical alignment\\n\\nSince TopicGPT is not a probabilistic model like LDA, we compare it to baselines by assessing the alignment between predicted topic assignments and topic labels in the ground truth, as also done in prior work (Chuang et al., 2013; Poursabzi-Sangdeh et al., 2016; Korencic et al., 2021; Hoyle et al., 2022). For LDA, we assign each document to its highest-probability topic. Concretely, given a set of ground-truth classes \\\\(\\\\mathbf{C}=\\\\{c_{1},...,c_{J}\\\\}\\\\) and a set of predicted assignment clusters \\\\(\\\\mathbf{\\\\Omega}=\\\\{\\\\omega_{1},...,\\\\omega_{K}\\\\}\\\\), we assessed the alignment between \\\\(\\\\mathbf{\\\\Omega}\\\\) and \\\\(\\\\mathbf{C}\\\\) using the external evaluation metrics for clustering detailed below.\\n\\nPurity.Purity measures the degree to which predicted clusters contain data points predominantly from a single ground-truth class (Zhao, 2005). Purity is computed by assigning each predicted cluster to the ground-truth class that occurs most frequently within it, and then counting the number of correctly assigned documents and dividing by the number of clustered items \\\\(N\\\\),\\n\\n\\\\[\\\\mathrm{Purity}(\\\\mathbf{\\\\Omega},\\\\mathbf{C})=\\\\sum_{k}\\\\frac{|\\\\omega_{k}|}{N} \\\\max_{j}\\\\mathrm{Precision}(\\\\omega_{k},c_{j}) \\\\tag{1}\\\\]\\n\\nwhere the precision of a cluster \\\\(\\\\omega_{k}\\\\) for a given ground-truth class \\\\(\\\\mathbf{C}_{i}\\\\) is calculated as:\\n\\n\\\\[\\\\mathrm{Precision}(\\\\omega_{k},c_{j})=\\\\frac{|\\\\omega_{k}\\\\cap c_{j}|}{|\\\\omega_{k}|} \\\\tag{2}\\\\]\\n\\nWhile high purity indicates low intra-cluster noise, it does not reward grouping items from the same ground-truth class. Purity reaches its maximum value of 1 when each cluster contains just one item. Inverse Purity, defined as \\\\(\\\\mathrm{Purity}^{-1}=\\\\mathrm{Purity}(\\\\mathbf{C},\\\\mathbf{\\\\Omega})\\\\), addresses this by rewarding clustering data points from the same class together. However, Inverse Purity fails to penalize mixing items from different ground-truth categories. Amigo et al. (2009) show that the harmonic mean of Purity and Inverse Purity balances these two objectives:\\n\\n\\\\[P_{1}=\\\\sum_{k}\\\\frac{|c_{k}|}{N}\\\\max_{j}F(c_{j},\\\\omega_{k})\\\\]\\n\\nwhere\\n\\n\\\\[F(c_{k},\\\\omega_{j})=\\\\frac{2*\\\\mathrm{Precision}(c_{k},\\\\omega_{j})*\\\\mathrm{Recall }(c_{k},\\\\omega_{j})}{\\\\mathrm{Precision}(c_{k},\\\\omega_{j})+\\\\mathrm{Recall}(c_{k },\\\\omega_{j})} \\\\tag{3}\\\\]\\n\\nand\\n\\n\\\\[\\\\mathrm{Recall}(\\\\mathbf{C},\\\\mathbf{\\\\Omega})=\\\\mathrm{Precision}(\\\\mathbf{\\\\Omega},\\\\mathbf{C}) \\\\tag{4}\\\\]\\n\\nAdjusted Rand Index.The Rand Index measures the pairwise agreement between two sets of clusterings (Rand, 1971):\\n\\n\\\\[\\\\mathit{RI}(\\\\mathbf{\\\\Omega},\\\\mathbf{C})=\\\\frac{TP+TN}{TP+FP+FN+TN} \\\\tag{5}\\\\]\\n\\nwhere \\\\(TP\\\\) is the number of true positives, \\\\(TN\\\\) is the number of true negatives, \\\\(FP\\\\) is the number of\\n\\nFigure 2: The number of topics generated over documents processed in the Bills and Wiki corpus. The grey line indicates the number of expected topics, simulated using the empirical distribution of ground-truth topics for the datasets. For both datasets, we see a similar pattern - after a “topic drought” period marked by the dashed red line, the number of initially generated topics (orange line) keeps increasing. However, the final refined topics (blue line) and expected number of topics (grey line) plateau, despite more documents being processed.\\n\\nfalse positives, and \\\\(FN\\\\) is the number of false negatives. Adjusted Rand Index (ARI) further corrects for chance by comparing it to the expected value\\n\\n\\\\[ARI(\\\\mathbf{\\\\Omega},\\\\mathbf{C})=\\\\frac{\\\\text{RI}-\\\\mathbb{E}[\\\\text{RI}]}{\\\\max( \\\\text{RI})-\\\\mathbb{E}[\\\\text{RI}]} \\\\tag{6}\\\\]\\n\\nwhere \\\\(\\\\mathbb{E}[\\\\text{RI}]\\\\) stands for expected rand index (Hubert and Arabie, 1985; Vinh et al., 2009). ARI yields a score close to 0 for random cluster assignments and near 1 for strongly consistent assignments.\\n\\nNormalized Mutual Information.Mutual Information (MI) measures the amount of shared information between two sets of clusterings (Shannon, 1948). Normalized Mutual Information (NMI) normalizes the MI score to a value between 0 and 1, making MI less sensitive to a varying number of clusters (Strehl and Ghosh, 2002):\\n\\n\\\\[NMI(\\\\mathbf{\\\\Omega},\\\\mathbf{C})=\\\\frac{I(\\\\mathbf{\\\\Omega},\\\\mathbf{C})}{[H( \\\\mathbf{\\\\Omega})+H(\\\\mathbf{C})]/2} \\\\tag{7}\\\\]\\n\\nwhere \\\\(I(\\\\mathbf{\\\\Omega},\\\\mathbf{C})\\\\) is the mutual information between \\\\(\\\\Omega\\\\) and \\\\(\\\\mathbf{C}\\\\), \\\\(H(\\\\mathbf{\\\\Omega})\\\\) and \\\\(H(\\\\mathbf{C})\\\\) are the entropy of \\\\(\\\\mathbf{\\\\Omega}\\\\) and \\\\(\\\\mathbf{C}\\\\).\\n\\nComparison of metrics:\\\\(P_{1}\\\\), ARI, and NMI provide complementary perspectives through set matching, counting pairs, and variation of information (Meila, 2007). While each of these scores provides useful insights, relying on just one can be misleading. For instance, \\\\(P_{1}\\\\) places a strong emphasis on cluster purity while being less concerned with the distribution of ground-truth labels. ARI, unlike NMI, is adjusted for chance, but it does not account for class imbalance, which makes it more sensitive to the number of predicted clusters.\\n\\n#### 4.5.2 Stability\\n\\nWe also assess the robustness of TopicGPT to changes in prompts and also different corpus samples for topic generation. More specifically, we measure if TopicGPT maintains high topical alignment in these modified settings.\\n\\nOut-of-domain prompts.In TopicGPT, users can change the seed topics and few-shot examples in the prompts to tailor the method to their dataset. We explore whether prompts written for one dataset can work on another by applying prompts for Wiki on the Bills dataset.\\n\\nAdditional seed topics.We assess the impacts of additional seed topics on TopicGPT\\'s performance in Bills. Our original prompt for topic generation contains two seed topics. In this experiment, we expand the prompt by adding three more seed topics.\\n\\nShuffling sampled documents for topic generation.We shuffle documents in the original generation sample in Bills to understand the importance of the order in which LLM processes documents.\\n\\nUsing a different sample for topic generation.To evaluate TopicGPT\\'s robustness to data shift, we apply TopicGPT to a different generation sample from Bills and examine how much the resulting topic outputs varied.\\n\\n## 5 Results\\n\\nIn this section, we present the findings of our experiments, demonstrating TopicGPT\\'s strong topical alignment with ground truth and robustness to variations in prompts and data. We also show through human experiments that the _semantic_ content of TopicGPT\\'s generated topics (ignoring document assignment) is significantly more aligned with ground-truth topics on both datasets. Furthermore, we explore the impact of modifications within the framework, including the use of an open-source model for topic generation, and qualitatively analyze its hierarchical variant.\\n\\n### TopicGPT is strongly aligned to ground truth labels\\n\\nOur experiments (Table 2) show that TopicGPT identifies topics that are substantially more aligned with human-annotated labels than baselines, and that this improvement holds across all datasets, settings, and evaluation metrics. Of the two baseline models, LDA is generally superior to BERTopic for all metrics, suggesting that LDA remains a strong baseline. However, neither baseline approaches the performance of TopicGPT: for example, TopicGPT achieves post-refinement harmonic purity scores \\\\(P_{1}\\\\) of 0.74 and 0.57 on Wiki and Bills, respectively, compared to 0.64 and 0.52 for LDA and 0.58 and 0.39 for BERTopic.\\n\\nWhere does TopicGPT disagree with the ground truth?To fully understand the disagreement between TopicGPT and human labels, we closely examine five assignments where the ground truth topics and TopicGPT\\'s assignments in the default setting do not match (Table 12), and we find that each sampled document could reasonably be assigned multiple topics. Therefore, TopicGPT\\'s assignments can still be valid even if they differ from the ground truth labels. For example, the second document in Table 12 could fit either the \"_Labor_\" or \"_Transportation Safety_\" topics, though \"_Labor_\" is more prominent. If we allow TopicGPT to assign multiple topics per document, can it successfully retrieve all appropriate topics? To check, we re-run topic assignment using a prompt that allows the assignment of multiple topics per document. With this updated prompt, three out of the five Wiki examples are assigned to the ground truth as well as the originally assigned topic (Table 12). Thus, we recommend that practitioners use multi-label assignment prompts to extract as many relevant topics as possible.\\n\\n### TopicGPT is stable\\n\\nBroadly speaking, TopicGPT produces comparable topical alignment with the ground truth across all modified experimental settings for the Bills data (lower portion of Table 2). The setting with additional seed topics obtains the worst performance across all metrics, suggesting that simply adding more seed topics is not always helpful. We hypothesize that too many seed topics may overwhelm the model and lead to poorer coherence, as the model tries to fit diverse topics rather than consolidating around the most salient themes. We recommend keeping the seed topic list small (2-3 high-quality topics) for best results, rather than lengthening the list arbitrarily. More details on curating seed topics can be found in Appendix A.1.\\n\\nConsistency between multiple settings of TopicGPT:To further evaluate the consistency of TopicGPT\\'s topic assignments under different settings, we computed alignment scores (\\\\(P_{1}\\\\), ARI, and NMI) between the default setting and each modified setting. As a benchmark, we ran LDA 10 times with \\\\(k=79\\\\) topics and calculated the average internal alignment between each pair of runs. Table 3 shows TopicGPT\\'s assignments were highly stable across settings, with all metrics within a tight 0.05 range. TopicGPT demonstrated greater stability than LDA in terms of \\\\(P_{1}\\\\) and ARI, while achieving comparable NMI. Interestingly, TopicGPT produces slightly different outputs between two runs with identical settings, a possible result of adding randomness to the self-correction process (see 3.2) as well as LLM API nondeterminism.\\n\\n### TopicGPT topics are semantically close to ground truth\\n\\nThe clustering metrics reported above do not capture whether the topics are semantically aligned with ground truth. To this end, we qualitatively compare LDA outputs and TopicGPT\\'s unrefined and refined topic generations. Specifically, we analyze the proportion of misaligned topics (defined below) generated by TopicGPT and LDA.\\n\\nManual topic matching process:Three annotators (the first author and two external annotators)10 went through the list of generated topics and assigned each topic to a ground truth class. If an\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l c c c c c c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Dataset} & \\\\multirow{2}{*}{Setting} & \\\\multicolumn{3}{c}{TopicGPT} & \\\\multicolumn{3}{c}{LDA} & \\\\multicolumn{3}{c}{BERTopic} \\\\\\\\ \\\\cline{3-11}  & & \\\\(P_{1}\\\\) & ARI & NMI & \\\\(P_{1}\\\\) & ARI & NMI & \\\\(P_{1}\\\\) & ARI & NMI \\\\\\\\ \\\\hline \\\\multirow{3}{*}{Wiki} & Default setting (\\\\(k=31\\\\)) & **0.73** & **0.58** & **0.71** & 0.59 & 0.44 & 0.65 & 0.54 & 0.24 & 0.50 \\\\\\\\  & Refined topics (\\\\(k=22\\\\)) & **0.74** & **0.60** & **0.70** & 0.64 & 0.52 & 0.67 & 0.58 & 0.28 & 0.50 \\\\\\\\ \\\\hline \\\\multirow{2}{*}{Bills} & Default setting (\\\\(k=79\\\\)) & **0.57** & **0.42** & **0.52** & 0.39 & 0.21 & 0.47 & 0.42 & 0.10 & 0.40 \\\\\\\\  & Refined topics (\\\\(k=24\\\\)) & **0.57** & **0.40** & **0.49** & 0.52 & 0.32 & 0.46 & 0.39 & 0.12 & 0.34 \\\\\\\\ \\\\hline \\\\multirow{6}{*}{Bills} & \\\\multicolumn{3}{c}{_TopicGPT stability solutions, baselines controlled to have the same number of topics (\\\\(k\\\\)),_} & \\\\multicolumn{1}{c}{} & & & & & \\\\\\\\  & Different generation sample (\\\\(k=73\\\\)) & **0.57** & **0.40** & **0.51** & 0.41 & 0.23 & 0.47 & 0.38 & 0.08 & 0.38 \\\\\\\\  & Out-of-domain prompts (\\\\(k=147\\\\)) & **0.55** & **0.39** & **0.51** & 0.31 & 0.14 & 0.47 & 0.35 & 0.07 & 0.41 \\\\\\\\  & Additional seed topics (\\\\(k=123\\\\)) & **0.50** & **0.33** & **0.49** & 0.33 & 0.15 & 0.46 & 0.36 & 0.07 & 0.40 \\\\\\\\  & Shuffled generation sample (\\\\(k=118\\\\)) & **0.55** & **0.40** & **0.52** & 0.33 & 0.16 & 0.47 & 0.36 & 0.08 & 0.40 \\\\\\\\  & Assigning with Mistral (\\\\(k=79\\\\)) & **0.51** & **0.37** & **0.46** & 0.39 & 0.21 & 0.47 & 0.42 & 0.10 & 0.40 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 2: Topical alignment between ground-truth labels and predicted assignments. Overall, TopicGPT achieves the best performance across all settings and metrics compared to LDA and BERTopic. The number of topics used in each setting is specified as \\\\(k\\\\). The largest values in each metric and setting are **bolded**.\\n\\nexact match was not possible, the annotators labeled the generated topic as either \"out-of-scope\", \"repeated\", or \"missing\" (Chuang et al., 2013) according to the following definition:\\n\\n1. Out-of-scope topics: topics that are too narrow or too broad compared to the associated ground truth topic.\\n2. Missing topics: topics present in the ground truth but not in the generated outputs.\\n3. Repeated topics: topics that are duplicates of other topics.\\n\\nAfter completing the mapping, the percentage of out-of-scope, missing, and repeated topics was calculated for each annotator\\'s mappings (Table 4). Each person completed a total of 6 mappings, including the LDA outputs, and unrefined and refined topic hierarchies for both the Bills and Wiki datasets.11\\n\\nFootnote 11: Mapping interface and instructions can be found here.\\n\\n**TopicGPT contains far fewer misaligned topics than LDA, especially after refinement.** Compared to LDA, both TopicGPT\\'s unrefined and refined topics are less likely to be misaligned overall (62.4% for LDA vs. 38.7% unrefined and 30.3% refined). Annotators also noted that TopicGPT\\'s outputs are much easier to work with compared to the ambiguous LDA outputs (see Table 5 for some examples of LDA and TopicGPT\\'s outputs). We notice that refinement consistently reduces the number of out-of-scope and repeated topics. On the other hand, refinement does increase the number of missing topics by 1 in the Bills dataset; upon closer examination, this missing topic was \"_Culture_\", which appears infrequently in the Bills corpus (only 23 documents out of 32,661 documents). This might be acceptable depending on the use case, and we emphasize again that practitioners should try different refinement thresholds to avoid filtering out topics that are important to their research question.\\n\\n### Implementing TopicGPT with open-source LLMs\\n\\nWe examine alternate LLMs for both topic assignment and topic generation and discover that while topic assignment can be feasibly performed with open-source LLMs, topic generation is too complex for all LLMs we tried other than GPT-4.\\n\\n**Mistral-7B-Instruct for topic assignment:** We experiment with Mistral-7B-Instruct (Jiang et al., 2023) for topic assignment to assess its feasibility as a lower-cost alternative to GPT-3.5-turbo. Mistral\\'s topic assignments align reasonably well with human ground truth, though not to the same degree as GPT-3.5-turbo; the bottom row of Table 2 shows an absolute purity decrease of about \\\\(\\\\sim 6\\\\) points. However, the resulting assignments still outperform LDA and BERTopic on all clustering metrics.\\n\\n**Mistral-7B-Instruct for topic generation:** We further test both Mistral and GPT-3.5-turbo as topic generation models, finding that both models struggle to follow formatting instructions for topic generation. In total, Mistral and GPT-3.5-turbo produced 1,418 and 151 topics, respectively. The large number of topics made it impossible to include all of them in a single-topic assignment prompt. Additionally, most of the generated topics are very fine-grained with low frequency of occurrence, meaning they would likely be removed during refinement. Here are some overly specific topics produced by Mistral for the Bills data:\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l c c c} \\\\hline \\\\hline Method & Setting & \\\\(P_{1}\\\\) & ARI & NMI \\\\\\\\ \\\\hline LDA & Default setting (\\\\(k=79\\\\)) & 0.64 & 0.55 & 0.71 \\\\\\\\ \\\\hline \\\\multirow{8}{*}{TopicGPT} & Different generation sample (\\\\(k=73\\\\)) & 0.67 & 0.61 & 0.69 \\\\\\\\  & Out-of-domain prompts (\\\\(k=147\\\\)) & 0.69 & 0.63 & 0.69 \\\\\\\\  & Additional seed topics (\\\\(k=123\\\\)) & 0.69 & 0.59 & 0.70 \\\\\\\\  & Shuffled generation sample (\\\\(k=118\\\\)) & 0.70 & 0.63 & 0.70 \\\\\\\\  & Assigning with Mistral (\\\\(k=79\\\\)) & 0.65 & 0.58 & 0.62 \\\\\\\\  & Refined topics (\\\\(k=24\\\\)) & 0.65 & 0.63 & 0.69 \\\\\\\\  & Running the pipeline twice (\\\\(k=79\\\\)) & 0.95 & 0.92 & 0.92 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 3: Stability of topic assignments of TopicGPT and LDA in the Bills dataset, as measured by the topical alignment between topic assignments of each modified setting against the default setting (unlike Table 2 which reports alignment against _ground-truth_ assignments). TopicGPT demonstrates overall stability across settings that is competitive with that of LDA.\\n\\n**Clinical Data Registries**: Mentions policies and regulations related to clinical data registries.\\n\\n**Quorum**: Mentions the requirement for a quorum of at least five members on the board of directors.\\n\\n**Lost Baggage**: Mentions procedures for returning lost baggage.\\n\\nWe emphasize that the instructions for topic generation are complex with many criteria (see Table 8). As such, we recommend sticking with GPT-4 or models with similar capabilities for topic generation.\\n\\n### Qualitatively inspecting topic hierarchies\\n\\nIt is simple to extend TopicGPT to generate a two-level topic hierarchy, as discussed in Section 3.1. Overall, the generated subtopics are informative and well-grounded in the documents associated with their parent topics. These subtopics successfully capture more narrow and nuanced themes within the broader parent topic, allowing for richer exploration and analysis compared to a flat topic list. Figure 3 provides a closer look into a portion of the generated topic hierarchy for the Wiki dataset. We plan to more rigorously evaluate these hierarchies in follow-up experiments.\\n\\n## 6 Limitations\\n\\nReliance on closed-source models.TopicGPT achieves optimal performance using GPT-4 for topic generation and GPT-3.5-turbo for assignment, both of which are closed-source LLMs that cost money to use (see Table 6 for our expenses). Furthermore, we have limited transparency into their pre-training and instruction tuning datasets, as well as their architectural details. Nevertheless, we observe promising results with open-source models like Mistral-7B-instruct for topic assignment. Our reliance on closed-source LLMs for topic generation reflects the current imbalance between the instruction-following abilities of closed and open-source models rather than a permanent limitation. We hope that this will eventually be addressed by the rapid advances in open-source LLMs.\\n\\nDealing with context limits.Another limitation of our current approach is the need to truncate documents to fit TopicGPT\\'s context length limit. By only providing partial documents, we lose potentially valuable context and risk misrepresenting the contents of full documents. While truncation was necessary in these initial experiments, we recognize it is not an ideal solution. We recommend future work to explore strategies to represent full documents within length limits, such as incrementally feeding in chunks of the document, sampling representative chunks, or providing a summarized version of the document.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l c c c c} \\\\hline \\\\hline Dataset & Setting & Out-of-scope & Missing & Repeated & Total \\\\\\\\ \\\\hline \\\\multirow{3}{*}{Wiki} & LDA (\\\\(k=31\\\\)) & 46.3 & 4.3 & 11.9 & 62.4 \\\\\\\\  & Unrefined (\\\\(k=31\\\\)) & 38.7 & **0.0** & 1.1 & 39.8 \\\\\\\\  & Refined (\\\\(k=22\\\\)) & **30.3** & **0.0** & **0.0** & **30.3** \\\\\\\\ \\\\hline \\\\multirow{3}{*}{Bills} & LDA (\\\\(k=79\\\\)) & 56.1 & 2.1 & 22.0 & 80.2 \\\\\\\\  & Unrefined (\\\\(k=79\\\\)) & 65.0 & **1.3** & 3.8 & 70.1 \\\\\\\\ \\\\cline{1-1}  & Refined (\\\\(k=24\\\\)) & **27.8** & 4.2 & **0.0** & **31.9** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 4: Comparison of misaligned topic proportions between LDA, unrefined TopicGPT, and refined TopicGPT outputs. Values are averaged over three annotations and rounded to one decimal place. Across both datasets, refined TopicGPT achieves the lowest proportion of misaligned topics compared to LDA. The best (lowest) misalignment proportion for each dataset is **bolded**.\\n\\nFigure 3: Example topic hierarchy for Wiki, with ”Architecture & Design” and “Animal Breeds & Husbandry” as the top-level topics generated by TopicGPT. This hierarchical topic structure, in which the upper-level topics are broad enough to encompass more detailed subtopics, allows users to explore topics at different levels of specificity.\\n\\nMultilinguality.We have not yet evaluated TopicGPT on non-English datasets. However, OpenAI\\'s LLMs are pre-trained and instruction-tuned primarily on English language data, and the instruction-following capabilities of LLMs in non-English languages are thus notably degraded (Huang et al., 2023; Li et al., 2023). We hope that future advances in multilingual LLMs will make TopicGPT more broadly accessible.\\n\\n## 7 Future Work\\n\\nHuman evaluation.While our automated metrics provide a first look at the stability and topical alignment of topics produced by TopicGPT, human judgment is still critical for evaluating how meaningful and interpretable the topics are in real-life usage (Chang et al., 2009; Hoyle et al., 2021). To better understand the practical utility of TopicGPT, we plan to conduct semi-structured interviews with domain experts to understand their experience using TopicGPT. The interviews will gather feedback on the intelligibility of topics, how well topics capture key semantic concepts, and whether the tool provided new insights or would be useful for real-world tasks. We will synthesize user comments to refine TopicGPT and gain insights into human perspectives on achievable standards of quality for topic modeling.\\n\\nHierarchy evaluation.As TopicGPT is capable of generating a multi-level topic hierarchy, our next objective is to evaluate its performance at more granular levels by determining whether TopicGPT\\'s subtopics are informative and relevant to their parent topics.\\n\\n## 8 Conclusion\\n\\nWe introduce a prompt-based framework, TopicGPT, specifically designed for topic modeling. TopicGPT addresses the interpretability and adaptability limitations of traditional topic models by generating descriptive topics that are well aligned with human-annotated ground truth. Our results demonstrate that TopicGPT outperforms baseline topic models in terms of topic alignment with ground truth clusters, while also showing robustness across different prompts and data subsets. We release our prompt templates and pipeline so that interested researchers and practitioners can try our novel topic modeling framework.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c} \\\\hline \\\\hline\\n**Stages** & Bills & Wiki \\\\\\\\ \\\\hline Topic generation & \\\\$30 & \\\\$90 \\\\\\\\ Topic refinement & \\\\$10 & \\\\$5 \\\\\\\\ Assignment + self-correction & \\\\$48 & \\\\$60 \\\\\\\\ \\\\hline Total & \\\\$88 & \\\\$155 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 6: Estimated cost (in US dollars) of running TopicGPT on the Bills and Wiki datasets. Though smaller in size, the Wiki dataset incurred a much higher cost to run TopicGPT than the Bills dataset. This is due to Wiki’s longer document length - on average 3,412 tokens per document compared to just 261 tokens per document in Bills.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l l l} \\\\hline \\\\hline\\n**Data** & **Document** & & **Ground-truth topic** & **TopicGPT assignment** & **LDA assignment** \\\\\\\\ \\\\hline Wiki & Grant Park Music Festival = The Grant Park Music Festival (formerly Grant Park Concerts) is an animal team-week classical music concert series held in Chicago, Illinois, USA. It features the Grant Park Symphony Orchestra and Grant Park Chorus along with featured guest performers andconductors. The Festival has earned non-profit organization status. It claims to be the nation’s only free, outdoor classical music series. The Grant Park Music Festival has been a Chicago tradition native 1993 when Chicago Mayor Anton Cermak suggested free concerts to lift the spirits of... & **Music \\\\& Performing Arts: Discuss creation, production, and performance of music, as well as related arts and cultural aspects.** & **City infrastructure: city, building, area, new, park \\\\\\\\ \\\\hline Bills & Perkins Fund for Equity and Excellence. This bill amends the Carl D. Perkins Career and Technical Education Act of 2006 to replace the existing Tech Prep program with a new competitive grant program to support career and technical education. Under the program, local educational agencies and their partners may apply to grant funding to support career and technical education programs that are aligned with postsecondary education programs, dual or concurrent enrollment programs and early college programs, certain evidence-based strategies and delivery models related to career and technical education, teacher and leader experimental & **Education** & **Education: Mentitions policies and programs related to higher education and student loans.** & **Programs and grants: program, grants, grant, programs, state \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 5: Example topic assignments from TopicGPT and LDA (showing top 5 topic words) on two documents. While TopicGPT’s topics closely align with the ground truth, LDA’s topics are influenced by frequently-occurring words, causing it to overlook the overarching theme of the document. TopicGPT’s topic labels and descriptions are both automatically generated, while LDA produces a bag of words that needs to be manually labeled.\\n\\n## Acknowledgement\\n\\nWe are grateful to Dzung Pham for the helpful discussion on evaluation metrics, Brendan O\\'Connor and Laure Thompson for their valuable insights, Chau-Anh for their assistance with the qualitative evaluation, and the UMass NLP community for helpful feedback throughout the project. This project was partially supported by awards IIS-2202506 and IIS-2046248 from the National Science Foundation (NSF).\\n\\n## References\\n\\n* Adler and Wilkerson (2018) E Scott Adler and John Wilkerson. 2018. Congressional bills project: 1995-2018.\\n* Amigo et al. (2009) Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felsia Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. _Information retrieval_, 12:461-486.\\n\\n* Bianchi et al. (2021) Federico Bianchi, Silvia Terragni, and Dirk Hovy. 2021. Pre-training is a hot topic: Contextualized document embeddings improve topic coherence.\\n* Blei et al. (2003) David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. _Journal of machine Learning research_, 3(Jan):993-1022.\\n* Chang et al. (2009) Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. _Advances in neural information processing systems_, 22.\\n* Chew et al. (2023) Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim. 2023. Llm-assisted content analysis: Using large language models to support deductive coding.\\n* Chuang et al. (2013) Jason Chuang, Sonal Gupta, Christopher Manning, and Jeffrey Heer. 2013. Topic model diagnostics: Assessing domain relevance via topical alignment. In _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 612-620, Atlanta, Georgia, USA. PMLR.\\n* Doogan and Buntine (2021) Caitlin Doogan and Wray Buntine. 2021. Topic model or topic twaddle? re-evaluating semantic interpretability measures. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3824-3848, Online. Association for Computational Linguistics.\\n* Griffiths et al. (2003) Thomas Griffiths, Michael Jordan, Joshua Tenenbaum, and David Blei. 2003. Hierarchical topic models and the nested chinese restaurant process. _Advances in neural information processing systems_, 16.\\n* Griffiths (2002) Tom Griffiths. 2002. Gibbs sampling in the generative model of latent dirichlet allocation. _Standford University_, 518(11):1-3.\\n* Grootendorst (2022) Maarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure.\\n* Hoyle et al. (2021) Alexander Hoyle, Pranav Goel, Andrew Hian-Cheong, Denis Peskov, Jordan Boyd-Graber, and Philip Resnik. 2021. Is Automated Topic Model Evaluation Broken? The Incoherence of Coherence. In _Advances in Neural Information Processing Systems_, volume 34, pages 2018-2033. Curran Associates, Inc.\\n* Hoyle et al. (2023) Alexander Hoyle, Rupak Sarkar, Pranav Goel, and Philip Resnik. 2023. Natural language decompositions of implicit content enable better text representations.\\n* Hoyle et al. (2022) Alexander Miserlis Hoyle, Pranav Goel, Rupak Sarkar, and Philip Resnik. 2022. Are neural topic models broken? In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 5321-5344, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n* Hsieh and Shannon (2005) Hsiu-Fang Hsieh and Sarah E Shannon. 2005. Three approaches to qualitative content analysis. _Qualitative health research_, 15(9):1277-1288.\\n* Hu et al. (2014) Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2014. Interactive topic modeling. _Machine learning_, 95:423-469.\\n* Huang et al. (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting. In _Findings of Empirical Methods in Natural Language Processing_.\\n* Hubert and Arabie (1985) Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. _Journal of classification_, 2:193-218.\\n* Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7b.\\n* Korencic et al. (2021) Damir Korencic, Strahl Ristov, Jelena Repar, and Jan Snajder. 2021. A Topic Coverage Approach to Evaluation of Topic Models. _IEEE Access_, 9:123280-123312. ArXiv:2012.06274 [cs].\\n* Kyngas (2020) Helvi Kyngas. 2020. Inductive content analysis. _The application of content analysis in nursing science research_, pages 13-21.\\n* Kyngas et al. (2014)Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In _Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies_, pages 1536-1545.\\n* Li et al. (2023) Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023. Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation.\\n* Liu and Healey (2023) Sengjie Liu and Christopher G Healey. 2023. Abstractive summarization of large document collections using gpt. _arXiv preprint arXiv:2310.05690_.\\n* McCallum (2002) Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. Http://www.cs.umass.edu/ mccallum/mallet.\\n* Mei et al. (2007) Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In _Knowledge Discovery and Data Mining_.\\n* Meila (2007) Marina Meila. 2007. Comparing clusterings--an information based distance. _Journal of multivariate analysis_, 98(5):873-895.\\n* May 3, 2018, Conference Track Proceedings_. OpenReview.net.\\n* Mimno et al. (2007) David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In _Proceedings of the 24th international conference on Machine learning_, pages 633-640.\\n* Newman et al. (2010) David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In _Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics_, pages 100-108.\\n* Nikolenko et al. (2017) Sergey I Nikolenko, Sergei Koltcov, and Olessia Koltsova. 2017. Topic modelling for qualitative studies. _Journal of Information Science_, 43(1):88-102.\\n* OpenAI (2023) OpenAI. 2023. Gpt-4 technical report.\\n* Paisley et al. (2014) John Paisley, Chong Wang, David M Blei, and Michael I Jordan. 2014. Nested hierarchical dirichlet processes. _IEEE transactions on pattern analysis and machine intelligence_, 37(2):256-270.\\n* Poursabzi-Sangdeh et al. (2016) Forough Poursabzi-Sangdeh, Jordan Boyd-Graber, Leah Findlater, and Kevin Seppi. 2016. Alto: Active learning with topic overviews for speeding label induction and document labeling. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1158-1169.\\n* Ramage et al. (2011) Daniel Ramage, Christopher D. Manning, and Susan Dumais. 2011. Partially labeled topic models for interpretable text mining. In _Proceedings of the 17th ACP SIGKDD international conference on Knowledge discovery and data mining_, pages 457-465, San Diego California USA. ACM.\\n* Rand (1971) William M Rand. 1971. Objective criteria for the evaluation of clustering methods. _Journal of the American Statistical association_, 66(336):846-850.\\n* Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\\n* Shannon (1948) Claude Elwood Shannon. 1948. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423.\\n* Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\\n* Sia et al. (2020) Suzanna Sia, Ayush Dalmia, and Sabrina J. Mielke. 2020. Tired of topic models? clusters of pretrained word embeddings make for fast and good topics too! In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1728-1736, Online. Association for Computational Linguistics.\\n* Srivastava and Sutton (2017) Akash Srivastava and Charles Sutton. 2017. Autoencoding variational inference for topic models. _arXiv preprint arXiv:1703.01488_.\\n* Strehl and Ghosh (2002) Alexander Strehl and Joydeep Ghosh. 2002. Cluster ensembles--a knowledge reuse framework for combining multiple partitions. _Journal of machine learning research_, 3(Dec):583-617.\\n* Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents.\\n* Tai et al. (2023) Robert H Tai, Lillian R Bentley, Xin Xia, Jason M Sitt, Sarah C Fankhauser, Ana M Chicas-Mosier, and Barnas G Monteith. 2023. Use of large language models to aid analysis of textual data. _bioRxiv_, pages 2023-07.\\n* Teh et al. (2006) Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical Dirichlet Processes. _Journal of the American Statistical Association_, 101(476):1566-1581.\\n* Thompson and Mimno (2020) Laure Thompson and David Mimno. 2020. Topic Modeling with Contextualized Word Representation Clusters. ArXiv:2010.12626 [cs].\\n* Tron et al. (2016)Danya F Vears and Lynn Gillam. 2022. Inductive content analysis: A guide for beginning qualitative researchers. _Focus on Health Professional Education: A Multi-disciplinary Journal_, 23(1):111-127.\\n* Vinh et al. (2009) Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2009. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In _Proceedings of the 26th annual international conference on machine learning_, pages 1073-1080.\\n* Viswanathan et al. (2023) Vijay Viswanathan, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, and Graham Neubig. 2023. Large language models enable few-shot clustering.\\n* Wallach et al. (2009) Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking lda: Why priors matter. _Advances in neural information processing systems_, 22.\\n* Wan and Wang (2016) Xiaojun Wan and Tianming Wang. 2016. Automatic labeling of topic models using text summaries. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2297-2305, Berlin, Germany. Association for Computational Linguistics.\\n* Wang et al. (2023) Zihan Wang, Jingbo Shang, and Ruiqi Zhong. 2023. Goal-driven explainable clustering via language descriptions.\\n* Zhang et al. (2023) Yuwei Zhang, Zihan Wang, and Jingbo Shang. 2023. ClusterLM: Large Language Models as a Guide for Text Clustering. ArXiv:2305.14871 [cs].\\n* Zhao (2005) Ying Zhao. 2005. _Criterion Functions for Document Clustering_. Ph.D. thesis, University of Minnesota, USA. AAI3180039.\\n\\n## Appendix A Appendix\\n\\n### Seed Topics\\n\\nIn all levels of topic generation, we included a list of seed topics along with corresponding examples. These seed topics can be tailored to specific end use cases to achieve a descriptive and granular output topics. In this section, we offer suggestions on crafting seed topics and provide accompanying examples to illustrate the impacts of varying seed topics.\\n\\n1. The format of the seed topics should resemble your desirable output format, which contains a correct topic level and a concise topic label.\\n\\n_Example 1: Using long seed topic formatting results in long topic labels._\\n\\nSeed topics:\\n\\n[1] Trade Policies\\n\\n[1] Agricultural Policies\\n\\nGenerated topics (using GPT-4):\\n\\n[1] Social Security Policies\\n\\n_[1] Food Safety Policies_\\n\\n_Example 2: Using short seed topic formatting._\\n\\nSeed topics:\\n\\n[1] Trade\\n\\n[1] Agriculture\\n\\nGenerated topics (using GPT-4):\\n\\n[1] Social Security\\n\\n[1] Food Safety\\n\\n2. The specificity of the seed topics should match the degree of granularity you would expect in a given hierarchy level.\\n\\n_Example 1: Using abstract seed topics for level 2._\\n\\nSeed topics:\\n\\n[1] Music & Performing Arts\\n\\n[2] Music\\n\\n[2] Performing Arts\\n\\nGenerated topics (using GPT-4):\\n\\n[1] Engineering & Technology\\n\\n[2] Civil & Transportation Engineering\\n\\n[2] Electrical Engineering\\n\\n_Example 2: Using detailed seed topic for level 2._\\n\\n_Seed topics:\\n\\n[1] Music & Performing Arts_\\n\\n[2] Albums\\n\\n[2] Songs\\n\\nGenerated topics (using GPT-4):\\n\\n[1] Engineering & Technology\\n\\n[2] Road & Highway Systems\\n\\n[2] Microwave Technology\\n\\n### Semantic Matching between Ground Truth and topics generated by LDA and TopicGPT\\n\\nTable 7 shows the proportion of misaligned topics determined by each annotator.\\n\\n## Appendix B Prompts\\n\\nSee below for the prompt templates that we used for generating, refining and assigning topics.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l} \\\\hline \\\\hline\\n**Prompt template for generating first-level/flat topics** \\\\\\\\ \\\\hline \\\\multicolumn{2}{l}{You will receive a document and a set of top-level topics from a topic hierarchy. Your task is to identify generalizable topics within the document that can act as top-level topics in the hierarchy. If any relevant topics are missing from the provided set, please add them. Otherwise, output the existing top-level topics as identified in the document. [Top-level topics] [Seed topics (containing “[1] Trade” in this example)] [Examples] [Example 1: Adding “[1] Agriculture” Document: \\\\\\\\ Saving Essential American Saliros Act or SEAS Act - Amends the Moving Ahead for Progress in the 21st Century Act (MAP-21) to repeal the Act’s repeal of the agricultural export requirements that: (1) 25 of the gross tonnage of certain agricultural commodities or their products exported each fiscal year be transported on U.S. commercial vessels, and (2) the Secretary of Transportation (DOT) finance any increased ocean freight charges incurred in the transportation of such items. Revives and reinstates those repealed requirements to read as if they were never repealed. \\\\\\\\ \\\\multicolumn{2}{l}{Your response: [1] Agriculture: Mentions policies relating to agricultural practices and products.} \\\\\\\\ \\\\multicolumn{2}{l}{Example 2: Duplicate “[1] Trade”, returning the existing topic} \\\\\\\\ \\\\multicolumn{2}{l}{Document: \\\\\\\\ Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.} \\\\\\\\ \\\\multicolumn{2}{l}{Your response: [1] Trade: Mentions the exchange of capital, goods, and services.} \\\\\\\\ \\\\multicolumn{2}{l}{[Instructions]} \\\\\\\\ \\\\multicolumn{2}{l}{Step 1: Determine topics mentioned in the document.} \\\\\\\\ \\\\multicolumn{2}{l}{- The topic labels must be as GENERALIZABLE as possible. They must not be document-specific.} \\\\\\\\ \\\\multicolumn{2}{l}{- The topics must reflect a SINGLE topic instead of a combination of topics.} \\\\\\\\ \\\\multicolumn{2}{l}{- The new topics must have a level number, a short general label, and a topic description.} \\\\\\\\ \\\\multicolumn{2}{l}{- The topics must be broad enough to accommodate future subtopics.} \\\\\\\\ \\\\multicolumn{2}{l}{Step 2: Perform ONE of the following operations:} \\\\\\\\ \\\\multicolumn{2}{l}{1. If there are already duplicates or relevant topics in the hierarchy, output those topics and stop here.} \\\\\\\\ \\\\multicolumn{2}{l}{2. If the document contains no topic, return ”None”.} \\\\\\\\ \\\\multicolumn{2}{l}{3. Otherwise, add your topic as a top-level topic. Stop here and output the added topic(s). DO NOT add any additional levels.} \\\\\\\\ \\\\multicolumn{2}{l}{[Document]} \\\\\\\\ \\\\multicolumn{2}{l}{Please ONLY return the relevant or modified topics at the top level in the hierarchy.} \\\\\\\\ \\\\multicolumn{2}{l}{[Your response]} \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 7: Proportion of misaligned topics in LDA’s outputs as well as TopicGPT’s unrefined and refined outputs (rounded to one decimal place). The lowest values in each column are bolded.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l l l l l l l l l l l l} \\\\hline \\\\hline \\\\multirow{2}{*}{Dataset} & \\\\multirow{2}{*}{Setting} & \\\\multicolumn{4}{c}{Out-of-scope} & \\\\multicolumn{4}{c}{Missing} & \\\\multicolumn{4}{c}{Repeated} & \\\\multicolumn{4}{c}{Total} \\\\\\\\ \\\\cline{3-14}  & & P1 & P2 & P3 & P1 & P2 & P3 & P1 & P2 & P3 & P1 & P2 & P3 \\\\\\\\ \\\\hline \\\\hline \\\\multirow{4}{*}{Wiki} & LDA (\\\\(k=31\\\\)) & 74.2 & 19.4 & 45.2 & 3.2 & 6.5 & 3.2 & 9.7 & 6.5 & 19.4 & 87.1 & 32.4 & 67.8 \\\\\\\\  & Unrefined (\\\\(k=31\\\\)) & 67.7 & 25.8 & **22.7** & **0.0** & **0.0** & **0.0** & 3.2 & **0.0** & **0.0** & 70.9 & 25.8 & 22.7 \\\\\\\\  & Refined (\\\\(k=22\\\\)) & **59.1** & **9.1** & **22.7** & **0.0** & **0.0** & **0.0** & **0.0** & **0.0** & **0.0** & **59.1** & **9.1** & **22.7** \\\\\\\\ \\\\hline \\\\multirow{4}{*}{Bills} & LDA (\\\\(k=79\\\\)) & 64.6 & 50.6 & 53.2 & 2.5 & 2.5 & 1.3 & 16.5 & 22.8 & 26.6 & 83.6 & 75.9 & 81.0 \\\\\\\\  & Unrefined (\\\\(k=79\\\\)) & 72.2 & 62.0 & 60.8 & **1.3** & **2.5** & **0.0** & 6.3 & 2.5 & 2.5 & 79.8 & 67.1 & 63.3 \\\\\\\\ \\\\cline{1-1}  & Refined (\\\\(k=24\\\\)) & **45.8** & **4.2** & **33.3** & 4.2 & 4.2 & 4.2 & **0.0** & **0.0** & **0.0** & **50.0** & **8.3** & **37.5** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 8: Prompt template for generating broad, high-level topics that can either serve as a flat list of standalone topics or as the first tier of a hierarchical topic taxonomy. The designation of ’first-level’ ensures these topics are sufficiently expansive to cover the topic distribution of the entire dataset. In practice, users need to modify the components highlighted in red (seed topic list and document) as well as tailor the examples to their specific dataset.\\n\\n**Prompt template for generating second-level subtopics**\\n\\nYou will receive a branch from a topic hierarchy along with some documents assigned to the top-level topic of that branch. Your task is to identify generalizable second-level topics that can act as subtopics to the top-level topic in the provided branch. Add your topic(s) if they are missing from the provided branch. Otherwise, return the existing relevant or duplicate topics.\\n\\n[Example] (Return \"[2] Exports\" (new) and \"[2] Tariff\" (existing) as the subtopics of \"[1] Trade\" (provided).)\\n\\nTopic branch:\\n\\n[1] Trade\\n\\n[2] Tariff\\n\\n[2] Foreign Investments\\n\\nDocument 1:\\n\\nExport Promotion Act of 2012 - Amends the Export Enhancement Act of 1988 to revise the duties of the Trade Promotion Coordinating Committee (TPCC). Requires the TPCC to: (1) make a recommendation for the annual unified federal trade promotion budget to the President; and (2) review the proposed fiscal year budget of each federal agency with responsibility for export promotion or export financing activities before it is submitted to the Office of Management and Budget (OMB) and the President, when (as required by current law) assessing the appropriate levels and allocation of resources among such agencies in support of such activities.\\n\\nDocument 2:\\n\\nAmends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.\\n\\nDocument 3:\\n\\nSecuring Exports Through Coordination and Technology Act - Amends the Foreign Relations Authorization Act, Fiscal Year 2003. Requires carriers obliged to file Shipper\\'s Export Declarations to file them through AES (either directly or through intermediaries) before items are exported from any U.S. port, unless the Secretary of Commerce grants an exception.\\n\\nYour response:\\n\\n[1] Trade\\n\\n[2] Exports (Document: 1, 3): Mentions export policies on goods.\\n\\n[2] Tariff (Document: 2): Mentions tax policies on imports or exports of goods.\\n\\n[Instructions]\\n\\nStep 1: Determine PRIMARY and GENERALIZABLE topics mentioned in the documents.\\n\\n- The topics must be generalizable among the provided documents.\\n\\n- Each topic must not be too specific so that it can accommodate future subtopics.\\n\\n- Each topic must reflect a SINGLE topic instead of a combination of topics.\\n\\n- Each top-level topic must have a level number and a short label. Second-level topics should also include the original documents associated with these topics (separated by commas) as well as a short description of the topic.\\n\\n- The number of topics proposed cannot exceed the number of documents provided.\\n\\nStep 2: Perform ONE of the following operations:\\n\\n1. If the provided top-level topic is specific enough, DO NOT add any subtopics. Return the provided top-level topic.\\n\\n2. If your topic is duplicate or relevant to the provided topics, DO NOT add any subtopics. Return the existing relevant topic.\\n\\n3. If your topic is relevant to and more specific than the provided top-level topic, add your topic as a second-level topic. DO NOT add to the first or third level of the hierarchy.\\n\\n[Topic branch]\\n\\n{Topic}\\n\\n[Documents]\\n\\nDO NOT add first- or third-level topics.\\n\\n[Your response]\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l} \\\\hline \\\\hline\\n**Prompt template for generating second-level subtopics** \\\\\\\\ \\\\hline You will receive a branch from a topic hierarchy along with some documents assigned to the top-level topic of that branch. Your task is to identify generalizable second-level topics that can act as subtopics to the top-level topic in the provided branch. Add your topic(s) if they are missing from the provided branch. Otherwise, return the existing relevant or duplicate topics. \\\\\\\\ \\\\hline\\n[Example] (Return \"[2] Exports\" (new) and \"[2] Tariff\" (existing) as the subtopics of \"[1] Trade\" (provided).) \\\\\\\\ Topic branch:\\n\\n[1] Trade\\n\\n[2] Tariff\\n\\n[2] Foreign Investments \\\\\\\\ Document 1: Export Promotion Act of 2012 - Amends the Export Enhancement Act of 1988 to revise the duties of the Trade Promotion Coordinating Committee (TPCC). Requires the TPCC to: (1) make a recommendation for the annual unified federal trade promotion budget to the President; and (2) review the proposed fiscal year budget of each federal agency with responsibility for export promotion or export financing activities before it is submitted to the Office of Management and Budget (OMB) and the President, when (as required by current law) assessing the appropriate levels and allocation of resources among such agencies in support of such activities. \\\\\\\\ Document 2: Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram. \\\\\\\\ Document 3: Securing Exports Through Coordination and Technology Act - Amends the Foreign Relations Authorization Act, Fiscal Year 2003. Requires carriers obliged to file Shipper’s Export Declarations to file them through AES (either directly or through intermediaries) before items are exported from any U.S. port, unless the Secretary of Commerce grants an exception. \\\\\\\\ \\\\hline Your response:\\n\\n[1] Trade\\n\\n[2] Exports (Document: 1, 3): Mentions export policies on goods.\\n\\n[2] Tariff (Document: 2): Mentions tax policies on imports or exports of goods.\\n\\n[Instructions]\\n\\nStep 1: Determine PRIMARY and GENERALIZABLE topics mentioned in the documents.\\n\\n- The topics must be generalizable among the provided documents.\\n\\n- Each topic must not be too specific so that it can accommodate future subtopics.\\n\\n- Each topic must reflect a SINGLE topic instead of a combination of topics.\\n\\n- Each top-level topic must have a level number and a short label. Second-level topics should also include the original documents associated with these topics (separated by commas) as well as a short description of the topic.\\n\\n- The number of topics proposed cannot exceed the number of documents provided.\\n\\nStep 2: Perform ONE of the following operations:\\n\\n1. If the provided top-level topic is specific enough, DO NOT add any subtopics. Return the provided top-level topic.\\n\\n2. If your topic is duplicate or relevant to the provided topics, DO NOT add any subtopics. Return the existing relevant topic.\\n\\n3. If your topic is relevant to and more specific than the provided top-level topic, add your topic as a second-level topic. DO NOT add to the first or third level of the hierarchy.\\n\\n[Topic branch]\\n\\n{Topic}\\n\\n[Documents]\\n\\nDO NOT add first- or third-level topics.\\n\\n[Your response]\\n\\n\\\\end{table}\\nTable 9: Prompt template for generating second-level subtopics in a topic hierarchy. In practice, users need to modify the components highlighted in red (generated topic branch and associated documents) as well as tailor the examples to their specific dataset. This prompt can be further accommodate subtopic generation for lower-level by changing the topic level in the examples and instructions.\\n\\n**Prompt template for refining (merging) topics**\\n\\nYou will receive a list of topics that belong to the same level of a topic hierarchy. Your task is to merge topics that are paraphrases or near duplicates of one another. Return \"None\" if no modification is needed.\\n\\n[Examples]\\n\\nExample 1: Merging topics (\"[1] Employer Taxes\" and \"[1] Employment Tax Reporting\" into \"[1] Employment Taxes\")\\n\\nTopic List:\\n\\n[1] Employer Taxes: Mentions taxation policy for employer\\n\\n[1] Employment Tax Reporting: Mentions reporting requirements for employer\\n\\n[1] Immigration: Mentions policies and laws on the immigration process\\n\\n[1] Voting: Mentions rules and regulation for the voting process\\n\\nYour response:\\n\\n[1] Employment Taxes: Mentions taxation report and requirement for employer ([1] Employer Taxes, [1] Employment Tax Reporting)\\n\\nExample 2: Merging topics ([2] Digital Literacy and [2] Telecommunications into [2] Technology)\\n\\n[2] Mathematics: Discuss mathematical concepts, figures and breakthroughs.\\n\\n[2] Digital Literacy: Discuss the ability to use technology to find, evaluate, create, and communicate information.\\n\\n[2] Telecommunications: Mentions policies and regulations related to the telecommunications industry, including wireless service providers and consumer rights.\\n\\nYour response\\n\\n[2] Technology: Discuss technology and its impact on society. ([2] Digital Literacy, [2] Telecommunications)\\n\\n[Rules]\\n\\n- Each line represents a topic, with a level indicator and a topic label.\\n\\n- Perform the following operations as many times as needed:\\n\\n- Merge relevant topics into a single topic.\\n\\n- Do nothing and return \"None\" if no modification is needed.\\n\\n- When merging, the output format should contain a level indicator, the updated label and description, followed by the original topics.\\n\\n[Topic List]\\n\\n{Topics}\\n\\nOutput the modification or \"None\" where appropriate. Do not output anything else.\\n\\n[Your response]\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l} \\\\hline\\n**Prompt template for refining (merging) topics** \\\\\\\\ \\\\hline You will receive a list of topics that belong to the same level of a topic hierarchy. Your task is to merge topics that are paraphrases or near duplicates of one another. Return \"None\" if no modification is needed. \\\\\\\\ \\\\hline\\n[Examples] \\\\\\\\ Example 1: Merging topics (”[1] Employer Taxes” and ”[1] Employment Tax Reporting” into ”[1] Employment Taxes”) \\\\\\\\ Topic List:\\n\\n[1] Employer Taxes: Mentions taxation policy for employer \\\\\\\\\\n[1] Employment Tax Reporting: Mentions reporting requirements for employer \\\\\\\\\\n[1] Immigration: Mentions policies and laws on the immigration process \\\\\\\\\\n[1] Voting: Mentions rules and regulation for the voting process \\\\\\\\ \\\\hline Your response:\\n\\n[1] Employment Taxes: Mentions taxation report and requirement for employer ([1] Employer Taxes, [1] Employment Tax Reporting) \\\\\\\\ Example 2: Merging topics ([2] Digital Literacy and [2] Telecommunications into [2] Technology) \\\\\\\\\\n[2] Mathematics: Discuss mathematical concepts, figures and breakthroughs. \\\\\\\\\\n[2] Digital Literacy: Discuss the ability to use technology to find, evaluate, create, and communicate information. \\\\\\\\\\n[2] Telecommunications: Mentions policies and regulations related to the telecommunications industry, including wireless service providers and consumer rights. \\\\\\\\ \\\\hline Your response \\\\\\\\\\n[2] Technology: Discuss technology and its impact on society. ([2] Digital Literacy, [2] Telecommunications) \\\\\\\\\\n[Rules] \\\\\\\\ - Each line represents a topic, with a level indicator and a topic label. \\\\\\\\ - Perform the following operations as many times as needed: \\\\\\\\ - Merge relevant topics into a single topic. \\\\\\\\ - Do nothing and return ”None” if no modification is needed. \\\\\\\\ - When merging, the output format should contain a level indicator, the updated label and description, followed by the original topics. \\\\\\\\ \\\\hline\\n[Topic List] \\\\\\\\ \\\\{Topics\\\\} \\\\\\\\ Output the modification or ”None” where appropriate. Do not output anything else. \\\\\\\\ \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 10: Prompt template for generating second-level subtopics in a topic hierarchy. In practice, users need to modify the components highlighted in red (seed topic list and document) as well as tailor the examples to their specific dataset.\\n\\n**Prompt template for assigning topics**\\n\\nYou will receive a document and a topic hierarchy. Assign the document to the most relevant topics the hierarchy. Then, output the topic labels, assignment reasoning and supporting quotes from the document. DO NOT make up new topics or quotes.\\n\\nHere is the topic hierarchy:\\n\\n{tree}\\n\\n[Examples]\\n\\nExample 1: Assign \"[1] Agriculture\" to the document\\n\\nDocument:\\n\\nSaving Essential American Saliors Act or SEAS Act - Amends the Moving Ahead for Progress in the 21st Century Act (MAP-21) to repeal the Act\\'s repeal of the agricultural export requirements that: (1) 25% of the gross tonnage of certain agricultural commodities or their products exported each fiscal year be transported on U.S. commercial vessels, and (2) the Secretary of Transportation (DOT) finance any increased ocean freight charges incurred in the transportation of such items.\\n\\nYour response:\\n\\n[1] Agriculture: Mentions changes in agricultural export requirements (\"...repeal of the agricultural export requirements that...\")\\n\\nExample 2: Assign \"[2] Tariff\" to the document\\n\\nDocument:\\n\\nAmends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.\\n\\nYour response:\\n\\n[1] Trade\\n\\n[2] Tariff: Mentions adjusting the taxation on mixtures containing Fluopyram (\"...suspend temporarily the duty on mixtures containing Fluopyram.\")\\n\\n[Instructions]\\n\\n1. Topic labels must be present in the provided topic hierarchy. You MUST NOT make up new topics.\\n\\n2. The quote must be taken from the document. You MUST NOT make up quotes.\\n\\n3. If the assigned topic is not on the top level, you must also output the path from the top-level topic to the assigned topic.\\n\\n[Document]\\n\\nDouble check that your assignment exists in the hierarchy!\\n\\n[Your response]\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l} \\\\hline \\\\hline You will receive a document and a topic hierarchy. Assign the document to the most relevant topics the hierarchy. Then, output the topic labels, assignment reasoning and supporting quotes from the document. DO NOT make up new topics or quotes. \\\\\\\\ Here is the topic hierarchy: \\\\\\\\ {tree} \\\\\\\\ \\\\hline\\n[Examples] \\\\\\\\ Example 1: Assign \"[1] Agriculture\" to the document \\\\\\\\ Document: \\\\\\\\ Saving Essential American Saliors Act or SEAS Act - Amends the Moving Ahead for Progress in the 21st Century Act (MAP-21) to repeal the Act’s repeal of the agricultural export requirements that: (1) 25% of the gross tonnage of certain agricultural commodities or their products exported each fiscal year be transported on U.S. commercial vessels, and (2) the Secretary of Transportation (DOT) finance any increased ocean freight charges incurred in the transportation of such items. \\\\\\\\ Your response: \\\\\\\\ [1] Agriculture: Mentions changes in agricultural export requirements (\"...repeal of the agricultural export requirements that...\") \\\\\\\\ Example 2: Assign \"[2] Tariff\" to the document \\\\\\\\ Document: \\\\\\\\ Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram. \\\\\\\\ Your response: \\\\\\\\\\n[1] Trade \\\\\\\\ [2] Tariff: Mentions adjusting the taxation on mixtures containing Fluopyram (\"...suspend temporarily the duty on mixtures containing Fluopyram.\") \\\\\\\\\\n[Instructions] \\\\\\\\\\n1. Topic labels must be present in the provided topic hierarchy. You MUST NOT make up new topics. \\\\\\\\\\n2. The quote must be taken from the document. You MUST NOT make up quotes. \\\\\\\\\\n3. If the assigned topic is not on the top level, you must also output the path from the top-level topic to the assigned topic. \\\\\\\\\\n[Document] \\\\\\\\ \\\\hline Double check that your assignment exists in the hierarchy! \\\\\\\\ [Your response] \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 11: Prompt template for assigning topics to a given document in the corpus. In practice, users need to modify the components highlighted in red as well as tailor the examples to the specific dataset (can be reused from prompts in previous stages). Users can also modify the prompt to strictly assign to one topic.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{|c|l|l|l|l|} \\\\hline Dataset & Document & Ground-truth label & Initial label & Reassigned labels \\\\\\\\ \\\\hline Bills & Securing Health for Ocean Resources and Environment Act or the SHORE Act - Requires the Under Secretary for Oceans and Atmosphere to: (1) review the National Oceanic and Atmospheric Administration’s (NOMA) capacity to respond to oil spills; (2) be responsible for developing and maintaining oil spill trajectory modeling capabilities... & Labor & Transportation Safety & Labor; Transportation Safety \\\\\\\\ \\\\hline Bills & Driver Fatigue Prevention Act. This bill amends the Fair Labor Standards Act of 1938 to apply its maximum hours requirements to over-the-road bus drivers. & Labor & Transportation Safety & Laboratory; Transportation Safety \\\\\\\\ \\\\hline Bills & Defense Travel Simplification Act of 2007 - Requires the Secretary of Defense to: (1) redesignate the Defense Travel System as the Defense Travel Accounting and Voucher Processing System; and (2) establish an intra-agency task force to recommend measures to streamline and simplify the commercial travel system... & Domestic Committee & State and Local Govern-Veterans \\\\\\\\ \\\\hline Bills & Amends the Internal Revenue Code to allow until June 30, 2010: (1) a first-time homebuyer tax credit for all purchasers of a principal residence (not just first-time homebuyers); and (2) a refundable tax credit, up to \\\\$3,000, for the costs of refinancing a principal residence. & Labor & Transportation Safety & Taxation; Transportation Safety \\\\\\\\ \\\\hline Bills & Amends the Internal Revenue Code to extend through 2014 the equalization of the exclusion from gross income for employer-provided mass transit and parking benefits. & Engineering and Technologies & History and Politics; Efficiency & History and Engineering and Technology \\\\\\\\ \\\\hline Wiki & Colonel Cyrus Kurtz Holliday (April 3, 1826 – March 29, 1900) was one of the founders of the township of Topeka, Kansas, in the mid 19th century; and was Adjutant General of Kansas during the American Civil War. The title Colonel, however, was honorary. He was the first president of the Atchison, Topeka and Santa Fe Railway, as well as one of the railroad’s directors for nearly 40 years, up to 1900... & Language and literature & Art and Craftman-ship & Art and Craftman-making; Literature and Writing \\\\\\\\ \\\\hline Wiki & Jack Banham Coggins (July 10, 1911 – January 30, 2006) was an artist, author, and illustrator. He is known in the United States for his oil paintings, which focused predominantly on marine subjects. He is also known for his books on space travel, which were both authored and illustrated by Coggins. Besides his own works, Coggins also provided illustrations for advertisements and magazine covers and articles.... & Warfare & History and Politics & Military and Warfare \\\\\\\\ \\\\hline Wiki & HMS Belfast is a museum ship, originally a Royal Navy light cruiser, permanently moored in London on the River Thames and operated by the Imperial War Museum. Construction of Belfast, the first Royal Navy ship to be named after the capital city of Northern Ireland, and one of ten Town-class cruisers, began in December 1936... & Art and architecture & Engineering and Technology & Engineering and Technology \\\\\\\\ \\\\hline Wiki & The Grand Street Bridge was a double-leaf deck-girder bascule bridge in Bridgeport, Connecticut, United States, that spanned the Pequounock River and connected Grand Street and Artic Street. It was one of three movable bridges planned by the City of Bridgeport in 1916 at the request of the War Department during World War I..... & Agriculture, food, and drink & Business and Finance & Food and Cooking; Business Business and Finance; Advertising and Marketing \\\\\\\\ \\\\hline Wiki & Burger King Specialty Sandwiches = The Burger King Specialty Sandwiches are a line of sandwiches developed by the international fast-food restaurant chain Burger King in 1978 and introduced in 1979 as part of a new product line designed to expand Burger King ’s menu with more sophisticated, adult oriented fare beyond hamburgers.... & Agriculture, food, and drink & Business and Finance & Food and Cooking; Business Business and Finance; Advertising and Marketing \\\\\\\\ \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 12: Error analysis on five examples from each of Bills and Wiki datasets. Documents are truncated for ease of viewing.', '# REAL: Resilience and Adaptation using\\n\\nLarge Language Models on Autonomous Aerial Robots\\n\\nAndrea Tagliabue\\n\\nLIDS\\n\\nMIT\\n\\natagliab@mit.edu\\n\\n&Kota Kondo1\\n\\nLIDS\\n\\nMIT\\n\\nkkondo@mit.edu\\n\\n&Tong Zhao1\\n\\nEECS\\n\\nMIT\\n\\nttzhao@mit.edu\\n\\n&Mason Peterson1\\n\\nLIDS\\n\\nMIT\\n\\nmasonbp@mit.edu\\n\\n&Claudius T. Tewari\\n\\nLIDS\\n\\nMIT\\n\\ncttewari@mit.edu\\n\\n&Jonathan P. How\\n\\nLIDS\\n\\nMIT\\n\\njhow@mit.edu\\n\\nFootnote 1: Equal Contribution. The authors are with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology. Work funded by the Air Force Office of Scientific Research MURI FA9550-19-1-0386.\\n\\n###### Abstract\\n\\nLarge Language Models (LLMs) pre-trained on internet-scale datasets have shown impressive capabilities in code understanding, synthesis, and general purpose question-and-answering. Key to their performance is the substantial prior knowledge acquired during training and their ability to reason over extended sequences of symbols, often presented in natural language. In this work, we aim to harness the extensive long-term reasoning, natural language comprehension, and the available prior knowledge of LLMs for increased resilience and adaptation in autonomous mobile robots. We introduce **REAL**, an approach for **RE**silience and **A**daptation using **LLMs. REAL provides a strategy to employ LLMs as a part of the mission planning and control framework of an autonomous robot. The LLM employed by REAL provides (i) a source of prior knowledge to increase resilience for challenging scenarios that the system had not been explicitly designed for; (ii) a way to interpret natural-language and other log/diagnostic information available in the autonomy stack, for mission planning; (iii) a way to adapt the control inputs using minimal user-provided prior knowledge about the dynamics/kinematics of the robot. We integrate REAL in the autonomy stack of a real multirotor, querying onboard an offboard LLM at \\\\(0.1\\\\)-\\\\(1.0\\\\) Hz as part the robot\\'s mission planning and control feedback loops. We demonstrate in real-world experiments the ability of the LLM to reduce the position tracking errors of a multirotor under the presence of (i) errors in the parameters of the controller and (ii) unmodeled dynamics. We also show (iii) decision making to avoid potentially dangerous scenarios (e.g., robot oscillates) that had not been explicitly accounted for in the initial prompt design.\\n\\nL  LLMs, Adaptive Control, Aerial Robotics\\n\\n## 1 Introduction\\n\\nCreating mission planning and control capabilities that are adaptive and resilient to unexpected scenarios has been a large area of research in recent years. Adaptive control has enabled exceptional performance when addressing specific failure modes, such as disturbances [1, 2, 3], incorrect models/parameters [4, 5, 6], or poor controller tuning [7, 8, 9]. However, these approaches work best under a pre-defined set of failure modalities, and/or leverage accurate models/prior knowledgeabout the robot from the designer. Similarly, complex missions for autonomous mobile robots have been successfully managed through sophisticated state machines and mission planners [10; 11; 12; 13; 14; 15]. However, these planners often need to reason over a pre-defined set of states and/or observation models, identified through extensive efforts.\\n\\nRecently, foundational models, and especially Large Language Models (LLMs) pre-trained on internet-scale datasets [16; 17; 18], have demonstrated impressive performance on a variety of reasoning problems, including natural language [19; 20] and mathematics [21]. This performance stems in part from the large size of their training data (e.g., internet-scale), which embeds a vast amount of prior knowledge into the weights of the model. Additionally, their billion-of-parameters model architectures enable reasoning over long sequences of symbols, causing them to be a natural choice for any problem that involves generating a sequence of symbols. The embedded prior knowledge and extended sequential reasoning capabilities have led to LLMs finding increased application in task planning and motion planning for robotics. In this context, the main focus of recent work has been planning for manipulation [22; 23; 24; 25; 26; 27; 28], using human input as a task specification and outputting calls to manipulator APIs. However, their potential has not been explored for _combined adaptive low-level control_ and mission _planning/reasoning_ on agile autonomous aerial robots.\\n\\nIn this paper, we present REAL (Resilience and Adaptation using LLMs), a method for harnessing the capabilities of LLMs for mission-planning and low-level adaptive control of an agile mobile robot, a multirotor UAV. Our work leverages the LLM\\'s embedded prior knowledge of the UAV\\'s dynamics to create adaptation throughout the stack, including altering low-level parameters, producing commands to better track trajectories, and making mission-level decisions. REAL uses a single human-crafted prompt (zero-shot prompting) to define minimal robot specification and task/controller API available to the LLM. Then, during real-time deployment, REAL receives as input a set of natural-language and numerical signals available onboard the multirotor which capture mission-relevant information at different timescales, including information about the dynamics of the robot and its high-level mission objectives. Then, based on these automatically generated robot prompts, REAL chooses the most suitable control/mission planning APIs that are executed by the robot. This feedback loop operates at about \\\\(0.1\\\\)-\\\\(1.0\\\\) Hz, while the prompts are processed remotely using the OpenAI GPT-4 API.\\n\\nFigure 1: Schematic representation of our approach. We use a Large Language Model (LLM) to achieve adaptation from low-level to mission-scale decision-making, enabling resilience across the different dynamics and components of our autonomous system. Despite the ability of the LLM to apply changes to the low-level control of the multirotor, the LLM does not need to know specific details about the autonomy stack of the platform, instead using embedded prior knowledge to make decisions. In experiments, we demonstrate that an LLM queried from onboard the robot can reason about the current and desired state of the robot, deciding which corrective actions to apply to the control input to achieve the desired objective.\\n\\nWe evaluate REAL in hardware experiments, exposing our multirotor to a variety of performance-lowering conditions. Some of these conditions require low-level adaptation (e.g., by adjusting the commanded thrust), while others which require mission-level adaptation (e.g., by improving controller tuning or conducting an emergency landing). Through these experiments, we also show that the behavior of an LLM as an adaptive controller can be modified by the use of natural language cues (e.g. using stronger language when the instructions given to the LLM are safety-related and important to follow). Additionally, we show that although the LLM cannot be queried at a high rate (up to \\\\(1\\\\) Hz), it can still process and make suggestions in response to high-frequency information by making use of algorithmically pre-processed information.\\n\\n**Contributions**:\\n\\n* We present REAL, an approach to leverage prior knowledge in LLMs to enable online adaptation and decision-making across different time scales and components (_low-level controller_, _mission planner_) of the autonomy stack of an aerial robot. We leverage zero-shot prompting, and we show that our prompt requires minimal knowledge of the robot\\'s model/dynamics and mission specifications.\\n* We present hardware experiments, demonstrating adaptation and decision-making capabilities using LLMs that improve the position control performance of the robot or regulate the safety of the mission. To the best of our knowledge, this is the first time that such capabilities have been demonstrated on an _aerial_ robot.\\n\\n## 2 Related Works\\n\\n### Adaptation at Mission-Scale and Low-Level Control\\n\\n**Adaptive Control.** There are two broad categories of methods used for adaptive control: direct and indirect methods. Indirect methods aim at explicitly estimating models or parameters, which are leveraged in model-based controllers, such as MPC [29], to improve performance. Model/parameter identification include filtering techniques [30; 31], disturbance observers [32; 33; 34], set-membership identification methods [35; 36] or learning-based methods [5; 37]. Direct methods, instead, develop policy updates that improve a certain performance metric. These updates are often done to drive the behavior of the system towards that of a reference model, with the updates themselves involving changing the shallow layers of the DNN policy [1; 38; 39]. Other strategies include learning a policy update strategy offline using meta-learning [2; 40], or using parametric adaptation laws such as \\\\(\\\\mathcal{L}_{1}\\\\) adaptive control [3]. While many adaptive control strategies are able to improve low-level performance in real-world systems, these strategies often fail when mission-level adaptation is required. Our work provides the first example of a system that exhibits both low-level and mission-level adaptation.\\n\\n**Uncertainty-Aware Mission Planning.** Mission-level adaptation is usually achieved with robot autonomy. State-of-the-art approaches to autonomy have involved the use of finite-state machines and uncertainty-aware planners [13; 15; 41], enabling autonomy on many systems, from a single autonomous car [10] to multiple heterogeneous robots [11; 12]. While these methods achieve impressive performance in the coordination of multiple autonomous systems, they do not leverage the internet-scale prior knowledge in LLMs that may be helpful in making decisions under natural language-based observations that are available at the system level (e.g., log), nor they require to specify observations models/mission states.\\n\\n### Foundational Models in Robotics\\n\\nFoundational models have quickly found a variety of applications in robotics, with a focus on planning from natural language instructions. [42] develop a holistic foundational model that performs perception, planning, and control using internet-scale datasets to train a multi-modal foundational model that, given a goal described in natural language, can use video feed to plan and execute a sequence of commands to achieve that goal. [22] decodes an LLM weighted by skill affordances [43] from value functions to generate feasible plans for robots. [23, 24, 25] all translate a high-level instruction into a plan expressed in code, which is then executed by the robot. [26] uses an LLM to translate a natural language planning problem into a domain-specific language, then runs a classical planner to solve the problem. [28] uses an LLM to generate a plan in natural language, then uses a similarity measure to translate the plan from natural language into one executable by the robot. [27] uses closed-loop environmental feedback to improve the performance of using an LLM for planning and control in manipulation tasks. While existing methods have focused on task-level planning (especially for manipulation), our work leverages LLMs for combined mission management and low-level control on an agile aerial robot, demonstrating a new domain of possible deployment of LLMs-based reasoning.\\n\\n## 3 Approach\\n\\n### Approach Overview\\n\\nThe objective of our work is to design a decision-making and adaptation mechanism that uses LLMs to enable successful and resilient mission execution in autonomous systems despite the presence of uncertainties and potentially unplanned/unexpected failures that may happen across different levels of the autonomy stack. The considered autonomous system is a multirotor, whose objective consists in reaching and hovering at a desired position. During the mission, the robot is subject to uncertainties, such as model errors or wind, that may cause a critical mission failure. The robot needs to understand how to mitigate the effect of those uncertainties and autonomously decide whether to abort the mission if the effects of those uncertainties cannot be corrected, based on a natural-language specified risk tolerance. Our approach, summarized in Fig. 1, leverages an LLM to trigger adaptive/resilient behaviors in the mission planning and control stack, taking as input available signals, pre-defined error codes, and natural-language based logs and error messages. In the following sections, we define in detail the interface between an existing autonomy stack and the LLM.\\n\\n### Autonomy Stack\\n\\n#### 3.2.1 Controller\\n\\nWe consider a multirotor controlled by a cascaded position and attitude controller. The employed position controlled is based on a Linear-Quadratic Regulator (LQR) that uses a hover-linearized model (derived from [44]) of an attitude-controlled robot of the form \\\\(\\\\mathbf{x}_{t+1}=\\\\mathbf{A}\\\\mathbf{x}_{t}+\\\\mathbf{B}\\\\mathbf{u}_{t}\\\\), where \\\\(\\\\mathbf{x}\\\\in\\\\mathbb{R}^{8}\\\\) is the state and \\\\(\\\\mathbf{u}\\\\in\\\\mathbb{R}^{3}\\\\) is the control input. The state is \\\\(\\\\mathbf{x}=[_{W}\\\\mathbf{P}^{\\\\top},_{W}\\\\mathbf{v}^{\\\\top},_{I}\\\\phi_{,I}\\\\theta]^{\\\\top}\\\\), where \\\\({}_{W}\\\\mathbf{P}^{\\\\top}\\\\in\\\\mathbb{R}^{3}\\\\) and \\\\({}_{W}\\\\mathbf{v}^{\\\\top}\\\\in\\\\mathbb{R}^{3}\\\\) represent, respectively, the position and velocity expressed in a world frame \\\\(W\\\\). The quantities \\\\({}_{I}\\\\phi\\\\) and \\\\({}_{I}\\\\theta\\\\) denote the attitude of the robot, expressed as roll and pitch Euler angles in a gravity-aligned, yaw-fixed frame \\\\(I\\\\), whose x-axis is aligned with the world reference frame \\\\(W\\\\).\\n\\nWe define the control input \\\\(\\\\mathbf{u}\\\\) to be: \\\\(\\\\mathbf{u}=[_{I}\\\\phi_{\\\\text{cmd},I}\\\\theta_{\\\\text{cmd},\\\\delta f_{\\\\text{cmd}}} ]^{\\\\top}\\\\), where \\\\(\\\\delta f_{\\\\text{cmd}}\\\\) denotes the linearized commanded thrust, and \\\\({}_{I}\\\\phi_{\\\\text{cmd}}\\\\) and \\\\({}_{I}\\\\theta_{\\\\text{cmd}}\\\\) are the commanded roll and pitch. These commands are executed by a cascaded attitude controller.\\n\\nThe control input is computed via:\\n\\n\\\\[\\\\mathbf{u}_{t}=\\\\mathbf{\\\\tilde{u}}_{t}+\\\\mathbf{K}(\\\\mathbf{x}_{t}-\\\\mathbf{x}_{t }^{\\\\text{ref}})+\\\\delta\\\\mathbf{u}_{t}, \\\\tag{1}\\\\]\\n\\nwhere \\\\(\\\\mathbf{\\\\tilde{u}}_{t}\\\\) represents the nominal command at hover and \\\\(\\\\mathbf{x}_{t}^{\\\\text{ref}}\\\\) a desired reference trajectory computed by the mission-level planner. \\\\(\\\\mathbf{K}\\\\) is a linear gain matrix, obtained by solving the Discrete Algebraic Riccati Equation (DARE) using the linearized model \\\\(\\\\mathbf{A}\\\\), \\\\(\\\\mathbf{B}\\\\) and given positive-definite tuning matrices \\\\(\\\\mathbf{R}\\\\) and \\\\(\\\\mathbf{Q}\\\\). Key to this work, the additive control input \\\\(\\\\delta\\\\mathbf{u}_{t}\\\\) represents an adaptive term that will be controlled by the LLM based on descriptions of the state of the system (error codes, logs), enabling adaptation at low-level control.\\n\\n#### 3.2.2 Mission Planner\\n\\nThe mission is managed by a finite state machine (FSM) that contains desired initial/terminal position setpoints, and timed transitions in between the desired states. Once a desired position is selected, the FSM generates reference trajectories (position, velocity) that are tracked via the position controller Eq. (1). Every state inside the FSM is connected to an emergency_landing action that leads to a safe state (on the ground below the robot), which can be triggered by the LLM upon seeing what it determines is sufficient cause to terminate the mission.\\n\\n### Prompt Design And Interface with the Autonomy Stack\\n\\nIn this section, we present the strategy to interface the control and mission/trajectory planning stack with the LLM. We use an approach inspired by [23], i.e., we leverage Python-based syntax to define the possible failure modes in the autonomy stack, as well as the description of a set of function callbacks (API) in our control framework available to execute corrective actions. However, our approach differs from [23], as we provide additional natural language instructions to express mission-level goals and trade-offs, i.e., the willingness to risk to continue the mission when complications arise, versus aborting the mission. Additionally, we limit the potentially dangerous execution of automatically generated Python code by providing the LLM with the instruction to call a set of pre-defined Python APIs. Last, in our experiments, the LLM is connected in a closed feedback loop with the rest of the autonomy stack, without human intervention beyond the initial prompt design.\\n\\n**Code Color Convention:** Note that throughout this work we use the following convention: green denotes the initial prompt to the LLM; this prompt is hand-crafted by a human and is loaded at the start of the mission; grey denotes the query automatically generated by the autonomous system, and blue denotes the reply from the LLM, closing the feedback loop.\\n\\nOur prompt begins with the following sentence:\\n\\n```\\nInput://initial(final-1) \\\\#InsidethecodebaseofmymultirotorIfoundthefollowingpythoncode\\n```\\n\\nThis sentence introduces the LLM to the Python-based syntax that will be used next to list possible mission failures/issues, requirements, and actions available, and additionally introduces the LLM to the type of platform it needs to control. Next, we introduce a list of possible, easy-to-monitor, state-based errors and failures:\\n\\n```\\nInput://initial(final-1) \\\\#InsidethecodebaseofmymultirotorIfoundthefollowingpythoncode\\n```\\n\\nThis sentence introduces the LLM to the Python-based syntax that will be used next to list possible mission failures/issues, requirements, and actions available, and additionally introduces the LLM to the type of platform it needs to control. Next, we introduce a list of possible, easy-to-monitor, state-based errors and failures:\\n\\n```\\nInput://initial(final-1) \\\\#InsidethecodebaseofmymultirotorIfoundthefollowingpythoncode\\n```\\n\\nThese failures can be easily detected, and their corresponding number is fed as input to the LLM. Additionally, we found that the LLM is more easily able to interpret failures expressed in natural language than failures expressed in numerical signals (i.e., current trajectory tracking errors). The corresponding error codes are generated by comparing the current trajectory tracking error \\\\(\\\\mathbf{p}_{t}-\\\\mathbf{p}_{t}^{\\\\text{des}}\\\\), and by triggering an issue on the corresponding axis if the error exceeds a predefined threshold.\\n\\nNext, we define a new fictitious Python variable and function call that computes the possible failures:\\n\\n```\\nInput://initial(final-1) \\\\#checkcurrentfailureusingcheckfailure.outputsalistofpossiblefailures,forexample[2,3].\\n#andastringwithadditionalinformation.Thestringmaybeempty.\\n#Examplecurrentfailure:(L2,3),\\'positionerror=[0.1,-0.1,1.5]\\') currentfailures=checkfailures()\\n```\\n\\nAs in [23], we make use of Python comments to provide contextual information on the output of the function call and describe additional inputs that we will be feeding into the LLM, using the secondterm in tuple of current_failure. This extra input can be used to provide descriptive error messages or other information that is not known/does not need to be specified a priori, providing additional flexibility in the type of information that we can feed to the LLM.\\n\\nNext, we provide the LLM with information about the system-level actions (APIs) that the LLM can select:\\n\\n```\\n#possiblefailuremitigationstrategies fromcontrollerimport(\\n#modifycontrolinput increase,trust,decrease_trust,accel_positive_x,accel_negative_x,accel_positive_y,accel_negative_y,\\n#Mission-leveldecisions emergency_landing_do_nothing,\\n#Controllertuning=useusLQR tune_controller_by_decreasing_the_cost_of_actuation_usage,\\n#time_controller_by_increasing_the_cost_of_actuation_usage,\\n#time_controller_by_increasing_penalty_on_position_errors,\\n#time_controller_by_decreasing_penalty_on_position_errors, }\\n```\\n\\nThese actions correspond to changes in the control inputs or to events in the mission planner. More specifically, increase_thrust and decrease_thrust increase/decrease an adaptive term in the control input, while accel_positive_... and accel_negative_... produce accelerations along an axis by increasing/decreasing the extra roll/pitch setpoints by a pre-specified amount. Additionally, tune_controller_... updates the corresponding part of the weight matrices \\\\(\\\\mathbf{R}\\\\) and \\\\(\\\\mathbf{Q}\\\\) of the position controller Eq. (1); the corresponding DARE is solved onboard and the resulting gain matrix updates \\\\(\\\\mathbf{K}\\\\).\\n\\nTowards the end of the prompt, we switch back to natural language to provide mission specifications:\\n\\n```\\n#Uncertainties(\\\\(\\\\mathbf{G}\\\\))\\n#fromnowon,Iprovideyouwiththevalueoftvariable\\'current_failure\", andyouroutputneedstobeyourbestguessofthefunctionnamesinthepythonlist\\n#list_of_function_names_to_be_executed_right_now\".\\n#instance,youroutput.[\"emergency_landing\"],\"low_battery_voltage\",\\n#becausethedronesanhardwoveitisssarertoland\"\\n#Trytothinklikeadromecontrolengineer.\\n```\\n\\nThis prompt specifies the output that we expect from the LLM (a list of names of functions the controller can execute). It additionally includes two elements that can help the LLM reason about its choice of actions, and a brief and long explanation of the issue. Following best prompting practices, we also encourage the LLM to role-play, i.e., thinking like a \"drone control engineer\".\\n\\nIn addition, we discourage the LLM from outputtingplannedfutureactionsandencouragebrevity in its explanations, by adding the following lines in the initial prompt:\\n\\n```\\n#Uncertainties(\\\\(\\\\mathbf{G}\\\\))\\n#DoNToutputfunctionnamestobecalledinthefuture,butaccountforpastproblemstocomeup withyourguessofthefunctionsin\"list_of_function_names_to_be_executed_right_now\".\\n```\\n\\nLast, we further make the LLM aware of the possibility of taking emergency landings:\\n\\n```\\n#Uncertainties(\\\\(\\\\mathbf{G}\\\\))\\n#Ifproblemspersist,donotbesitatetoemergencyland.\\n#ifyouractionsdonottakethedesiredeffect,youmustperformanemergencylanding.\\n```\\n\\nWe note that omitting these sentences was making the LLM less prone to trigger an emergency landing, while exaggerating the need to emergency land (e.g., using \"MUST\" instead of \"must\") made the LLM more prone to immediately trigger an emergency landing, potentially providing a natural-language avenue to specify willingness to take risks in an autonomous system.\\n\\n## 4 Evaluation\\n\\n### Implementation Details\\n\\nWe perform real-world experiments by deploying REAL on a multirotor. The multirotor is equipped with an Intel(r) NUC(tm) 10. Our system operates in real-time on the NUC(tm) 10, driven by the Intel(r) Core(tm) i7-10710U Processor. All planning, control, and state estimation, which merges IMU data with a motion capture system, are executed onboard, except the LLM, which is queried from onboard, receiving replies generated via the OpenAI GPT-4 API. The UAV connects to the internet over Wi-Fi and queries the LLM at as high of a rate as possible. This results in the LLM running at from \\\\(0.1\\\\) to \\\\(1.0\\\\) Hz, depending on network latency and API usage.\\n\\n### Low-Level Adaptation and Controller Auto-Tuning\\n\\nIn this experiment, we evaluate the ability of the LLM to perform low-level adaptive control and decision-making, by adjusting the control input of our UAV based on the issues reported to the LLM. To introduce tracking error, we purposefully use the wrong value for the mass parameter while synthesizing the controller; the value of the mass parameter used is about \\\\(15\\\\%\\\\) of the robot\\'s true mass, resulting in a large altitude error. We then deploy the UAV in a mission that consists of taking off, following a figure-eight trajectory, and then landing. We repeat the experiment two times, with the difference that in the first experiment, to study the choice that the LLM would make absent this parameter, we removed the tune_controller_by_... in Initial Prompt (Part 4) that API call.\\n\\nFig. 1(a) shows the result without tuning the API call tune_controller_by_... in Initial Prompt (Part 4), highlighting that throughout the mission, REAL succeeds at improving the altitude improve the trajectory tracking through the duration of the experiment (about \\\\(100\\\\,\\\\mathrm{s}\\\\)) by repeatedly calling the increase_thrust in Initial Prompt (Part 4), obtaining an altitude tracking error within \\\\(30\\\\,\\\\mathrm{c}\\\\mathrm{m}\\\\). Fig. 1(b) shows the results on the same trajectory, but with the tune_controller_by_... in Initial Prompt (Part 4) added back to the prompt. In this experiment, the LLM calls both commands, as shown by the conversation between the LLM and the robot during the experiment (please see Conversation 1 in the Appendix).\\n\\nThe altitude tracking error resulting from this experiment is within only \\\\(10\\\\,\\\\mathrm{c}\\\\mathrm{m}\\\\). This shows that the LLM is capable of selecting multiple mission-relevant low-level control APIs, without providing detailed prior instructions on its choice, and whose usage is beneficial at improving the outcome of the mission.\\n\\nFigure 2: REAL’s successful altitude (\\\\(z\\\\)-axis) adaptation during a figure eight trajectory (\\\\(0.25\\\\,\\\\mathrm{m}\\\\mathrm{/}\\\\mathrm{s}\\\\)) with an incorrect mass parameter used in the controller (\\\\(15\\\\,\\\\mathrm{\\\\char 37}\\\\) error from nominal value). The top figure shows the experiment performed when the API calls tune_controller_by_... removed from the initial prompt, and the bottom figure shows the same experiment with the tune_controller_by_... APIs added back. These results illustrate that REAL can successfully interpret the system-provided error messages, and call useful APIs. Specifically, REAL achieves adaptation in the top plot by calling the increase_thrust API, successfully converging to the desired altitude. In the secondary scenario (bottom figure), REAL calls a combination of tune_controller_by_... and increase_thrust APIs, triggering changes in the controller matrix \\\\(\\\\mathbf{K}\\\\). This results in faster/better convergence to the desired trajectory.\\n\\n### Low-Level Adaptation to Unmodeled Dynamics\\n\\nThis experiment presents a more challenging scenario that highlights REAL\\'s ability to correct for unmodeled dynamics. To test this, we place a large extra mass on the end of one of the multirotor\\'s arms, creating an unmodeled torque disturbance. For brevity, the prompt is not shown, but adaptation is triggered by repeatedly selecting the expected API calls: increase_thrust, accel_negative_y and accel_negative_x. Fig. 2(a) shows the hardware experiment results of the LLM successfully reasoning how to eliminate error along each of its axes. Note that in this earlier experiment, roll and pitch torque function names were used in the prompt to control acceleration along the \\\\(y\\\\) and \\\\(x\\\\) axes respectively. We later found that the LLM was more consistent when using the commands for requesting acceleration in \\\\(x\\\\) and \\\\(y\\\\) directions directly.\\n\\n### Mission-scale Decision Making via Unsafe State Detection and Automatic Mission Abortion\\n\\nThe purpose of this experiment is two-fold: (1) test the LLM\\'s ability to make critical mission-level decisions in the event that unforeseen circumstances cause the UAV to lose control and (2) test the LLM\\'s ability to process additional information that was not in the original prompt.\\n\\nSince the LLM is only able to make adaptive corrections at a slow rate, we would expect it to call for an emergency landing in the event of loss-of-control. To simulate a loss-of-control event, we apply large external disturbances to the robot by pulling it via a rope, causing large oscillations along the \\\\(y\\\\) axis, as seen in Fig. 2(b). Onboard the drone, we implement a module that stores a buffer of positions and uses a Fast Fourier Transform (FFT) to evaluate the maximum amplitude of any frequency content. If the maximum frequency content is above a threshold, this information is outputted to the log received by the LLM, as shown in the conversation history in Conversation 2 (please see Conversation 2 in Appendix). Although the LLM was not told it would receive any information about oscillations nor what to do if it did receive this information, it was able to make the critical decision to select the emergency_land API when it recognized that the UAV was in a dangerous condition via the provided log message \"DANGEROUS oscillations...\". Fig. 2(b) additionally highlights the LLM\\'s ability to correct the altitude errors of the UAV, caused by an additional \\\\(15\\\\%\\\\) mass mismatch, as visible before the interaction begins.\\n\\nFigure 3: Real’s ability to perform low-level adaptation and trigger mission-relevant decision.\\n\\nConclusions\\n\\nWe have presented REAL, a method to utilize LLMs for zeros-shot cross-stack adaptation and autonomy on mobile robots. The method works by leveraging LLMs\\' natural language understanding, its ability to reason over long sequences, and its embedded prior knowledge of the robot\\'s model/dynamics. Our evaluation in hardware experiments has demonstrated promising first-steps towards low-level adaptation and increased mission-level resilience. In the future we would like to further extend this framework to more complex, multi-robot autonomous systems, where more complex reasoning is required to diagnose and recover from failures.\\n\\n## References\\n\\n* [1] G. Joshi and G. Chowdhary. Deep model reference adaptive control. In _2019 IEEE 58th Conference on Decision and Control (CDC)_, pages 4601-4608. IEEE, 2019.\\n* [2] S. M. Richards, N. Azizan, J.-J. Slotine, and M. Pavone. Adaptive-control-oriented meta-learning for nonlinear systems. _Robotics: Science and Systems (RSS)_, 2021.\\n* [3] N. Hovakimyan, C. Cao, E. Kharisov, E. Xargay, and I. M. Gregory. \\\\(\\\\mathcal{L}_{1}\\\\) adaptive control for safety-critical systems. _IEEE Control Systems Magazine_, 31(5):54-104, 2011.\\n* [4] L. Ljung. System identification. In _Signal analysis and prediction_, pages 163-173. Springer, 1998.\\n* [5] A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots. _Robotics: Science and Systems (RSS)_, 2021.\\n* [6] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Sparse identification of nonlinear dynamics with control (sindyc). _IFAC-PapersOnLine_, 49(18):710-715, 2016.\\n* [7] A. Loquercio, A. Saviolo, and D. Scaramuzza. Autotune: Controller tuning for high-speed flight. _IEEE Robotics and Automation Letters_, 7(2):4432-4439, 2022.\\n* [8] A. Marco, P. Hennig, J. Bohg, S. Schaal, and S. Trimpe. Automatic lqr tuning based on gaussian process global optimization. In _2016 IEEE international conference on robotics and automation (ICRA)_, pages 270-277. IEEE, 2016.\\n* [9] F. Berkenkamp, A. P. Schoellig, and A. Krause. Safe controller optimization for quadrotors with gaussian processes. In _2016 IEEE international conference on robotics and automation (ICRA)_, pages 491-496. IEEE, 2016.\\n* [10] A. Furda and L. Vlacic. Towards increased road safety: Real-time decision making for driverless city vehicles. In _2009 IEEE International Conference on Systems, Man and Cybernetics_, pages 2421-2426, 2009. doi:10.1109/ICSMC.2009.5346395.\\n* [11] M. Tranzatto, M. Dharmadhikari, L. Bernreiter, M. Camurri, S. Khattak, F. Mascarich, P. Pfreundschuh, D. Wisht, S. Zimmermann, M. Kulkarni, V. Reijgwart, B. Casseau, T. Homberger, P. D. Petris, L. Ott, W. Tubby, G. Waibel, H. Nguyen, C. Cadena, R. Buchanan, L. Wellhausen, N. Khedekar, O. Andersson, L. Zhang, T. Miki, T. Dang, M. Mattamala, M. Montenegro, K. Meyer, X. Wu, A. Briod, M. Mueller, M. Fallon, R. Siegwart, M. Hutter, and K. Alexis. Team cterberus wins the darpa subterranean challenge: Technical overview and lessons learned, 2022.\\n* [12] A. Agha, K. Otsu, B. Morrell, D. D. Fan, R. Thakker, A. Santamaria-Navarro, S.-K. Kim, A. Bouman, X. Lei, J. Eadlund, M. F. Ginting, K. Ebadi, M. Anderson, T. Pailevanian, E. Terry, M. Wolf, A. Tagliabue, T. S. Vaquero, M. Palieri, S. Tepsuporn, Y. Chang, A. Kalantari, F. Chavez, B. Lopez, N. Funabiki, G. Miles, T. Touma, A. Buscicchio, J. Tordesillas, N. Alatur, J. Nash, W. Walsh, S. Jung, H. Lee, C. Kanellakis, J. Mayo, S. Harper, M. Kaufmann, A. Dixit, G. Correa, C. Lee, J. Gao, G. Merewether, J. Maldonado-Contreras, G. Salhotra, M. S. D. Silva, B. Ramtoula, Y. Kubo, S. Fakoorian, A. Hatteland, T. Kim, T. Bartlett, A. Stephens, L. Kim, C. Bergh, E. Heiden, T. Lew, A. Cauligi, T. Heywood, A. Kramer, H. A. Leopold, C. Choi, S. Daftry, O. Toupet, I. Wee, A. Thakur, M. Feras, G. Beltrame, G. Nikolakopoulos, D. Shim, L. Carlone, and J. Burdick. Nebula: Quest for robotic autonomy in challenging environments; team costar at the darpa subterranean challenge, 2021.\\n* [13] A.-A. Agha-Mohammadi, S. Chakravorty, and N. M. Amato. Firm: Sampling-based feedback motion-planning under motion uncertainty and imperfect measurements. _The International Journal of Robotics Research_, 33(2):268-304, 2014.\\n* [14] S. Omidshafiei, A.-A. Agha-Mohammadi, C. Amato, and J. P. How. Decentralized control of partially observable markov decision processes using belief space macro-actions. In _2015 IEEE international conference on robotics and automation (ICRA)_, pages 5962-5969. IEEE, 2015.\\n\\n* Bouman et al. [2020] A. Bouman, M. F. Ginting, N. Alatur, M. Palieri, D. D. Fan, T. Touma, T. Pailevanian, S.-K. Kim, K. Otsu, J. Burdick, et al. Autonomous spot: Long-range autonomous exploration of extreme environments with legged locomotion. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2518-2525. IEEE, 2020.\\n* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.\\n* Touvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\\n* Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n* Drori et al. [2022] I. Drori, S. Zhang, R. Shuttleworth, L. Tang, A. Lu, E. Ke, K. Liu, L. Chen, S. Tran, N. Cheng, R. Wang, N. Singh, T. L. Patti, J. Lynch, A. Shporer, N. Verma, E. Wu, and G. Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. _Proceedings of the National Academy of Sciences_, 119(32):e2123433119, 2022. doi: 10.1073/pnas.2123433119. URL [https://www.pnas.org/doi/abs/10.1073/pnas.2123433119](https://www.pnas.org/doi/abs/10.1073/pnas.2123433119).\\n* Lewkowycz et al. [2022] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models, 2022.\\n* Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.\\n* Ahn et al. [2022] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quianbao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In _arXiv preprint arXiv:2204.01691_, 2022.\\n* Liang et al. [2023] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9493-9500. IEEE, 2023.\\n* Huang et al. [2023] S. Huang, Z. Jiang, H. Dong, Y. Qiao, P. Gao, and H. Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model, 2023.\\n* Wu et al. [2023] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. Tidybot: Personalized robot assistance with large language models, 2023.\\n* Liu et al. [2023] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering large language models with optimal planning proficiency, 2023.\\n* Huang et al. [2023] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter.\\n\\nInner monologue: Embodied reasoning through planning with language models. In _arXiv preprint arXiv:2207.05608_, 2022.\\n* [28] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. _arXiv preprint arXiv:2201.07207_, 2022.\\n* [29] F. Borrelli, A. Bemporad, and M. Morari. _Predictive control for linear and hybrid systems_. Cambridge University Press, 2017.\\n* [30] J. Svacha, J. Paulos, G. Loianno, and V. Kumar. Imu-based inertia estimation for a quadrotor using newton-euler dynamics. _IEEE Robotics and Automation Letters_, 5(3):3861-3867, 2020.\\n* [31] V. Wuest, V. Kumar, and G. Loianno. Online estimation of geometric and inertia parameters for multirotor aerial vehicles. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 1884-1890. IEEE, 2019.\\n* [32] A. Tagliabue, A. Paris, S. Kim, R. Kubicek, S. Bergbreiter, and J. P. How. Touch the wind: Simultaneous airflow, drag and interaction sensing on a multirotor. In _RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1645-1652.\\n* [33] A. Tagliabue, M. Kamel, R. Siegwart, and J. Nieto. Robust collaborative object transportation using multiple MAVs. _The International Journal of Robotics Research_, 38(9):1020-1044, 2019.\\n* [34] C. D. McKinnon and A. P. Schoellig. Unscented external force and torque estimation for quadrotors. In _2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 5651-5657. IEEE, 2016.\\n* [35] B. T. Lopez. _Adaptive robust model predictive control for nonlinear systems_. PhD thesis, Massachusetts Institute of Technology, 2019.\\n* [36] J. P. How, B. Lopez, P. Lusk, and S. Morozov. Performance analysis of adaptive dynamic tube MPC. page 0785, 2021.\\n* [37] A. Saviolo, J. Frey, A. Rathod, M. Diehl, and G. Loianno. Active learning of discrete-time dynamics for uncertainty-aware model predictive control. _arXiv preprint arXiv:2210.12583_, 2022.\\n* [38] G. Joshi, J. Virdi, and G. Chowdhary. Design and flight evaluation of deep model reference adaptive controller. In _AIAA Scitech 2020 Forum_, page 1336, 2020.\\n* [39] S. Zhou, K. Pereida, W. Zhao, and A. P. Schoellig. Bridging the model-reality gap with lipschitz network adaptation. _IEEE Robotics and Automation Letters_, 7(1):642-649, 2021.\\n* [40] M. O\\'Connell, G. Shi, X. Shi, K. Azizzadenesheli, A. Anandkumar, Y. Yue, and S.-J. Chung. Neural-fly enables rapid learning for agile flight in strong winds. _Science Robotics_, 7(66):eabm6597, 2022. doi:10.1126/scirobotics.abm6597.\\n* [41] S. Omidshafiei, A.-A. Agha-Mohammadi, C. Amato, S.-Y. Liu, J. P. How, and J. Vian. Decentralized control of multi-robot partially observable markov decision processes using belief space macro-actions. _The International Journal of Robotics Research_, 36(2):231-258, 2017.\\n* [42] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In _arXiv preprint arXiv:2307.15818_, 2023.\\n\\n* [43] A. Zeng. _Learning Visual Affordances for Robotic Manipulation_. PhD thesis, Princeton University, 2019.\\n* [44] M. Kamel, M. Burri, and R. Siegwart. Linear vs nonlinear mpc for trajectory tracking applied to rotary wing micro aerial vehicles. _IFAC-PapersOnLine_, 50(1):3463-3469, 2017. ISSN 2405-8963. doi:[https://doi.org/10.1016/j.ifacol.2017.08.849](https://doi.org/10.1016/j.ifacol.2017.08.849). URL [https://www.sciencedirect.com/science/article/pii/S2405896317313083](https://www.sciencedirect.com/science/article/pii/S2405896317313083). 20th IFAC World Congress.\\n\\n## 6 Appendix\\n\\n### Conversation 1\\n\\n```\\n{\"A\":3.92s:Prompt([0],\"A\") {\"A\":3.92s:Responselist_of_function_names_to_be_executed_right_now:[\"do_nothing\"],\"no_issue\",\"Since there are currently no issues detected with the drone, no corrective actions are needed at this time.\" {\"A\":8.47s:Prompt([4,6],\"A\":erroris-0.14,\"A\":) {\"A\":8.47s:Responselist_of_function_names_to_be_executed_right_now:[\"increase_thrust\",\"accel_positive_\"] \"flying_too_low_and_negative_position\",\"The drone is currently flying too low and also has a negative error in the y-axis. Increasing thrust will correct the altitude issue, and accelerating in the positive Y direction will correct the positional error.\" {\"A\":14.03s:Prompt([4,6],\"A\":erroris-0.63,\"A\":erroris-1.18,\"A\":) {\"A\":14.03s:Responselist_of_function_names_to_be_executed_right_now:[\"increase_thrust\",\"accel_positive_\",\"tune_controller_by_increasing_penalty_on_position_errors\"],\"Tlying_too_low_and_large_negative_position\",\"The drone is flying significantly too low and has a large negative error on the Y-axis. It\\'s necessary to increase thrust and accelerate in the positive Y direction to correct these issues. Additionally, the large errors suggest it may be beneficial to tune the controller to penalize position errors more severely.\" {\"A\":=19.0s:Prompt([4,5,7],\"A\":erroris-0.30,\"A\":erroris0.15,\"A\":erroris-0.12,\"A\":\\n{\"A\":=19.0s:Responselist_of_function_names_to_be_executed_right_now:[\"increase_thrust\",\"accel_negative_y\",\"accel_negative_x\"],\"flying_too_low_and_positive_position_errors\",\"The drone is below the desired altitude and has minor positive errors in both X and Y axis. To correct these; it should increase thrust to gain altitude, and decelerate in Y and X directions.\"\\n```\\n\\n### Conversation 2\\n\\n``` {\"A\":6.07s:Prompt([0],\"A\") {\"A\":6.07s:Responselist_of_function_names_to_be_executed_right_now:[\"do_nothing\"] reason: The reported information indicates that there are currently no discernible issues with the drone. Therefore, no actions are necessary at this time. {\"A\":=13.95s:Prompt([4,6],\"A\":erroris-0.63,\"A\":erroris-0.62,\"A\":erroris-0.40,\"A\":) {\"A\":=13.95s:Responselist_of_function_names_to_be_executed_right_now:[\"increase_thrust\",\"tune_controller_by_decreasing_the_cost_of_action_usage\",\"accel_positive_y\",\"accel_positive_x\"] reason: The drone is flying too low and has negative position errors in both X and Y directions, hence, increasing thrust is the first step, along with acceleration in positive X and Y directions to correct the position errors. Besides, the cost of actuation usage should also be decreased to ensure a higher rate of control input changes for faster response. It\\'s preferable to decrease the cost of actuation usage to make the drone respond more swiftly to the control inputs, as there are multiple issues happening simultaneously. {\"A\":=57.48s:Prompt([8,8],\"A\":erroris-0.65,\"A\":erroris-0.28,\"A\":VERY\":OBJECTOSSocillicationson_yaxis.Frequencyis0.67[0],\"A\":amplitudeis0.19[0],\"A\":\\n{\"A\":=57.48s:Responselist_of_function_names_to_be_executed_right_now:[\"emergency_landing\"] reason: The drone has large errors in both X and Y directions, and additionally, is exhibiting dangerous oscillations on the Y-axis. This indicates unstable flight dynamics which could be detrimental to the safety of the operation. An immediate emergency landing should be executed to prevent potential damage or hazards.'], 'file_names': ['2311_01449.mmd', '2311_01403.mmd']}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"args\": {\n",
    "        \"pdf\":[\n",
    "            \"https://arxiv.org/pdf/2311.01449\",\n",
    "            \"https://arxiv.org/pdf/2311.01403\"\n",
    "        ],\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(f'start time:{start_time}')\n",
    "\n",
    "now_time = time.time()\n",
    "\n",
    "model_url = req_url + '/model_predict/'\n",
    "flag = False\n",
    "\n",
    "response = requests.post(model_url, json=params)\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if post params is wrong, it will return error message for each wrong key with all acceptable value types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:request body error[], status: request error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:2023-11-22 11:12:06\n",
      "request error input params error: \n",
      "error 1:type: type_error.list, location: request body, param args input:None, msg: value is not a valid list\n",
      "error 2:type: type_error.list, location: request body, param args input:None, msg: value is not a valid list\n",
      "error 3:type: type_error.str, location: request body, param args input:None, msg: str type expected\n",
      "error 4:type: type_error.path, location: request body, param args input:None, msg: value is not a valid path\n",
      "error 5:type: type_error.dict, location: request body, param proxy input:None, msg: value is not a valid dict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"args\": {\n",
    "        \"pdf\":  {\n",
    "            \"url\": \"https://arxiv.org/pdf/2311.01449\",\n",
    "            \"title\": \"test\"\n",
    "        },\n",
    "},\n",
    "    \"proxy\":222\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(f'start time:{start_time}')\n",
    "\n",
    "now_time = time.time()\n",
    "\n",
    "model_url = req_url + '/model_predict/'\n",
    "flag = False\n",
    "\n",
    "response = requests.post(model_url, json=params)\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### get_summaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器格式为：\n",
    "\n",
    "```python\n",
    "class SummaryRequest(BaseModel):\n",
    "    openai_info:dict = ...\n",
    "    artile_text:Union[str,None] = ...\n",
    "    file_name:Union[str,None] = ...\n",
    "    requests_per_minute:Union[int,None] = None\n",
    "    proxy:Union[dict,None] = None\n",
    "    summary_prompts: Union[dict, str] = gpt_prompts['section summary']\n",
    "    resummry_prompts: Union[dict, str] = gpt_prompts[\"blog summary\"]\n",
    "    ignore_titles:Union[List,None] = [\"references\",\"appendix\"]\n",
    "    acquire_mode: Literal['url', 'openai'] = 'url'\n",
    "    prompt_factor: float = 0.8  # prompt tokens / total tokens\n",
    "    num_processes: int = 3\n",
    "    base_url: str = \"https://openai.huatuogpt.cn/v1\"\n",
    "    init_grid: int = 2\n",
    "    split_mode: str = 'group'\n",
    "    gpt_config: Union[basic_model_info,None] = None\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        if data.get(\"gpt_config\",None) is None:\n",
    "            data[\"gpt_config\"] = basic_model_info()\n",
    "        super().__init__(**data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果格式：\n",
    "```json\n",
    "{\n",
    "    \"titles\": \"str:titles\",\n",
    "    \"authors\": \"str:authors\",\n",
    "    \"affiliations\": \"str:affiliations\",\n",
    "    \"total_resp\": \"str:summary result\",\n",
    "    \"re_respnse\": \"str:blog result or resummary result\"\n",
    "                \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params标准格式：\n",
    "```json\n",
    "    params = {\n",
    "        \"openai_info\": {\n",
    "            \"api_key\": \"sk-xxxx\",\n",
    "            \"ignore_title\":[\"references\",\"appendix\"],\n",
    "            \"  base_url\": \"https://openai.huatuogpt.cn/v1\"\n",
    "        },\n",
    "        \"artile_text\": \"str:article content\",\n",
    "        \"file_name\": \"str:save name of file_name(to identity)\",\n",
    "        \"requests_per_minute\": 3, /* api key is limited by openai with 3/min,if not set None*/\n",
    "        \"proxy\": proxy,\n",
    "        \"acquire_mode\": \"url or openai\",\n",
    "        \"prompt_factor\": \"float:0.8, the max length of prompts / the max_tokens\",\n",
    "        \"summary_prompts\": \"summary_prompts\",\n",
    "        \"resummry_prompts\": \"resummry_prompts\",\n",
    "        \"ignore_title\":[\"references\",\"appendix\"],\n",
    "        \"num_processes\": \"int:3,the number of processing\"\n",
    "        \"base_url\": \"https://openai.huatuogpt.cn/v1 or https://api.openai.com/v1\",\n",
    "        \"init_grid\": \":int =2,The title level of the subgroup\",\n",
    "        \"split_mode\": \"group or half\",\n",
    "        \"gpt_config\": \"{\n",
    "                model: str = 'gpt-3.5-turbo-16k-0613'\n",
    "                temperature:float = 0.9\n",
    "                max_tokens: int = 16385\n",
    "                top_p:float = 1\n",
    "                frequency_penalty:float = 0\n",
    "                presence_penalty:float = 0\n",
    "        }\"\n",
    "\n",
    "    }\n",
    "```\n",
    "\n",
    "简易格式：\n",
    "```json\n",
    "    params = \n",
    "    {\n",
    "        \"openai_info\": {\n",
    "            \"api_key\": \"sk-xxxx\",\n",
    "        },\n",
    "        \"artile_text\": \"str:article content\",\n",
    "        \"file_name\": \"str:save name of file_name(to identity)\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:2023-11-22 11:14:22\n",
      "request url:  http://61.241.103.32:5010/get_summaries/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: process success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titles': '# Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level', 'authors': 'Jingzhe DingYan Cen Xinyuan Wei{yancen,xinyuanwei}@fudan.edu.cn', 'affiliations': 'Columbia Universityjd4001@columbia.eduFudan University', 'total_resp': '\\n## abstract:\\nrequest openai error, SpawnPoolWorker-1:1 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## intro:\\nrequest openai error, SpawnPoolWorker-1:2 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## related work:\\nrequest openai error, SpawnPoolWorker-1:3 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## dataset:\\nrequest openai error, SpawnPoolWorker-1:3 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## experiment:\\nrequest openai error, SpawnPoolWorker-1:2 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## result:\\nrequest openai error, SpawnPoolWorker-1:1 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## conclusion:\\nrequest openai error, SpawnPoolWorker-1:2 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## 7 Ethical Impacts:\\nrequest openai error, SpawnPoolWorker-1:1 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions\\n## Acknowledgements:\\nrequest openai error, SpawnPoolWorker-1:3 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions', 're_respnse': 'request openai error, SpawnPoolWorker-1:4 error code: Request Error: 401 Client Error: Authorization Required for url: https://api.ai-gaochao.cn/v1/chat/completions'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(f'start time:{start_time}')\n",
    "\n",
    "\n",
    "summary_url = req_url + \"/get_summaries/\"\n",
    "print('request url: ', summary_url)\n",
    "text_path = './res/raw_mmd/2309_08182.mmd'\n",
    "with open(text_path,'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "params = {\n",
    "    \"openai_info\":\n",
    "    {\n",
    "        \"api_key\":\"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\"\n",
    "    },\n",
    "    \"artile_text\":text,\n",
    "    \"file_name\" : \"2309_08182.mmd\"\n",
    "}\n",
    "\n",
    "flag = False\n",
    "\n",
    "now_time = time.time()\n",
    "response = requests.post(summary_url, json=params)\n",
    "json_info = custom_response_handler(response)\n",
    "if not \"error\" in json_info:\n",
    "    print(json_info)\n",
    "else:\n",
    "    print(json_info[\"error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
