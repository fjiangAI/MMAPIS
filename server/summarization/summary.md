# Human-like Controllable Image Captioning with Verb-specific Semantic Roles

- Authors: Long Chen<sup>2,3*,</sup>    Zhihong Jiang<sup>1*</sup>    Jun Xiao\({}^{1\dagger}\)   Wei Liu<sup>4</sup>

- Affiliations: <sup>1</sup>Zhejiang University   <sup>2</sup>Tencent AI Lab   <sup>3</sup>Columbia University   <sup>4</sup>Tencent Data Platform<br><br>zjuchenlong@gmail.com, {zju_jiangzhihong, junx}@zju.edu.cn, wl2223@columbia.edu<br><br>denotes equal contributions,   <sup>&dagger;</sup> denotes the corresponding author.

## Abstract
This paper proposes a novel approach for controllable image captioning using Verb-specific Semantic Roles (VSR) as control signals. The VSR consists of a verb and semantic roles, representing the targeted activity and the roles of entities involved. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL component extracts object proposals and calculates similarity scores between semantic roles and proposal sets. The SSP component learns a sequence of sub-roles using sentence-level and role-level SSP. The Role-shift Caption Generation component generates captions by focusing on specific sub-roles and their grounded regions. The proposed approach can be extended to multiple VSRs. The summary lacks specific numerical measurements or results to demonstrate the effectiveness of the approach.
## Introduction
The proposed approach in this paper introduces a novel control signal called Verb-specific Semantic Roles (VSR) for generating controllable image captions. The VSR consists of a verb and semantic roles that represent a targeted activity and the roles of entities involved in the activity. The approach includes three main components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation.

In the GSRL component, an object detector is used to extract object proposals from the image, and a similarity score is calculated between each semantic role and proposal set. The top proposal sets are selected as the grounding results for each sub-role. In the SSP component, a two-level structure learning model is introduced to learn a reasonable sequence of sub-roles. The S-level SSP learns a coarse-grained structure by predicting the sequence of general semantic roles, while the R-level SSP ranks the sub-roles within the same semantic role using a soft permutation matrix.

The Role-shift Caption Generation component utilizes a two-layer LSTM to generate the final caption based on the semantic structure sequence and proposal feature sequence. An adaptive attention mechanism is used to control the shift of sub-roles, and a context vector is obtained to distinguish textual and visual words. The approach is trained separately for each component using various training objectives, including binary cross-entropy loss, cross-entropy loss, mean square loss, and self-critical baseline.

During the inference stage, the GSRL, SSP, and captioning model are sequentially used to generate captions. The framework can also be extended to handle multiple VSRs as control signals.

Overall, the proposed approach introduces VSR as a new control signal for controllable image captioning and combines it with grounded semantic role labeling, semantic structure planning, and role-shift caption generation to achieve human-like controllability in generating captions.
## Related Work
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as control signals for controllable image captioning. It consists of three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL component uses an object detector to extract object proposals and calculate similarity scores between semantic roles and proposal sets. The SSP component learns a reasonable sequence of sub-roles using a sentence-level SSP and a role-level SSP. The Role-shift Caption Generation component generates the final caption by predicting the shift of sub-roles and generating words based on the semantic structure sequence. The approach can be extended to multiple VSRs as control signals. The effectiveness of the proposed approach should be supported by numerical measurements or experimental results.
## Proposed Approach
This paper proposes a novel approach for controllable image captioning using Verb-specific Semantic Roles (VSR) as control signals. The VSR consists of a verb and semantic roles, representing a targeted activity and the roles of entities involved. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL model grounds entities for each role, while the SSP learns human-like semantic structures. The Role-shift Caption Generation model generates captions by focusing on different roles. The proposed approach achieves better controllability and can generate diverse captions. The mathematical formulas in equations 4 to 10 are key technical details that contribute to the academic significance of the approach. Numerical measurements can be used to demonstrate the improvement in controllability achieved by the proposed approach.
### Controllable Caption Generation with VSR
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as a control signal for controllable image captioning. It consists of three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. In GSRL, object proposals are extracted and similarity scores between semantic roles and proposal sets are calculated. SSP learns a reasonable sequence of sub-roles using sentence-level and role-level SSP. Role-shift Caption Generation generates captions by focusing on specific sub-roles and grounded region sets. The training stage involves training each component separately, while the inference stage utilizes the GSRL, SSP, and captioning model sequentially. The summary could be improved by providing more specific details about the techniques used in each component and including quantitative results to highlight the effectiveness of the proposed approach.
### Training and Inference
The paper introduces a novel control signal, Verb-specific Semantic Roles (VSR), for controllable image captioning. VSR consists of a verb and semantic roles that represent the targeted activity and the roles of entities involved. The framework includes Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. GSRL grounds the semantic roles to visual regions using an object detector, while SSP learns the hierarchical semantic structure and ranking of sub-roles. The Role-shift Caption Generation model generates captions based on the semantic structure and grounded regions. The training objectives include BCE loss for GSRL, XE and MSE losses for SSP, and XE and RL losses for the captioning model. In the testing stage, the framework can be extended to multiple VSRs by merging semantic structures and grounded regions.
## Experiments

### Datasets and Metrics
The researchers used two datasets for their experiments: Flickr30K Entities and COCO Entities. The Flickr30K Entities dataset is an extension of the Flickr30K dataset, where each noun phrase in the image descriptions is manually linked to one or more visual regions. It contains 31,000 images, with five captions per image. The COCO Entities dataset, on the other hand, is built upon the COCO dataset and consists of 120,000 images, also with five captions per image. In this dataset, entity annotations are automatically detected and aligned with object proposals of the same class.

However, it should be noted that there are some samples in both datasets (3.26% in COCO Entities and 0.04% in Flickr30K Entities) that do not contain any verbs in their captions. To address this issue, the researchers dropped these samples from the training and testing stages of the experiments. They acknowledge that covering these extreme cases could be a direction for future work.

By using these datasets, the researchers ensured that their proposed approach was evaluated on a diverse range of images and captioning scenarios. The use of multiple captions per image and the manual grounding of entities in the Flickr30K Entities dataset added richness and specificity to the evaluation process.

The choice of these datasets and the careful handling of samples without verbs demonstrate the researchers' commitment to ensuring the scientificity, reliability, and validity of their experimental methods.
### Implementation Details
The researchers used a Faster R-CNN model with ResNet-101 to generate proposals for each image. The model was finetuned on the VG dataset. For COCO Entities, the proposals were grouped based on their detected class labels, while for Flickr30K Entities, each proposal was treated as a separate proposal set.

To annotate the verbs and semantic roles for each caption, a pretrained Semantic Role Labeling (SRL) tool was used. The tool provided ground truth annotations for verbs and semantic roles. A verb dictionary was constructed for each dataset, with sizes of 2,662 for COCO and 2,926 for Flickr30K. A total of 24 types of semantic roles were identified for all verbs.

In the S-level SSP, a multi-head attention mechanism with 8 heads and a transformer with a hidden size of 512 were used. The length of the transformer was set to 10. In the R-level SSP, the maximum number of entities for each role was set to 10.

During the reinforcement learning (RL) training of the captioning model, the CIDEr-D score was used as the training reward.

The researchers provided more detailed parameter settings in the supplementary material to ensure the scientificity, reliability, and validity of their technical methods.
### Evaluation on Controllability
The research conducted an evaluation to assess the controllability and diversity of the proposed framework. The evaluation used VSRs aligned with ground truth captions as control signals. Several baselines were compared, including C-LSTM, C-UpDn, and SCT. 

The evaluation was performed in two settings. In the first setting, given a VSR and grounded visual regions aligned with the ground truth caption, two semantic structures were selected using the SSP, and two diverse captions were generated for each structure. The same set of visual regions was used for the baselines. In the second setting, new VSRs were constructed by randomly sampling a subset of semantic roles for each verb.

Two types of metrics were used for evaluation: accuracy-based metrics and diversity-based metrics. Accuracy-based metrics measured the similarity between the generated captions and the ground truth captions, while diversity-based metrics focused on the language similarity among the generated captions.

The quantitative results showed that the diverse captions generated by the proposed framework had higher accuracy compared to the baselines. The diversity of the captions was slightly behind the SCT baseline, which randomly shuffled regions. The proposed framework achieved a better trade-off between quality and diversity in diverse image captioning.

Visualizations of the generated captions for two images with different VSRs were also provided, demonstrating that the captions effectively followed the given VSRs and exhibited significant diversity based on the variations in VSRs.
## Conclusions & Future Works
The paper introduces Controllable Image Captioning (CIC) and proposes a new control signal called Verb-specific Semantic Roles (VSR) to address the limitations of existing control signals. The VSR-guided framework for CIC consists of a Grounded Semantic Role Labeling (GSRL) model, a Semantic Structure Planner (SSP), and a Role-shift Caption Generation model. Experimental results demonstrate the effectiveness of the framework in generating diverse captions. Future research directions include improving the captioning model, extending VSR to other tasks, and developing a more general framework.
