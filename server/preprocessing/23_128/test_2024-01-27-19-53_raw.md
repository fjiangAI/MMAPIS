![img](img/Abstract_0.png)

![img](img/Abstract_1.png)

![img](img/Abstract_2.png)

![img](img/Abstract_3.png)

![img](img/Abstract_4.png)

![img](img/Abstract_5.png)

![img](img/Abstract_6.png)

![img](img/Abstract_7.png)

## Analysis of Deep Residual Networks


The ResNets developed in [1] are _modularized_ architectures that stack building blocks of the same connecting shape. In this paper we call these blocks "_Residual





[MISSING_PAGE_FAIL:3]

is unlikely for the gradient \(\frac{\partial\mathcal{E}}{\partial\mathbf{x}_{l}}\) to be canceled out for a mini-batch, because in general the term \(\frac{\partial}{\partial\mathbf{x}_{l}}\sum_{i=l}^{L-1}\mathcal{F}\) cannot be always -1 for all samples in a mini-batch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small.
#### Discussions


Eqn.(4) and Eqn.(5) suggest that the signal can be directly propagated from any unit to another, both forward and backward. The foundation of Eqn.(4) is two identity mappings: (i) the identity skip connection \(h(\mathbf{x}_{l})=\mathbf{x}_{l}\), and (ii) the condition that \(f\) is an identity mapping.

These directly propagated information flows are represented by the grey arrows in Fig. 1, 2, and 4. And the above two conditions are true when these grey arrows cover no operations (expect addition) and thus are "clean". In the following two sections we separately investigate the impacts of the two conditions.
## On the Importance of Identity Skip Connections


Let's consider a simple modification, \(h(\mathbf{x}_{l})=\lambda_{l}\mathbf{x}_{l}\), to break the identity shortcut:

\[\mathbf{x}_{l+1}=\lambda_{l}\mathbf{x}_{l}+\mathcal{F}(\mathbf{x}_{l}, \mathcal{W}_{l}),\] (6)

where \(\lambda_{l}\) is a modulating scalar (for simplicity we still assume \(f\) is identity). Recursively applying this formulation we obtain an equation similar to Eqn. (4): \(\mathbf{x}_{L}=(\prod_{i=l}^{L-1}\lambda_{i})\mathbf{x}_{l}+\sum_{i=l}^{L-1}( \prod_{j=i+1}^{L-1}\lambda_{j})\mathcal{F}(\mathbf{x}_{i},\mathcal{W}_{i})\), or simply:

\[\mathbf{x}_{L}=(\prod_{i=l}^{L-1}\lambda_{i})\mathbf{x}_{l}+\sum_{i=l}^{L-1} \mathcal{\hat{F}}(\mathbf{x}_{i},\mathcal
### Experiments on Skip Connections


We experiment with the 110-layer ResNet as presented in [1] on CIFAR-10 [10]. This extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of 3\(\times\)3 convolutional layers) and is challenging for optimization. Our implementation details (see appendix) are the same as [1]. Throughout this paper we report the median accuracy of **5 runs** for each architecture on CIFAR, reducing the impacts of random variations.

Though our above analysis is driven by identity \(f\), the experiments in this section are all based on \(f=\) ReLU as in [1]; we address identity \(f\) in the next section. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons of other variants (Fig. 2 and Table 1) are summarized as follows:

**Constant scaling**. We set \(\lambda=0.5\) for all shortcuts (Fig. 2(b)). We further study two cases of scaling \(\mathcal{F}\): (i) \(\mathcal{F}\) is not scaled; or (ii) \(\mathcal{F}\) is scaled by a constant scalar of \(1-\lambda=0.5\), which is similar to the highway gating [6, 7] but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table 1, 12.35%) is substantially higher than the original ResNet-110. Fig 3(a) shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down.





**Exclusive gating**. Following the Highway Networks [6, 7] that adopt a gating mechanism [5], we consider a gating function \(g(\mathbf{x})=\sigma(\mathrm{W}_{g}\mathbf{x}+b_{g})\) where a transform is represented by weights \(\mathrm{W}_{g}\) and biases \(b_{g}\) followed by the sigmoid function \(\sigma(x)=\frac{1}{1+e^{-x}}\). In a convolutional network \(g(\mathbf{x})\) is realized by a 1\(\times\)1 convolutional layer. The gating function modulates the signal by element-wise multiplication.

We investigate the "exclusive" gates as used in [6, 7] -- the \(\mathcal{F}\) path is scaled by \(g(\mathbf{x})\) and the shortcut path is scaled by \(1-g(\mathbf{x})\). See Fig 2(c). We find that the initialization of the biases \(b_{g}\) is critical for training gated models, and following the guidelines2 in [6, 7], we conduct hyper-parameter search on the initial value of \(b_{g}\) in the range of 0 to -10 with a decrement step of -1 on the training set by cross-validation. The best value (\(-6\) here) is then used for training on the training set, leading to a test result of 8.70% (Table 1), which still lags far behind the ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when \(b_{g}\) is not appropriately initialized.

Footnote 2: See also: people.idsia.ch/~rupesh/very_deep_learning/ by [6, 7].

The impact of the exclusive gating mechanism is two-fold. When \(1-g(\mathbf{x})\) approaches 1, the gated shortcut connections are closer to identity which helps information propagation; but in this case \(g(\mathbf{x})\) approaches 0 and suppresses the function \(\mathcal{F}\). To isolate the effects of the gating functions on the shortcut path alone, we investigate a non-exclusive gating mechanism in the next.

**Shortcut-only gating**. In this case the function \(\mathcal{F}\) is not scaled; only the shortcut path is gated by \(1-g(\mathbf{x})\). See Fig 2(d). The initialized value of \(b_{g}\) is still essential in this case. When the initialized \(b_{g}\) is 0 (so initially the expectation of \(1-g(\mathbf{x})\) is 0.5), the network converges to a poor result of 12.86% (Table 1). This is also caused by higher training error (Fig 3(c)).




When the initialized \(b_{g}\) is very negatively biased (_e.g._, \(-6\)), the value of \(1-g(\mathbf{x})\) is closer to 1 and the shortcut connection is nearly an identity mapping. Therefore, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.

**1\(\times\)1 convolutional shortcut**. Next we experiment with 1\(\times\)1 convolutional shortcut connections that replace the identity. This option has been investigated in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that 1\(\times\)1 shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22%, Table 1) when using 1\(\times\)1 convolutional shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using 1\(\times\)1 convolutional shortcuts.

**Dropout shortcut**. Last we experiment with dropout [11] (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. 2(f)). The network fails to converge to a good solution. Dropout statistically imposes a scale of \(\lambda\) with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation.




### Discussions


As indicated by the grey arrows in Fig. 2, the shortcut connections are the most direct paths for the information to propagate. _Multiplicative_ manipulations (scaling, gating, 1\(\times\)1 convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems.

It is noteworthy that the gating and 1\(\times\)1 convolutional shortcuts introduce more parameters, and should have stronger _representational_ abilities than identity shortcuts. In fact, the shortcut-only gating and 1\(\times\)1 convolution cover the solution space of identity shortcuts (_i.e._, they could be optimized as identity shortcuts). However, their training error is higher than that of identity shortcuts, indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.
## On the Usage of Activation Functions


Experiments in the above section support the analysis in Eqn.(5) and Eqn.(8), both being derived under the assumption that the after-addition activation








is the identity mapping. But in the above experiments \(f\) is ReLU as designed in [1], so Eqn.(5) and (8) are approximate in the above experiments. Next we investigate the impact of \(f\).

We want to make \(f\) an identity mapping, which is done by re-arranging the activation functions (ReLU and/or BN). The original Residual Unit in [1] has a shape in Fig. 4(a) -- BN is used after each weight layer, and ReLU is adopted after BN except that the last ReLU in a Residual Unit is after element-wise addition (\(f\) = ReLU). Fig. 4(b-e) show the alternatives we investigated, explained as following.
### Experiments on Activation


In this section we experiment with ResNet-110 and a 164-layer _Bottleneck_[1] architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a 1\(\times\)1 layer for reducing dimension, a 3\(\times\)3 layer, and a 1\(\times\)1 layer for restoring dimension. As designed in [1], its computational complexity is similar to the two-3\(\times\)3 Residual Unit. More details are in the appendix. The baseline ResNet-164 has a competitive result of 5.93% on CIFAR-10 (Table 2).

**BN after addition**. Before turning \(f\) into an identity mapping, we go the opposite way by adopting BN after addition (Fig. 4(b)). In this case \(f\) involves BN and ReLU. The results become considerably worse than the baseline (Table 2). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the beginning of training (Fig. 6 left).

**ReLU before addition**. A naive choice of making \(f\) into an identity mapping is to move the ReLU before addition (Fig. 4(c)). However, this leads to a _non-negative_ output from the transform \(\mathcal{F}\), while intuitively a "residual" function should take values in \((-\infty,+\infty)\). As a result, the forward propagated signal is monotonically increasing. This may impact the representational ability, and the result is worse (7.84%, Table 2) than the baseline. We expect to have a residual function taking values in \((-\infty,+\infty)\). This condition is satisfied by other Residual Units including the following ones.

**Post-activation or pre-activation?** In the original design (Eqn.(1) and Eqn.(2)), the activation \(\mathbf{x}_{l+1}=f(\mathbf{y}_{l})\) affects _both paths_ in the _next_ Residual Unit: \(\mathbf{y}_{l+1}=f(\mathbf{y}_{l})+\mathcal{F}(f(\mathbf{y}_{l}),\mathcal{W}_ {l+1})\). Next we develop an _asymmetric_ form where an activation \(\hat{f}\) only affects the \(\mathcal{F}\) path: \(\mathbf{y}_{l+1}=\mathbf{y}_{l}+\mathcal{F}(\hat{f}(\mathbf{y}_{l}),\mathcal{W }_{l+1})\), for any \(l\) (Fig. 5 (a) to (b)). By renaming the notations, we have the following form:

\[\mathbf{x}_{l+1}=\mathbf{x}_{l}+\mathcal{F}(\hat{f}(\mathbf{x}_{l}),\mathcal{W }_{l}),.\] (9)

It is easy to see that Eqn.(9) is similar to Eqn.(4), and can enable a backward formulation similar to Eqn.(5). For this new Residual Unit as in Eqn.(9), the new after-addition activation becomes an identity mapping. This design means that if a new after-addition activation \(\hat{f}\) is asymmetrically adopted, it is equivalent to recasting \(\hat{f}\) as the _pre-activation_ of the next Residual Unit. This is illustrated in Fig. 5.

The distinction between post-activation/pre-activation is caused by the presence of the element-wise _addition_. For a plain network that has \(N\) layers, there are \(N-1\) activations (BN/ReLU), and it does not matter whether we think of them as post- or pre-activations. But for branched layers merged by addition, the position of activation matters.

We experiment with two such designs: (i) ReLU-only pre-activation (Fig. 4(d)), and (ii) full pre-activation (Fig. 4(e)) where BN and ReLU are both adopted before weight layers. Table 2 shows that the ReLU-only pre-activation performs very similar to the baseline on ResNet-110/164. This ReLU layer is not used in conjunction with a BN layer, and may not enjoy the benefits of BN [8].

Somehow surprisingly, when BN and ReLU are both used as pre-activation, the results are improved by healthy margins (Table 2 and Table 3). In Table 3 we report results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii) a 110-layer ResNet architecture in which each shortcut skips only 1 layer (_i.e._,








a Residual Unit has only 1 layer), denoted as "ResNet-110(1layer)", and (iv) a 1001-layer bottleneck architecture that has 333 Residual Units (111 on each feature map size), denoted as "ResNet-1001". We also experiment on CIFAR-100. Table 3 shows that our "pre-activation" models are consistently better than the baseline counterparts. We analyze these results in the following.
### Analysis


We find the impact of pre-activation is twofold. First, the optimization is further eased (comparing with the baseline ResNet) because \(f\) is an identity mapping. Second, using BN as pre-activation improves regularization of the models.

**Ease of optimization**. This effect is particularly obvious when training the _1001-layer_ ResNet. Fig. 1 shows the curves. Using the original design in [1], the training error is reduced very slowly at the beginning of training. For \(f=\) ReLU, the signal is impacted if it is negative, and when there are many Residual Units, this effect becomes prominent and Eqn.(3) (so Eqn.(5)) is not a good approximation. On the other hand, when \(f\) is an identity mapping, the signal can be propagated directly between any two units. Our 1001-layer network reduces the training loss very quickly (Fig. 1). It also achieves the lowest loss among all models we investigated, suggesting the success of optimization.

We also find that the impact of \(f=\) ReLU is not severe when the ResNet has fewer layers (_e.g._, 164 in Fig. 6(right)). The training curve seems to suffer a little bit at the beginning of training, but goes into a healthy status soon. By monitoring the responses we observe that this is because after some training, the weights are adjusted into a status such that \(\mathbf{y}_{l}\) in Eqn.(1) is more frequently above zero and \(f\) does not truncate it (\(\mathbf{x}_{l}\) is always non-negative due to the previous ReLU, so \(\mathbf{y}_{l}\) is below zero only when the magnitude of \(\mathcal{F}\) is very negative). The truncation, however, is more frequent when there are 1000 layers.





**Reducing overfitting**. Another impact of using the proposed pre-activation unit is on regularization, as shown in Fig. 6 (right). The pre-activation version reaches slightly higher training loss at convergence, but produces lower test error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and ResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN's regularization effect [8]. In the original Residual Unit (Fig. 4(a)), although the BN normalizes the signal, this is soon added to the shortcut and thus the merged signal is not normalized. This unnormalized signal is then used as the input of the next weight layer. On the contrary, in our pre-activation version, the inputs to all weight layers have been normalized.
## Results


**Comparisons on CIFAR-10/100.** Table 4 compares the state-of-the-art methods on CIFAR-10/100, where we achieve competitive results. We note that we do not specially tailor the network width or filter sizes, nor use regularization techniques (such as dropout) which are very effective for these small datasets. We obtain these results via a simple but essential concept -- going deeper. These results demonstrate the potential of _pushing the limits of depth_.

**Comparisons on ImageNet.** Next we report experimental results on the 1000-class ImageNet dataset [3]. We have done preliminary experiments using the skip connections studied in Fig. 2 & 3 on ImageNet with ResNet-101 [1], and observed similar optimization difficulties. The training error of these non-identity shortcut networks is obviously higher than the original ResNet at the first learning rate




(similar to Fig. 3), and we decided to halt training due to limited resources. But we did finish a "BN after addition" version (Fig. 4(b)) of ResNet-101 on ImageNet and observed higher training loss and validation error. This model's single-crop (224\(\times\)224) validation error is 24.6%/7.5%, _vs._ the original ResNet-101's 23.6%/7.1%. This is in line with the results on CIFAR in Fig. 6 (left).

Table 5 shows the results of ResNet-152 [1] and ResNet-20031, all trained from scratch. We notice that the original ResNet paper [1] trained the models using scale jittering with shorter side \(s\in[256,480]\), and so the test of a 224\(\times\)224 crop on \(s=256\) (as did in [1]) is negatively biased. Instead, we test a single 320\(\times\)320 crop from \(s=320\), for all original and our ResNets. Even though the ResNets are trained on smaller crops, they can be easily tested on larger crops because the ResNets are fully convolutional by design. This size is also close to 299\(\times\)299 used by Inception v3 [19], allowing a fairer comparison.

Footnote 1: The ResNet-200 has 16 more 3-layer bottleneck Residual Units than ResNet-152, which are added on the feature map of 28\(\times\)28.

The original ResNet-152 [1] has top-1 error of 21.3% on a 320\(\times\)320 crop, and our pre-activation counterpart has 21.1%. The gain is not big on ResNet-152 because this model has not shown severe generalization difficulties. However, the original ResNet-200 has an error rate of 21.8%, higher than the baseline ResNet-152. But we find that the original ResNet-200 has _lower_ training error than ResNet-152, suggesting that it suffers from overfitting.

Our pre-activation ResNet-200 has an error rate of 20.7%, which is **1.1%** lower than the baseline ResNet-200 and also lower than the two versions of ResNet-152. When using the scale and aspect ratio augmentation of [20, 19], our ResNet-200 has a result better than Inception v3 [19] (Table 5). Concurrent with our work, an Inception-ResNet-v2 model [21] achieves a single-crop result of 19.9%/4.9%. We expect our observations and the proposed Residual Unit will help this type and generally other types of ResNets.

**Computational Cost.** Our models' computational complexity is linear on




depth (so a 1001-layer net is \(\sim\)10\(\times\) complex of a 100-layer net). On CIFAR, ResNet-1001 takes about 27 hours to train on 2 GPUs; on ImageNet, ResNet-200 takes about 3 weeks to train on 8 GPUs (on par with VGG nets [22]).
## Conclusions


This paper investigates the propagation formulations behind the connection mechanisms of deep residual networks. Our derivations imply that identity shortcut connections and identity after-addition activation are essential for making information propagation smooth. Ablation experiments demonstrate phenomena that are consistent with our derivations. We also present 1000-layer deep networks that can be easily trained and achieve improved accuracy.