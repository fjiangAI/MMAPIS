# Human-like Controllable Image Captioning with Verb-specific Semantic Roles

- Authors: Long Chen<sup>2,3*,</sup>    Zhihong Jiang<sup>1*</sup>    Jun Xiao\({}^{1\dagger}\)   Wei Liu<sup>4</sup>

- Organization: <sup>1</sup>Zhejiang University   <sup>2</sup>Tencent AI Lab   <sup>3</sup>Columbia University   <sup>4</sup>Tencent Data Platform

## Overview
Today, we will explore a paper titled "Human-like Controllable Image Captioning with Verb-specific Semantic Roles," authored by Long Chen, Zhihong Jiang, Jun Xiao, and Wei Liu. The paper proposes a novel approach called Verb-specific Semantic Roles (VSR) for controllable image captioning. The VSR control signal consists of a verb and semantic roles, capturing the activity and the roles of entities involved in the image. The proposed approach includes three main components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The training stage involves training the GSRL, SSP, and captioning model separately using appropriate loss functions. In the testing stage, the framework can be extended from one VSR to multiple VSRs as control signals. The experimental results demonstrate the effectiveness of the framework on challenging benchmarks, achieving better controllability in image captioning and generating diverse captions.

## Introduction
Image captioning is a challenging task that aims to generate natural language descriptions for images. However, existing approaches lack controllability, making it difficult to generate captions that align with specific requirements. To address this limitation, the authors propose a novel approach called Verb-specific Semantic Roles (VSR) for controllable image captioning. VSR consists of a verb capturing the activity in the image and semantic roles categorizing how objects participate in the activity. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. GSRL grounds the semantic roles to visual regions, SSP learns a reasonable sequence of sub-roles, and Role-shift Caption Generation generates captions by focusing on specific sub-roles and their corresponding grounded region sets.

## Related Work
Existing approaches for controllable image captioning often lack a comprehensive understanding of the relationships between verbs, semantic roles, and image content. The proposed approach introduces Verb-specific Semantic Roles (VSR) as a control signal for controllable image captioning. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. In the GSRL component, object proposals are extracted, and a similarity score is calculated between semantic roles and proposal sets. The SSP component learns a sequence of sub-roles using a sentence-level SSP and a role-level SSP. The Role-shift Caption Generation component generates captions by focusing on specific sub-roles and their grounded region sets.

## Proposed Approach
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as a control signal for controllable image captioning. VSR consists of a verb capturing the activity in the image and semantic roles categorizing how objects participate in the activity. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. GSRL grounds the semantic roles to visual regions, SSP learns a reasonable sequence of sub-roles, and Role-shift Caption Generation generates captions by focusing on specific sub-roles and their corresponding grounded region sets. The training stage involves separate training of each component using appropriate loss functions, and the inference stage uses the trained models to generate captions. The framework can be extended to multiple VSRs as control signals. Overall, the proposed approach provides a comprehensive solution for controllable image captioning.

### Controllable Caption Generation with VSR
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as control signals for controllable image captioning. The approach comprises three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. In GSRL, similarity scores between semantic roles and proposal sets are calculated. SSP learns a reasonable sequence of sub-roles using sentence-level and role-level SSP. Role-shift Caption Generation generates captions by focusing on specific sub-roles and grounded region sets. The training objectives for each component are defined, including binary cross-entropy loss for GSRL, cross-entropy loss for S-level SSP, mean square loss for R-level SSP, and XE and RL stages for the captioning model. In the inference stage, the proposed approach can be extended to multiple VSRs.

### Training and Inference
This paper proposes a controllable image captioning approach using a new control signal called Verb-specific Semantic Roles (VSR). The VSR consists of a verb and semantic roles that represent an activity and the roles of entities involved. The approach includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL model grounds the semantic roles to visual regions, while the SSP learns the hierarchical semantic structure. The Role-shift Caption Generation component generates captions by focusing on specific sub-roles and their grounded regions. The approach is trained separately for each component and can be extended to multiple control signals. Experimental results demonstrate the effectiveness of the proposed approach in achieving controllability and generating diverse captions.

## Experiments

### Datasets and Metrics
The researchers used two datasets for their experiments: Flickr30K Entities and COCO Entities. The Flickr30K Entities dataset is an extension of the Flickr30K dataset, where each noun phrase in the captions is manually grounded with one or more visual regions. The COCO Entities dataset, on the other hand, is an extension of the COCO dataset and consists of 120,000 images, also with five captions per image. In this dataset, the annotations are automatically detected, and each entity is aligned to the detected proposals with the same object class.

However, there are some images in both datasets that do not contain any verbs (activities) in their captions. To address this issue, the researchers dropped these samples from the training and testing stages. The percentage of samples without verbs is 3.26% in COCO Entities and 0.04% in Flickr30K Entities.

By using these datasets, the researchers were able to evaluate their proposed approach and compare it with baselines. It is worth noting that all baselines used the same visual regions as the models with VSRs, ensuring a fair comparison between the different methods.

### Implementation Details
The researchers used a Faster R-CNN model with ResNet-101 to generate proposals for each image. The model was finetuned on the VG dataset. For COCO Entities, the proposals were grouped based on their detected class labels, while for Flickr30K Entities, each proposal was considered as a separate proposal set.

To annotate the Verb-specific Semantic Roles (VSR), a pretrained Semantic Role Labeling (SRL) tool was used to annotate verbs and semantic roles for each caption. These annotations were treated as ground truth. A verb dictionary was built for each dataset, with sizes of 2,662 for COCO and 2,926 for Flickr30K. There were a total of 24 types of semantic roles for all verbs.

In the S-level SSP, the multi-head attention had 8 heads, and the transformer's hidden size was set to 512. The length of the transformer was set to 10. For the R-level SSP, the maximum number of entities for each role was set to 10.

During the reinforcement learning (RL) training of the captioning model, the CIDEr-D score was used as the training reward.

Overall, these experimental techniques and procedures ensured the scientificity, reliability, and validity of the technical methods used in the research. The use of a pretrained SRL tool for VSR annotations provided a reliable source of ground truth data. The specific parameter settings and model architectures were carefully chosen to achieve optimal performance in the proposed approach.

### Evaluation on Controllability
The paper proposes a novel control signal called Verb-specific Semantic Roles (VSR) for Controllable Image Captioning (CIC). VSR addresses the limitations of existing objective control signals by considering event-compatibility and sample-suitability. The paper presents the Grounded Semantic Role Labeling (GSRL) model, Semantic Structure Planner (SSP), and Role-shift Caption Generation as components of the VSR-guided controllable image captioning framework. Experimental results demonstrate the effectiveness of VSR in achieving better controllability and generating diverse captions. Future work includes improving the captioning model, extending VSR to other text generation tasks, and designing a more general framework for images without verbs.

## Conclusions & Future Works
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as a control signal for controllable image captioning. The authors argue that existing control signals overlook the need for event-compatibility and sample-suitability. VSR consists of a verb and semantic roles that ensure event-compatibility, and it only restricts the involved semantic roles, making it suitable for different image samples. The approach includes a Grounded Semantic Role Labeling (GSRL) model, a Semantic Structure Planner (SSP), and a Role-shift Caption Generation model. Experimental results demonstrate the effectiveness of VSR in achieving better controllability and generating diverse captions. Future work includes improving the captioning model, extending VSR to other tasks, and developing a more general framework. This research contributes to the field of controllable image captioning by addressing the limitations of existing control signals and providing a novel approach for generating customized captions.