# Human-like Controllable Image Captioning with Verb-specific Semantic Roles

Long Chen\({}^{2,3*}\)   Zhihong Jiang\({}^{1*}\)   Jun Xiao\({}^{1\dagger}\)   Wei Liu\({}^{4}\)

\({}^{1}\)Zhejiang University  \({}^{2}\)Tencent AI Lab  \({}^{3}\)Columbia University  \({}^{4}\)Tencent Data Platform

zjuchenlong@gmail.com, {zju_jiangzhihong, junx}@zju.edu.cn, wl2223@columbia.edu

denotes equal contributions, \({}^{\dagger}\) denotes the corresponding author.

###### Abstract

Controllable Image Captioning (CIC) -- generating image descriptions following designated control signals -- has received unprecedented attention over the last few years. To emulate the human ability in controlling caption generation, current CIC studies focus exclusively on control signals concerning objective properties, such as contents of interest or descriptive patterns. However, we argue that almost all existing objective control signals have overlooked two indispensable characteristics of an ideal control signal: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. 2) Sample-suitable: the control signals should be suitable for a specific image sample. To this end, we propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists of a verb and some semantic roles, which represents a targeted activity and the roles of entities involved in this activity. Given a designated VSR, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to learn human-like descriptive semantic structures. Lastly, we use a role-shift captioning model to generate the captions. Extensive experiments and ablations demonstrate that our framework can achieve better controllability than several strong baselines on two challenging CIC benchmarks. Besides, we can generate multi-level diverse captions easily. The code is available at: [https://github.com/mad-red/VSR-guided-CIC](https://github.com/mad-red/VSR-guided-CIC).

## 1 Introduction

Image captioning, _i.e_., generating fluent and meaningful descriptions to summarize the salient contents of an image, is a classic proxy task for comprehensive scene understand

Figure 1: The CS and Cap in each sample denote the control signal and generated caption, respectively. **(a):** The captions are generated by model SCT [16], which uses a set of visual regions as control signals. When control signals don’t meet the **event-compatible** requirement (_e.g_., objects hand and sky), SCT generates lower quality captions (red cross). **(b):** The captions are generated by model LaBERT [19], which uses different length-levels as control signals. When control signals don’t meet the **sample-suitable** requirement (_e.g_., level 4), the LaBERT generates lower quality captions. **(c):** The captions are generated by our framework with VSR as control signals. For brevity, we abbreviate \(<\) role, \(1>\) to role in all samples. Arg and LOC denote “argument” and “location”, respectively. For verb sit, Arg1 and Arg2 are “thing sitting” and “sitting position”, respectively. For verb read, Arg0 and Arg1 are “reader” and “thing read”, respectively.

ing [21]. With the release of several large scale datasets and advanced encoder-decoder frameworks, current captioning models plausibly have already achieved "super-human" performance in all accuracy-based evaluation metrics. However, many studies have indicated that these models tend to produce generic descriptions, and fail to control the caption generation process as humans, _e.g._, referring to different contents of interest or descriptive patterns. In order to endow the captioning models with human-like controllability, a recent surge of efforts [16, 10, 19, 76, 46, 75, 27, 20] resort to introducing extra control signals as constraints of the generated captions, called **Controllable Image Captioning** (CIC). As a byproduct, the CIC models can easily generate diverse descriptions by feeding different control signals.

Early CIC works mainly focus on _subjective_ control signals, such as sentiments [40], emotions [41, 22], and personality [14, 52], _i.e._, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on _objective_ control signals. More specifically, they can be coarsely classified into two categories: 1) _Content-controlled_: the control signals are about the contents of interest which need to be described. As the example shown in Figure 1 (a), given the region set (\(\raisebox{-0.86pt}{\includegraphics[]{figures/1.eps}}\raisebox{-0.86pt}{ \includegraphics[]{figures/2.eps}}\raisebox{-0.86pt}{\includegraphics[]{figures/ 3.eps}}\)) as a control signal, we hope that the generated caption can cover all regions (_i.e._, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations [27], object regions [16, 34], scene graphs [10, 76], and mouse trace [46]. 2) _Structure-controlled_: the control signals are about the semantic structures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [77] of the sentence (cf. Figure 1 (b)) are some typical structure-controlled signals.

Nevertheless, all existing objective control signals (_i.e._, both content-controlled and structure-controlled) have overlooked two indispensable characteristics of an ideal control signal towards "human-like" controllable image captioning: 1) **Event-compatible**: all visual contents referred to in a single sentence should be compatible with the described activity. Imaging how humans describe images -- our brains always quickly structure a descriptive pattern like "sth do sth at someplace" first, and then fill in the detailed description [54, 44, 29, 69], _i.e._, we have subconsciously made sure that all the mentioned entities are event-compatible (_e.g._, man, wave, surfboard are all involved in activity riding in Figure 1 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliberately utilize two more objects (hand and sky, _i.e._, \(\raisebox{-0.86pt}{\includegraphics[]{figures/1.eps}}\raisebox{-0.86pt}{ \includegraphics[]{figures/2.eps}}\)) as part of the control signal, and the model generates an incoherent and illogical caption. 2) **Sample-suitable**: the control signals should be suitable for the specific image sample. By "suitable", we mean that there do exist reasonable descriptions satisfying the control signals, _e.g._, a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difficult to decide whether a control signal is sample-suitable in advance. For example in Figure 1 (b), although the two control signals (_i.e._, length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.

In this paper, we propose a new event-oriented objective control signal, _Verb-specific Semantic Roles_ (VSR), to meet both event-compatible and sample-suitable requirements simultaneously. VSR consists of a verb (_i.e._, predicate [8]) and some user-interested semantic roles [30]. As shown in Figure 2, the verb captures the scope of a salient activity in the image (_e.g._, eating), and the corresponding semantic roles1 (_e.g._, agent, food, container, and tool) categorize how objects participate in this activity, _i.e._, a child (agent) is eating (activity) a pancake (food) from a plate (container) with a fork (tool). Thus, VSR is designed to guarantee that all the mentioned entities are event-compatible. Meanwhile, unlike the existing structure-controlled signals which directly impose constraints on the generated captions, VSR only restricts the involved semantic roles, which is theoretically suitable for all the images with the activity, _i.e._, sample-suitable.

Footnote 1: We use PropBank-style annotations of semantic roles (_e.g._, Arg0, Arg1) in all experiments (cf. Figure 1). The FrameNet-style annotations of semantic roles (_e.g._, Agent) here are just for a more intuitive illustration. In the PropBank-style annotations, Arg denotes “argument”, MNR denotes “manner”, DIR denotes “directional”, and LOC denotes “location”. We leave more details in the supplementary material.

In order to generate sentences with respect to the designated VSRs, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, _e.g._, Arg0\({}_{\text{reader}}\)\(-\)read\(-\)Arg1\({}_{\text{thing}}\)\(-\)LOC in Figure 1 (c). Finally, we combine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.

Although these are no available captioning datasets with the VSR annotations, they can be easily obtained by off-the-shelf semantic role parsing toolkits [51]. Extensive experiments on two challenging CIC benchmarks (_i.e._, COCO

Figure 2: Two image examples of a verb and its semantic roles. The verb eating captures the scope of the activity, and agent, food, container, tool are all reasonable semantic roles for this activity.

Entities [16] and Flickr30K Entities [45]) demonstrate that our framework can achieve better controllability given designated VSRs than several strong baselines. Moreover, our framework can also realize diverse image captioning and achieve a better trade-off between quality and diversity.

In summary, we make three contributions in this paper:

1. We propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). To the best of our knowledge, VSR is the first control signal to consider both event-compatible and sample-suitable requirements2. Footnote 2: When using control signals extracted from GT captions, existing control signals can always meet both requirements and generate reasonable captions. However, in more general settings (_e.g._, construct control signals without GT captions), the form of VSR is more human-friendly, and it is easier to construct signals which meet both requirements compared with all existing forms of control signals, which is the main advantage of VSR.
2. We can learn human-like verb-specific semantic structures automatically, and abundant visualization examples demonstrate that these patterns are reasonable.
3. We achieve state-of-the-art controllability on two challenging benchmarks, and generate diverse captions by using different verbs, semantic roles, or structures.

## 2 Related Work

**Controllable Image Captioning.** Compared with conventional image captioning [61, 66, 9, 25, 13], CIC is a more challenging task, which needs to consider extra constraints. Early CIC works are mostly about stylized image captioning, _i.e._, constraints are the linguistic styles of sentences. According to the requirements of parallel training samples, existing solutions can be divided into two types: models using parallel stylized image-caption data [40, 11, 52, 1] or not [22, 41]. Subsequently, the community gradually shifts the emphasis to controlling described contents [16, 75, 27, 10, 76, 46, 34] or structures [20, 19, 73, 74] of the sentences. In this paper, we propose a novel control signal VSR, which is the first control signal to consider both the event-compatible and sample-suitable requirements.

**Diverse and Distinctive Image Captioning.** Diverse image captioning, _i.e._, describing the image contents with diverse wordings and rich expressions, is an essential property of human-like captioning models. Except from feeding different control signals to the CIC models, other diverse captioning methods can be coarsely grouped into four types: 1) GAN-based [17, 50, 31]: they use a discriminator to force the generator to generate human-indistinguishable captions. 2) VAE-based [63, 7]: the diversity obtained with them is by sampling from a learned latent space. 3) RL-based [38]: they regard diversity as an extra reward in the RL training stage. 4) BS-based [60]: they decode a list of diverse captions by optimizing a diversity-augmented objective.

Meanwhile, distinctive image captioning is another close research direction [18, 58, 36, 35, 62], which aims to generate discriminative and unique captions for individual images. Unfortunately, due to the subjective nature of diverse and distinctive captions, effective evaluation remains as an open problem, and several new metrics are proposed, such as SPICE-U [65], CIDErBtw [62], self-CIDEr [64], word recall [56], mBLEU [50]. In this paper, we can easily generate diverse captions in both lexical-level and syntactic-level.

**Semantic Roles in Images.** Inspired from the semantic role labeling task [6] in NLP, several tasks have been proposed to label the roles of each object in an activity in an image:

_Visual Semantic Role Labeling (VSRL)_, also called situation recognition, is a generalization of action recognition and human-object interaction, which aims to label an image with a set of verb-specific action _frames_[71]. Specifically, each action frame describes details of the activity captured by the verb, and it consists of a fixed set of verb-specific semantic roles and their corresponding values. The values are the entities or objects involved in the activity and the semantic roles categorize how objects participate in the activity. The current VSRL methods [23, 71, 39, 32, 70, 55, 15] usually learn an independent action classifier first, and then model the role inter-dependency by RNNs or GNNs.

_Grounded Semantic Role Labeling (GSRL)_, also called grounded situation recognition, builds upon the VSRL task, which requires the models not only to label a set of frames, but also to localize each role-value pair in the image [47, 53, 68, 23]. In this paper, we use the GSRL model as a bridge to connect the control signals (VSR) and related regions. To the best of our knowledge, we are the first captioning work to benefit from the verb lexicon developed by linguists.

## 3 Proposed Approach

For human-like controllable image captioning, we first propose the Verb-specific Semantic Roles (VSR) as the control signal for generating customized captions. As shown in Figure 3, we formally represent a control signal VSR as:

\[\mathcal{VSR}=\{v,<s_{1},n_{1}>,...,<s_{m},n_{m}>\}, \tag{1}\]

where \(v\) is a **verb** capturing the scope of a salient activity in the image (_e.g._, ride), \(s_{i}\) is a **semantic role** of verb \(v\) (_e.g._, LOC), and \(n_{i}\) is the number of interested entities in the role \(s_{i}\). For example, for \(\mathcal{VSR}=\{\text{ride},<\text{Arg0},1>,<\text{Arg1},1>,<\text{Loc},2>\}\), we hope to generate a caption which not only focuses on describing the ride activity, but also contains one entity respectively in the role \(\text{Arg0}_{\text{rider}}\) and \(\text{Arg1}_{\text{xeed}}\), and two entities in the role LOC. Thus, VSR can effectively control the amount of information carried in the whole sentence and each role, _i.e._, the level of details.

It is convenient to construct VSRs automatically or manually. For the verbs, they can be accurately predicted by an off-the-shelf action recognition network with a predefined verb vocabulary. For the verb-specific semantic roles, they can be easily retrieved from the verb lexicon such as PropBank or FrameNet. Then, the users can easily select a subset of roles or an automatic sampling to generate a subset of roles, and randomly assign the entity number for each role.

Given an image \(\mathbf{I}\) and a control signal \(\mathcal{VSR}\), the controllable image captioning model aims to describe \(\mathbf{I}\) by a textual sentence \(\mathbf{y}=\{y_{1},...,y_{T}\}\), _i.e._, modeling the probability \(p(\mathbf{y}|\mathbf{I},\mathcal{VSR})\). Inspired from the human habit of describing images, we decompose this task into two steps: structuring a descriptive pattern and filling in detailed captions:

\[p(\mathbf{y}|\mathbf{I},\mathcal{VSR})=p(\mathbf{y}|\text{pattern})p(\text{pattern}|\mathbf{I},\mathcal{VSR}). \tag{2}\]

Further, we utilize two sequences \(\mathcal{S}=(s_{1}^{b},...,s_{K}^{b})\) and \(\mathcal{R}=(\mathbf{r}_{1},...,\mathbf{r}_{K})\) to model the descriptive patterns. Specifically, \(\mathcal{S}\) is a semantic structure of the sentence and each \(s_{i}^{b}\in\mathcal{S}\) is a sub-role. By "sub-role", we mean that each role \(s_{i}\in\mathcal{VSR}\) can be divided into \(n_{i}\) sub-roles, and when \(n_{i}=1\), role \(s_{i}\) itself is a sub-role. Thus, VSR in Figure 3 can be rewritten as Arg0, Arg1, LOC-1, and LOC-2. \(\mathcal{R}\) is a sequence of visual features of the corresponding grounded entities for each sub-role in \(\mathcal{S}\) (_e.g._, \(\mathbf{r}_{i}\) is the features of visual regions referring to \(s_{i}^{b}\)). Particularly, for presentation conciseness, we regard the verb in \(\mathcal{VSR}\) as a special type of sub-role, and since there are no grounded visual regions referring to the verb, we use the global image feature as the grounded region feature in \(\mathcal{R}\). Meanwhile, we use \(\mathcal{\tilde{R}}\) to denote a set of all elements in the sequence \(\mathcal{R}\). Thus, we further decompose this task into three components:

\[p(\mathbf{y}|\mathbf{I},\mathcal{VSR})=\underbrace{p(\mathbf{y}|\mathcal{S},\mathcal{R})}_ {\text{Captioner}}\underbrace{p(\mathcal{S},\mathcal{R}|\mathcal{\tilde{R}}, \mathcal{VSR})}_{\text{SSP}}\underbrace{p(\mathcal{\tilde{R}}|\mathbf{I},\mathcal{ VSR})}_{\text{GSRL}}. \tag{3}\]

In this section, we first introduce each component of the whole framework of the VSR-guided controllable image captioning model sequentially in Section 3.1 (cf. Figure 3), including a grounded semantic role labeling (GSRL) model, a semantic structure planner (SSP), and a role-shift captioning model. Then, we demonstrate the details about all training objectives and the inference stage in Section 3.2, including extending from a single VSR to multiple VSRs.

### Controllable Caption Generation with VSR

#### 3.1.1 Grounded Semantic Role Labeling (GSRL)

Given an image \(\mathbf{I}\), we first utilize an object detector [48] to extract a set of object proposals \(\mathcal{B}\). Each proposal \(\mathbf{b}_{i}\in\mathcal{B}\) is associated with a visual feature \(\mathbf{f}_{i}\) and a class label \(c_{i}\in\mathcal{C}\). Then, we group all these proposals into \(N\) disjoint sets, _i.e._, \(\mathcal{B}=\{\mathcal{B}_{1},...,\mathcal{B}_{N}\}\)3, and each proposal set \(\mathcal{B}_{i}\) consists of one or more proposals. In this GSRL step, we need to refer each sub-role in the \(\mathcal{VSR}\) to a proposal set in \(\mathcal{B}\). Specifically, we calculate the similarity score \(a_{ij}\) between semantic role \(s_{i}\) and proposal set \(\mathcal{B}_{j}\) by:

Footnote 3: Due to different annotation natures of specific CIC datasets, we group proposals by different principles. Details are shown in Section 4.2.

\[\mathbf{q}_{i}=\left[\mathbf{e}_{v}^{g};\mathbf{e}_{s_{i}}^{g};\mathbf{\tilde{f}}\right],\quad a _{ij}=F_{a}(\mathbf{q}_{i},\mathbf{\tilde{f}}_{j}), \tag{4}\]

where \(\mathbf{e}_{v}^{g}\) and \(\mathbf{e}_{s_{i}}^{g}\) are the word embedding features of verb \(v\) and semantic role \(s_{i}\), \(\mathbf{\tilde{f}}\) and \(\mathbf{\tilde{f}}_{j}\) represent the average-pooled visual features of proposal set \(\mathcal{B}\) and \(\mathcal{B}_{j}\), [;] is a concatenation operation, and \(F_{a}\) is a learnable similarity function4.

Footnote 4: For conciseness, we leave the details in the supplementary material.

After obtaining the grounding similarity scores \(\{a_{ij}\}\) between semantic role \(s_{i}\) and all proposal sets \(\{\mathcal{B}_{j}\}\), we then select the top \(n_{i}\) proposal sets with the highest scores as the grounding results for all sub-roles of \(s_{i}\). \(\mathcal{\tilde{R}}\) in Eq. (3) is the set of visual features of all grounded proposal sets.

#### 3.1.2 Semantic Structure Planner (SSP)

Semantic structure planner (SSP) is a hierarchical semantic structure learning model, which aims to learn a reasonable sequence of sub-roles \(\mathcal{S}\). As shown in Figure 3, it consists of two subnets: an S-level SSP and an R-level SSP.

**S-level SSP.** The sentence-level (S-level) SSP is a coarse-grained structure learning model, which only learns a sequence of all involved general semantic roles (including the verb) in \(\mathcal{VSR}\) (_e.g._, ride, Arg0\({}_{\text{rider}}\), Arg1\({}_{\text{steed}}\) and LOC in Figure 3). To this end, we formulate this sentence-level structure learning as a role sequence generation task, as long as we constrain that each output role token belongs to the

Figure 3: The whole architecture of our proposed VSR-guided CIC model. This framework consists of three components: 1) a GSRL model to ground the entities for each role; 2) an SSP to learn a semantic structure; 3) a role-shift captioning model to generate the caption.

given role set and each role can only appear once. Specifically, we utilize a three-layer Transformer [57]5 to calucate the probability of roles \(p(s_{i}|\mathcal{VSR})\) at each time step \(t^{4}\):

Footnote 5: More comparison results between Transformer and Sinkhorn networks [42, 16] are left in supplementary material.

\[\begin{split}\mathbf{H}&=\text{Transformer}_{\text{enc} }\left(\{\text{FC}_{a}(\mathbf{e}_{v}^{i}+\mathbf{e}_{s_{i}}^{i})\}\right),\\ p(s_{t}|\mathcal{VSR})&=\text{Transformer}_{\text{dec }}\left(\mathbf{H},\mathbf{e}_{s_{c1}}^{o}\right),\end{split} \tag{5}\]

where Transformer, are the encoder (enc) and decoder (dec) of the standard multi-head transformer. \(\mathbf{e}_{v}^{i}\) and \(\mathbf{e}_{s_{i}}^{i}\) are the word embedding features of verb \(v\) and semantic role \(s_{j}\), respectively. FC\({}_{a}\) is a learnable fc-layer to obtain the embedding of each input token. \(\mathbf{e}_{s_{c1}}^{o}\) is the sequence of embeddings of previous roles. Based on \(p(s_{t}|\mathcal{VSR})\), we can predict a role at time step \(t\) and obtain an initial role sequence,, \(\text{Arg}_{\text{order}}-\text{ride}-\text{Arg}_{\text{t-ated}}-\text{LOC}\) in Figure 3.

**R-level SSP.** The role-level (R-level) SSP is a fine-grained structure model which aims to rank all sub-roles within the same semantic role (, LOC-1 and LOC-2 are two sub-roles of role Loc in Figure 3). Since the only differences among these sub-roles are the grounded visual regions, we borrow ideas from the Sinkhorn networks [42, 16], which use a differentiable Sinkhorn operation to learn a _soft_ permutation matrix \(\mathbf{P}\). Specifically, for each role \(s_{i}\) with multiple sub-roles (, \(n_{i}>1\)), we first select all the corresponding grounded proposal sets for these sub-roles, denoted as \(\hat{\mathcal{B}}=\{\hat{\mathcal{B}}_{1},...,\hat{\mathcal{B}}_{n_{i}}\}\). And for each proposal \(\mathbf{b}_{*}\in\hat{\mathcal{B}}\), we encode a feature vector \(\mathbf{z}_{*}=[\mathbf{z}_{*}^{v};\mathbf{z}_{*}^{s_{i}};\mathbf{z}_{*}^{l}]\), where \(\mathbf{z}_{*}^{v}\) is a transformation of its visual feature \(\mathbf{f}_{*}\), \(\mathbf{z}_{*}^{s_{i}}\) is the word embedding feature of the semantic role \(s_{i}\), and \(\mathbf{z}_{*}^{l}\) is a 4-d encoding of the spatial position of proposal \(\mathbf{b}_{*}\). Then, we transform each feature \(\mathbf{z}_{*}\) into \(n_{i}\)-d, and average-pooled all features among the same proposal set,, we can obtain an \(n_{i}\)-d feature for each \(\hat{\mathcal{B}}_{i}\). We concatenate all these features to get an \(n_{i}\times n_{i}\) matrix \(\mathbf{Z}\). Finally, we use the Sinkhorn operation to obtain the soft permutation matrix \(\mathbf{P}^{4}\):

\[\mathbf{P}=\text{Sinkhorn}(\mathbf{Z}). \tag{6}\]

After the two SSP subnets (, S-level and R-level), we can obtain the semantic structure \(\mathcal{S}\) (cf. Eq. (3)). Based on the sequence of \(\mathcal{S}\) and the set of proposal features \(\tilde{\mathcal{R}}\) from the GSRL model, we re-rank \(\tilde{\mathcal{R}}\) based on \(\mathcal{S}\) and obtain \(\mathcal{R}\).

#### 3.1.3 Role-shift Caption Generation

Given the semantic structure sequence \(\mathcal{S}=(s_{1}^{b},...,s_{K}^{b})\) and corresponding proposal feature sequence \(\mathcal{R}=(\mathbf{r}_{1},...,\mathbf{r}_{K})\), we utilize a two-layer LSTM to generate the final caption \(\mathbf{y}\). At each time step, the model focuses on one specific sub-role \(s_{t}^{b}\) and its grounded region set \(\mathbf{r}_{t}\), and then generates the word \(y_{t}\). Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: \(p(g_{t}|\mathcal{S},\mathcal{R})\) for controlling the shift of sub-roles, and \(p(y_{t}|\mathcal{S},\mathcal{R})\) to predict the distribution of a word.

As for the role-shift, we use an adaptive attention mechanism [37] to predict the probability of shifting6:

Footnote 6: Note that the proposed method uses the same semantic role \(s_{t}^{b}\) and its grounded region set \(\mathbf{r}_{t}\), and then generates the word \(y_{t}\). Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: \(p(g_{t}|\mathcal{S},\mathcal{R})\) for controlling the shift of sub-roles, and \(p(y_{t}|\mathcal{S},\mathcal{R})\) to predict the distribution of a word.

As for the role-shift, we use an adaptive attention mechanism [37] to predict the probability of shifting7:

Footnote 7: Note that the proposed method uses the same semantic role \(s_{t}^{b}\) and its grounded region set \(\mathbf{r}_{t}\), and then generates the word \(y_{t}\). Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: \(p(g_{t}|\mathcal{S},\mathcal{R})\) for controlling the shift of sub-roles, and \(p(y_{t}|\mathcal{S},\mathcal{R})\) to predict the distribution of a word.

\[\alpha_{t}^{g},\mathbf{\alpha}_{t}^{r},\mathbf{s}\mathbf{r}_{t}^{g}=\text{AdaptiveAttn}_{a}( \mathbf{x}_{t},\mathbf{r}_{t}), \tag{7}\]

where \(\text{AdaptiveAttn}_{a}\) is an adaptive attention network, \(\mathbf{x}_{t}\) is the input query for attention, \(\mathbf{s}\mathbf{r}_{t}^{g}\) is a spatial vector, \(\alpha_{t}^{g}\) and \(\mathbf{\alpha}_{t}^{r}\) are the attention weights for the spatial vector and region features, respectively. We directly use attention weight \(\alpha_{t}^{g}\) as the probability of shifting sub-roles,, \(p(g_{t}|\mathcal{S},\mathcal{R})=\alpha_{t}^{g}\). Based on probability \(p(g_{t}|\mathcal{S},\mathcal{R})\), we can sample a gate value \(g_{j}\in\{0,1\}\), and the focused sub-role at time step \(t\) is:

\[s_{t}^{b}\leftarrow\mathcal{S}[i],\text{where }i=\min\left(1+\sum_{j=1}^{t-1}g_{j},K \right). \tag{8}\]

Due to the special nature of sub-role "verb", we fix \(g_{t+1}=1\) when \(s_{t}^{b}\) is the verb.

For each sub-role \(s_{t}^{b}\), we use the corresponding proposal set features \(\mathbf{r}_{t}\) and a two-layer LSTM to generate word \(y_{t}\):

\[\begin{split}\mathbf{h}_{t}^{1}&=\text{LSTM}_{1}\left( \mathbf{h}_{t-1}^{1},\{y_{t-1},\bar{\mathbf{f}},\mathbf{h}_{t-1}^{2}\}\right),\\ \mathbf{h}_{t}^{2}&=\text{LSTM}_{2}\left(\mathbf{h}_{t-1}^{ 2},\{\mathbf{h}_{t}^{1},\mathbf{c}_{t}\}\right),\\ y_{t}&\sim p(y_{t}|\mathcal{S},\mathcal{R})=\text{ FC}_{b}(\mathbf{h}_{t}^{2}),\end{split} \tag{9}\]

where \(\mathbf{h}_{t}^{1}\) and \(\mathbf{h}_{t}^{2}\) are hidden states of the first- and second-layer LSTM (, LSTM\({}_{1}\) and LSTM\({}_{2}\)), FC\({}_{b}\) is a learnable fc-layer, and \(\mathbf{c}_{t}\) is a context vector. To further distinguish the textual and visual words, we use another adaptive attention network to obtain the context vector \(\mathbf{c}_{t}\)6:

Footnote 6: Note that the proposed method uses the same semantic role \(s_{t}^{b}\) and its grounded region set \(\mathbf{c}_{t}\), and then generates the word \(y_{t}\).

\[\begin{split}\alpha_{t}^{v},\mathbf{\alpha}_{t}^{r},\mathbf{s}\mathbf{r}_{t}^{ v}&=\text{AdaptiveAttn}_{b}(\mathbf{x}_{t},\mathbf{r}_{t}),\\ \mathbf{c}_{t}&=\alpha_{t}^{v}\cdot\mathbf{s}\mathbf{r}_{t}^{ v}+\sum_{i}\mathbf{\alpha}_{t,i}^{r}\cdot\mathbf{r}_{t,i},\end{split} \tag{10}\]

where \(\mathbf{x}_{t}\) is the query for adaptive attention (, the input of the LSTM\({}_{1}\)), \(\mathbf{s}\mathbf{r}_{t}^{v}\) is a spatial vector, and \(\alpha_{t}^{v}\) and \(\mathbf{\alpha}_{t}^{r}\) are the attention weights for the spatial vector and region features.

### Training and Inference

**Training Stage.** In the training stage, we train the three components (GSRL, SSP and captioning model) separately:

_Training objective of GSRL._ For the GSRL model, we use a binary cross-entropy (BCE) loss between the predicted similarity scores \(\hat{a}_{ij}\) and its ground truth \(a_{ij}^{*}\) as the training loss:

\[L_{\text{GSRL}}=\sum_{ij}\text{BCE}(\hat{a}_{ij},a_{ij}^{*}). \tag{11}\]

_Training objective of SSP._ For S-level SSP, we use a cross-entropy (XE) loss between prediction \(\hat{s}_{t}\) and its ground truth \(s_{t}^{*}\) as the training objective. For R-level SSP, we use a mean square (MSE) loss between prediction \(\mathbf{\hat{P}}_{t}\) and its ground truth \(\mathbf{P}_{t}^{*}\) as the training objective:

\[L_{\text{SSP}}^{S}=\sum_{t}\text{XE}(\hat{s}_{t},s_{t}^{*}),L_{\text{SSP}}^{R}= \sum_{t}\mathbf{1}_{(n_{t}>1)}\text{MSE}(\mathbf{\hat{P}}_{t},\mathbf{P}_{t}^{*}), \tag{12}\]here \(\mathbf{1}_{(n_{t}>1)}\) is an indicator function, being 1 if \(n_{t}>1\) and 0 otherwise.

_Training objective of captioning model._ We follow the conventions of previous captioning works and use a two-stage training scheme: XE and RL stages. In the XE stage, we use an XE loss between predicted words and ground truth words as the training loss. In the RL stage, we use a self-critical baseline [49]. At each step, we sample from \(p(y_{t}|\mathcal{S},\mathcal{R})\) and \(p(g_{t}|\mathcal{S},\mathcal{R})\) to obtain the next word \(y_{t+1}\) and sub-role \(s^{b}_{t+1}\). Then we calculate the reward \(r(\mathbf{y}^{s})\) of the sampled sentence \(\mathbf{y}^{s}\). Baseline \(b\) is the reward of the greedily generated sentence. Thus, the gradient expression of the training loss is:

\[\nabla_{\theta}L=-(r(\mathbf{y}^{s})-b)(\nabla_{\theta}\log p(\mathbf{y}^{s})+\nabla_{ \theta}\log p(\mathbf{g}^{s})), \tag{13}\]

where \(\mathbf{g}^{s}\) is the sequence of role-shift gates.

**Inference.** In testing stage, given an image and one \(\mathcal{VSR}\), we sequentially use the GSRL, SSP, and captioning model to generate the final captions. Meanwhile, our framework can be easily extended from one \(\mathcal{VSR}\) to multiple \(\mathcal{VSR}\)s as the control signal. Taking an example of two \(\mathcal{VSR}\)s, we first use GSRL and SSP to obtain semantic structures and grounded regions features: \((\mathcal{S}^{a},\mathcal{R}^{a})\) and \((\mathcal{S}^{b},\mathcal{R}^{b})\). Then, as shown in Figure 4, we merge them by two steps4: (a) find the sub-roles in both \(\mathcal{S}^{a}\) and \(\mathcal{S}^{b}\) which refer to the same visual regions (_e.g._, \(s^{a}_{1}\) and \(s^{b}_{1}\) refer to the same proposal set); (b) insert all other sub-roles between the nearest two selected sub-roles (_e.g._, \(s^{*}_{2}\) are still between \(s^{*}_{1}\) and \(s^{*}_{3}\)). Concerning the order of sub-roles from different verbs, we follow the rank of two verbs (_e.g._, \(s^{a}_{2}\) is in front of \(s^{b}_{2}\)).

Footnote 4: All baselines use the same visual regions as models with VSRs.

## 4 Experiments

### Datasets and Metrics

**Flickr30K Entities [45].** It builds upon the Flickr30K [72] dataset, by manually grounding each noun phrase in the descriptions with one or more visual regions. It consists of 31,000 images, and each image is associated with five captions. We use the same splits as [26] in our experiments.

**COCO Entities [16].** It builds upon the COCO [12] dataset which consists of 120,000 images and each image is annotated with five captions. Different from Flickr30K Entities where all grounding entities are annotated by humans, all annotations in COCO Entities are detected automatically. Especially, they align each entity to all the detected proposals with the same object class.

Although we only assume that there exists at least one verb (_i.e._, activity) in each image; unfortunately, there are still a few samples (_i.e._, 3.26% in COCO Entities and 0.04% in Flickr30K Entities) having no verbs in their captions. We use the same split as [16] and further drop the those samples with no verb in the training and testing stages5. We will try to cover these extreme cases and leave it for future work.

Footnote 5: All baselines use the same visual regions as models with VSRs.

### Implementation Details

**Proposal Generation and Grouping.** We utilize a Faster R-CNN [48] with ResNet-101 [24] to obtain all proposals for each image. Especially, we use the model released by [3], which is finetuned on VG dataset [28]. For COCO Entities, since the "ground truth" annotations for each noun phrase are the proposals with the same class, we group the proposals by their detected class labels. But for Flickr30K Entities, we directly regard each proposal as a proposal set.

**VSR Annotations.** Since there are no ground truth semantic role annotations for CIC datasets, we use a pretrained SRL tool [51] to annotate verbs and semantic roles for each caption, and regard them as ground truth annotations. For each detected verb, we convert it into its base form and build a verb dictionary for each dataset. The dictionary sizes for COCO and Flickr30K are 2,662 and 2,926, respectively. There are a total of 24 types of semantic roles for all verbs.

**Experimental Settings.** For the S-level SSP, the head number of multi-head attention is set to 8, and the hidden size of the transformer is set to 512. The length of the transformer is set to 10. For the R-level SSP, we set the maximum number of entities for each role to 10. For the RL training of the captioning model, we use CIDEr-D [59] score as the training reward. Due to the limited space, we leave more detailed parameter settings in the supplementary material.

### Evaluation on Controllability

**Settings.** To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works [16, 76, 10], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines6: 1) **C-LSTM**: It is a Controllable LSTM model [61]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) **C-UpDn**: It is a Controllable UpDn model [3], which uses an adaptive attention to generate the captions. 3) **SCT**[16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) **Ours _w/o_ verb**: We ablate our model by removing the verb information in both the SSP

Figure 4: A toy example of merging two different semantic structures \(\mathcal{S}^{a}\) and \(\mathcal{S}^{b}\) into a single sub-role sequence.

[MISSING_PAGE_FAIL:7]

two settings: 1) Given a VSR and grounded visual regions of each role aligned with the ground truth caption, we first use an SSP to select two semantic structures, and then respectively generate two diverse captions. For fair comparisons, we utilize the same set of visual regions on two strong baselines: a) **BS**: an UpDn model uses beam search to produce two captions, and b) **SCT**: an SCT model takes a permutation of all region sets to generate two captions. 2) For each verb, we can randomly sample a subset of all semantic roles to construct new VSRs. Specifically, we sample two more sets of semantic roles, and generate two diverse captions for each role set following the same manner.

**Evaluation Metrics.** We used two types of metrics to evaluate the diverse captions: 1) Accuracy-based: we followed the conventions of the previous works [16, 20, 63] and reported the best-1 accuracy, _i.e._, the generated caption with the maximum score for each metric is chosen. Analogously, we evaluate the generated captions against the single ground truth caption. 2) Diversity-based: we followed [10] and used two metrics which only focus on the language similarity: Div-n (D-n) [4, 20] and self-CIDEr (s-C) [64].

**Quantitative Results.** The quantitative results are reported in Table 2. From Table 2, we can observe that the diverse captions generated by our framework in both two settings have much higher accuracy (_e.g._, CIDEr 267.3 vs. 222.5 in SCT), and that the diversity is slightly behind SCT (_e.g._, self-CIDEr 67.0 vs. 69.1 in SCT). This is because SCT generates captions by randomly shuffling regions. Instead, we tend to learn more reasonable structures. Thus, we can achieve much higher results on accuracy, _i.e._, our method can achieve a better trade-off between quality and diversity on diverse image captioning than the two strong baselines.

**Visualizations.** We further illustrate the generated captions of two images with different VSRs in Figure 7. The captions are generated effectively according to the given VSR, and the diversity of VSR leads to significant diverse captions.

## 5 Conclusions & Future Works

In this paper, we argued that all existing objective control signals for CIC have overlooked two indispensable characteristics: event-compatible and sample-suitable. To this end, we proposed a novel control signal called VSR. VSR consists of a verb and several semantic roles, _i.e._, all components are guaranteed to be event-compatible. Meanwhile, VSR only restricts the involved semantic roles, which is also sample-suitable for all the images containing the activity. We have validated the effectiveness of VSR through extensive experiments. Moving forward, we will plan to 1) design a more effective captioning model to benefit more from the VSR signals; 2) extend VSR to other controllable text generation tasks, _e.g._, video captioning [67]; 3) design a more general framework to cover the images without verbs.

**Acknowledgements.** This work was supported by the National Natural Science Foundation of China (U19B2043,61976185), Zhejiang Natural Science Foundation (LR19F020002), Zhejiang Innovation Foundation (2019R52002), and Fundamental Research Funds for Central Universities.

\begin{table}
\begin{tabular}{|l|c|c c c c|c c c|} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{\#caps} & \multicolumn{4}{c|}{Accuracy-based} & \multicolumn{2}{c|}{Diversity-based} \\  & & B4 & M & R & C & S & D-1 & D-2 & s-C \\ \hline BS & 2 & 18.1 & 24.0 & 48.8 & 18.7 & 43.7 & 45.5 & 61.2 & 53.5 \\ SCT & 2 & 20.5 & 25.8 & 53.0 & 210.0 & 51.6 & **52.2** & **73.7** & **76.0** \\ Ours & 2 & **24.8** & **29.5** & **57.8** & **251.9** & **53.1** & 48.3 & 70.0 & 68.3 \\ \hline BS & 6 & 20.9 & 25.4 & 52.1 & 209.5 & 47.9 & 22.7 & 35.6 & 53.9 \\ SCT & 6 & 22.0 & 26.5 & 55.4 & 222.5 & 54.9 & **27.7** & **45.7** & **69.1** \\ Ours & 6 & **26.6** & **30.2** & **59.8** & **267.3** & **56.6** & 25.1 & 43.8 & 67.0 \\ \hline \end{tabular}
\end{table}
Table 2: Performance compared with two strong baselines for diverse image captioning on dataset COCO Entities.

Figure 7: Examples of diverse image caption generation conditioned on different VSRs. Best viewed in color.

## References

* [1] Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, and Matthew Stone. Clue: Cross-modal coherence modeling for caption generation. In _ACL_, 2020.
* [2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In _ECCV_, 2016.
* [3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In _CVPR_, 2018.
* [4] Jyoti Aneja, Harsh Agrawal, Dhruv Batra, and Alexander Schwing. Sequential latent spaces for modeling the intention during diverse image captioning. In _ICCV_, 2019.
* [5] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In _ACL workshop_, 2005.
* [6] Xavier Carreras and Lluis Marquez. Introduction to the conll-2005 shared task: Semantic role labeling. In _CoNLL_, 2005.
* [7] Fuhai Chen, Rongrong Ji, Jiayi Ji, Xiaoshuai Sun, Baochang Zhang, Xuri Ge, Yongjian Wu, Feiyue Huang, and Yan Wang. Variational structured semantic inference for diverse image captioning. In _NeurIPS_, 2019.
* [8] Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, and Shih-Fu Chang. Counterfactual critic multi-agent training for scene graph generation. In _ICCV_, 2019.
* [9] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In _CVPR_, 2017.
* [10] Shizhe Chen, Qin Jin, Peng Wang, and Qi Wu. Say as you wish: Fine-grained control of image caption generation with abstract scene graphs. In _CVPR_, 2020.
* [11] Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen Fang, Zhaowen Wang, Hailin Jin, and Jiebo Luo. "factual"or"emotional": Stylized image captioning with adaptive learning and attention. In _ECCV_, 2018.
* [12] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. In _arXiv_, 2015.
* [13] Xinpeng Chen, Lin Ma, Wenhao Jiang, Jian Yao, and Wei Liu. Regularizing rnns for caption generation by reconstructing the past with the present. In _CVPR_, 2018.
* [14] Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. Attend to you: Personalized image captioning with context sequence memory networks. In _CVPR_, 2017.
* [15] Thilini Cooray, Ngai-Man Cheung, and Wei Lu. Attention-based context aware reasoning for situation recognition. In _CVPR_, 2020.
* [16] Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Show, control and tell: A framework for generating controllable and grounded captions. In _CVPR_, 2019.
* [17] Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. Towards diverse and natural image descriptions via a conditional gan. In _ICCV_, 2017.
* [18] Bo Dai and Dahua Lin. Contrastive learning for image captioning. In _NeurIPS_, 2017.
* [19] Chaorui Deng, Ning Ding, Mingkui Tan, and Qi Wu. Length-controllable image captioning. In _ECCV_, 2020.
* [20] Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, and David Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. In _CVPR_, 2019.
* [21] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. Every picture tells a story: Generating sentences from images. In _ECCV_, 2010.
* [22] Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng. Stylenet: Generating attractive visual captions with styles. In _CVPR_, 2017.
* [23] Saurabh Gupta and Jitendra Malik. Visual semantic role labeling. In _arXiv_, 2015.
* [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [25] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong Zhang. Recurrent fusion network for image captioning. In _ECCV_, 2018.
* [26] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In _CVPR_, 2015.
* [27] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning. In _CVPR_, 2019.
* [28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 2017.
* [29] Eun-Kyung Lee, Sarah Brown-Schmidt, and Duane G Watson. Ways of looking ahead: Hierarchical planning in language production. _Cognition_, 2013.
* [30] Beth Levin. _English verb classes and alternations: A preliminary investigation_. 1993.
* [31] Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, and Ming-Ting Sun. Generating diverse and accurate visual captions by comparative adversarial learning. In _arXiv_, 2018.
* [32] Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, and Sanja Fidler. Situation recognition with graph neural networks. In _ICCV_, 2017.
* [33] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _ACL workshop_, 2004.
* [34] Annika Lindh, Robert J Ross, and John D Kelleher. Language-driven region pointer advancement for controllable image captioning. In _COLING_, 2020.
* [35] Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo. Generating diverse and descriptive image captions using visual paraphrases. In _ICCV_, 2019.
** [36] Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xiaogang Wang. Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data. In _ECCV_, 2018.
* [37] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In _CVPR_, 2017.
* [38] Ruotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich. Discriminability objective for training descriptive captions. In _CVPR_, 2018.
* [39] Arun Mallya and Svetlana Lazebnik. Recurrent models for situation recognition. In _ICCV_, 2017.
* [40] Alexander Mathews, Lexing Xie, and Xuming He. Senticap: Generating image descriptions with sentiments. In _AAAI_, 2016.
* [41] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle: Learning to generate stylised image captions using unaligned text. In _CVPR_, 2018.
* [42] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations with gumbel-sinkhorn networks. In _ICLR_, 2018.
* [43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _ACL_, 2002.
* [44] Martin J Pickering and Simon Garrod. An integrated theory of language production and comprehension. _Behavioral and brain sciences_, 2013.
* [45] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _ICCV_, 2015.
* [46] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In _ECCV_, 2020.
* [47] Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi. Grounded situation recognition. In _ECCV_, 2020.
* [48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _IEEE TPAMI_, 2016.
* [49] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In _CVPR_, 2017.
* [50] Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, and Bernt Schiele. Speaking the same language: Matching machine to human captions by adversarial training. In _ICCV_, 2017.
* [51] Peng Shi and Jimmy Lin. Simple bert models for relation extraction and semantic role labeling. In _arXiv_, 2019.
* [52] Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, and Jason Weston. Engaging image captioning via personality. In _CVPR_, 2019.
* [53] Carina Silberer and Manfred Pinkal. Grounding semantic roles in images. In _EMNLP_, 2018.
* [54] L Robert Slevc. Saying what's on your mind: Working memory effects on sentence production. _Journal of experimental psychology: Learning, memory, and cognition_, 2011.
* [55] Mohammed Suhail and Leonid Sigal. Mixture-kernel graph attention network for situation recognition. In _ICCV_, 2019.
* [56] Emiel Van Miltenburg, Desmond Elliott, and Piek Vossen. Measuring the diversity of automatic image descriptions. In _COLING_, 2018.
* [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [58] Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context-aware captions from context-agnostic supervision. In _CVPR_, 2017.
* [59] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _CVPR_, 2015.
* [60] Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. In _AAAI_, 2018.
* [61] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In _CVPR_, 2015.
* [62] Jiuniu Wang, Wenjia Xu, Qingzhong Wang, and Antoni B Chan. Compare and reweight: Distinctive image captioning using similar images sets. In _ECCV_, 2020.
* [63] Liwei Wang, Alexander Schwing, and Svetlana Lazebnik. Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space. In _NeurIPS_, 2017.
* [64] Qingzhong Wang and Antoni B Chan. Describing like humans: on diversity in image captioning. In _CVPR_, 2019.
* [65] Zeyu Wang, Berthy Feng, Karthik Narasimhan, and Olga Russakovsky. Towards unique and informative captioning of images. In _ECCV_, 2020.
* [66] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In _ICML_, 2015.
* [67] Ning Xu, An-An Liu, Yongkang Wong, Yongdong Zhang, Weizhi Nie, Yuting Su, and Mohan Kankanhalli. Dual-stream recurrent neural network for video captioning. _IEEE TCSVT_, 2018.
* [68] Shaohua Yang, Qiaozi Gao, Changsong Liu, Caiming Xiong, Song-Chun Zhu, and Joyce Chai. Grounded semantic role labeling. In _NAACL-HLT_, 2016.
* [69] Xu Yang, Hanwang Zhang, and Jianfei Cai. Learning to collocate neural modules for image captioning. In _ICCV_, 2019.
* [70] Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, and Ali Farhadi. Commonly uncommon: Semantic sparsity in situation recognition. In _CVPR_, 2017.
* [71] Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation recognition: Visual semantic role labeling for image understanding. In _CVPR_, 2016.
* [72] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: Newsimilarity metrics for semantic inference over event descriptions. _TACL_, 2014.
* [73] Yitian Yuan, Lin Ma, Jingwen Wang, and Wenwu Zhu. Controllable video captioning with an exemplar sentence. In _ACM MM_, 2020.
* [74] Qi Zheng, Chaoyue Wang, and Dacheng Tao. Syntax-aware action targeting for video captioning. In _CVPR_, 2020.
* [75] Yue Zheng, Yali Li, and Shengjin Wang. Intention oriented image captions with guiding objects. In _CVPR_, 2019.
* [76] Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and Yin Li. Comprehensive image captioning via scene graph decomposition. In _ECCV_, 2020.
* [77] Zhangzi Zhu, Tianlei Wang, and Hong Qu. Macroscopic control of text generation for image captioning. In _arXiv_, 2021.