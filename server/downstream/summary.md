# Human-like Controllable Image Captioning with Verb-specific Semantic Roles

- Authors: Long Chen<sup>2,3*,</sup>    Zhihong Jiang<sup>1*</sup>    Jun Xiao\({}^{1\dagger}\)   Wei Liu<sup>4</sup>

- Affiliations: <sup>1</sup>Zhejiang University   <sup>2</sup>Tencent AI Lab   <sup>3</sup>Columbia University   <sup>4</sup>Tencent Data Platform<br><br>zjuchenlong@gmail.com, {zju_jiangzhihong, junx}@zju.edu.cn, wl2223@columbia.edu<br><br>denotes equal contributions,   <sup>&dagger;</sup> denotes the corresponding author.

## Abstract
The paper proposes a novel approach called Verb-specific Semantic Roles (VSR) for controllable image captioning. The VSR control signal consists of a verb and semantic roles that represent a targeted activity and the roles of entities involved in the activity. The proposed approach consists of three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation.

In the GSRL component, an object detector is used to extract object proposals from the image, and a similarity score is calculated between each semantic role and proposal set. The top proposal sets with the highest scores are selected as the grounding results for each sub-role. The SSP component includes a sentence-level SSP and a role-level SSP. The sentence-level SSP learns a sequence of general semantic roles, while the role-level SSP ranks sub-roles within the same semantic role using a soft permutation matrix. The Role-shift Caption Generation component generates the final caption by focusing on one sub-role at a time and predicting the probability of shifting sub-roles.

During training, the GSRL, SSP, and captioning model are trained separately using binary cross-entropy loss, cross-entropy loss, and self-critical baseline loss respectively. In the testing stage, the framework can be extended to multiple VSRs as control signals. The proposed approach achieves state-of-the-art controllability on challenging benchmarks and can generate diverse captions.

Overall, the paper introduces a novel control signal and a comprehensive framework for controllable image captioning. The proposed approach addresses the limitations of existing objective control signals and achieves better controllability and diversity in caption generation.
## Introduction
The proposed approach in the paper introduces Verb-specific Semantic Roles (VSR) as control signals for controllable image captioning. It consists of three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL model grounds semantic roles to visual regions in the image, while the SSP learns a reasonable sequence of sub-roles. The Role-shift Caption Generation model generates the final caption by focusing on specific sub-roles and their corresponding grounded regions. The training stage involves training each component separately using specific loss functions, and the inference stage utilizes the GSRL, SSP, and captioning model to generate captions. The approach can be extended to multiple VSRs as control signals. The summary could be improved by including key mathematical formulas (Equations 1, 4, 5, 6, 7, 9, 10, 11, 12, 13) and quantitative results to provide a more comprehensive understanding of the proposed approach's technical details and effectiveness.
## Related Work
This paper proposes a novel approach for controllable image captioning using Verb-specific Semantic Roles (VSR) as control signals. The framework consists of a grounded semantic role labeling (GSRL) model, a semantic structure planner (SSP), and a role-shift captioning model. The GSRL model identifies and grounds entities based on the VSR, while the SSP learns human-like descriptive semantic structures. The role-shift captioning model generates captions by focusing on specific sub-roles and their grounded regions. The training objectives for each component are defined, and a two-stage training scheme is used for the captioning model. In the inference stage, the GSRL, SSP, and captioning model are sequentially used to generate captions. The framework can be extended to multiple VSRs as control signals.
## Proposed Approach
This paper introduces a novel control signal, Verb-specific Semantic Roles (VSR), for controllable image captioning. The VSR consists of a verb and semantic roles, representing the targeted activity and the roles of entities involved. The proposed framework includes three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. The GSRL model grounds the semantic roles to visual proposals, while the SSP learns the semantic structure of the sentence. The Role-shift Caption Generation model generates the final captions by focusing on specific sub-roles and grounded regions. The training stage involves training each component separately, while the inference stage utilizes the GSRL, SSP, and captioning model to generate captions. The framework can be extended to multiple VSRs as control signals.
## Controllable Caption Generation with VSR
The proposed approach in this paper focuses on controllable image captioning using Verb-specific Semantic Roles (VSR) as the control signal. It consists of three main components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. In GSRL, object proposals are extracted and a similarity score is calculated between each semantic role and proposal set. SSP learns the sequence of general semantic roles and ranks sub-roles within each role. Role-shift Caption Generation uses adaptive attention to control sub-role shifting and generate captions. The approach can be extended to multiple VSRs as control signals.
## Training and Inference
The proposed approach in this paper introduces Verb-specific Semantic Roles (VSR) as the control signal for controllable image captioning. The framework consists of three components: Grounded Semantic Role Labeling (GSRL), Semantic Structure Planner (SSP), and Role-shift Caption Generation. In the GSRL component, object proposals are extracted and similarity scores between semantic roles and proposal sets are calculated. The SSP component learns a reasonable sequence of sub-roles using a sentence-level SSP and a role-level SSP. The Role-shift Caption Generation component generates captions by focusing on specific sub-roles and their grounded regions. The training and inference stages are also described. The framework can be extended to multiple VSRs as control signals.
## Experiments

## Datasets and Metrics
The research utilizes two datasets for evaluation: Flickr30K Entities and COCO Entities. The Flickr30K Entities dataset is an extension of the Flickr30K dataset, where each noun phrase in the image descriptions is manually grounded with one or more visual regions. It contains 31,000 images, with five captions per image. The COCO Entities dataset is built upon the COCO dataset and consists of 120,000 images, also with five captions per image. In contrast to Flickr30K Entities, the entity annotations in COCO Entities are automatically detected.

To ensure the scientificity, reliability, and validity of the technical methods, the research assumes that there is at least one verb (activity) in each image. However, a small percentage of samples (3.26% in COCO Entities and 0.04% in Flickr30K Entities) do not have any verbs in their captions. To address this, the research removes these samples from the training and testing stages. It is acknowledged that covering these extreme cases could be a potential avenue for future work.

In terms of data processing, the research uses the same splits as previous work for training and testing. Additionally, all baselines use the same visual regions as models with VSRs, ensuring consistency in the evaluation process.
## Implementation Details
The researchers used a Faster R-CNN with ResNet-101 to generate proposals for each image. They utilized a model that was fine-tuned on the VG dataset. For COCO Entities, the proposals were grouped based on their detected class labels, while for Flickr30K Entities, each proposal was treated as a separate proposal set.

To annotate the verbs and semantic roles for the VSR control signals, the researchers used a pretrained Semantic Role Labeling (SRL) tool. The annotated verbs were converted into their base form and a verb dictionary was created for each dataset. The COCO dataset had a verb dictionary size of 2,662, while the Flickr30K dataset had a size of 2,926. There were a total of 24 types of semantic roles for all verbs.

In terms of experimental settings, the S-level SSP used a multi-head attention with 8 heads and a transformer with a hidden size of 512. The length of the transformer was set to 10. For the R-level SSP, the maximum number of entities for each role was set to 10. During RL training of the captioning model, the CIDEr-D score was used as the training reward.

These experimental techniques and procedures were crucial in ensuring the scientificity, reliability, and validity of the technical methods used in the research. The use of a pretrained SRL tool allowed for accurate annotation of verbs and semantic roles, which formed the basis of the VSR control signals. The proposal generation and grouping techniques provided a comprehensive set of proposals for each image, enabling effective grounding of semantic roles. The specific parameter settings and reward mechanism during training ensured that the captioning model learned to generate high-quality captions based on the VSR control signals.
## Evaluation on Controllability
The researchers evaluated the controllability of their proposed framework using VSRs aligned with ground truth captions as control signals. They compared their framework with several baselines, including C-LSTM, C-UpDn, and SCT, to assess its performance.

To evaluate the diversity of the generated captions, the researchers used accuracy-based metrics (such as CIDEr score) and diversity-based metrics (such as Div-n and self-CIDEr). The results showed that the diverse captions generated by their framework had higher accuracy compared to the baselines, while still maintaining a good level of diversity.

The researchers also provided visualizations to demonstrate the effectiveness of their framework. The generated captions aligned well with the given VSRs, and the diversity of the VSRs resulted in significantly diverse captions.

Overall, the experimental techniques and procedures used in this research involved comparing the proposed framework with carefully designed baselines and evaluating the controllability and diversity of the generated captions using appropriate metrics. The results demonstrated the effectiveness and advantages of the proposed framework in achieving better trade-offs between quality and diversity in image captioning.
## Conclusions & Future Works
This paper presents a novel approach called Verb-specific Semantic Roles (VSR) for controllable image captioning. The authors argue that existing control signals overlook the characteristics of event-compatibility and sample-suitability. VSR consists of a verb and semantic roles, ensuring event-compatibility and sample-suitability in all images containing the activity. The proposed approach includes a grounded semantic role labeling (GSRL) model, a semantic structure planner (SSP), and a role-shift captioning model. Extensive experiments demonstrate the effectiveness of VSR in achieving better controllability and generating diverse captions. Future work includes improving the captioning model, extending VSR to other text generation tasks, and designing a more general framework.
