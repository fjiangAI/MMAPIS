## abstract

The  exponential growth of academic articles on the Internet due to expanding research domains has rendered the retrieval of target articles a notably time-consuming endeavor. A majority of these scholarly publications presented in the form of PDFs, a factor that sadly contributes to substantial information loss, especially mathematical expressions. In response , we introduce  =="arxiv summarizer"==, a novel Scientific Automatic Summarization System (SASS) based on Nougat， converting document pages to markup， and large language models(LLMs) like GPT-3.5, to create concise and easily digestible summaries.  The proposed solution facilitates the interactive generation of precise and fluent summaries suitable for academic papers, interpretive blogs, and broadcasts. Our experimental results showcase the effectiveness of SASS, demonstrating its capability to outperform even GPT-4. This positions it as a reliable option to expedite the retrieval of target documents in the scholarly landscape.

<!-- TODO: Figure -->

## introduction

The dawn of the digital age has led to a rapid surge in the transmission of academic articles online, making the retrieval of scientific documents that scientific researcher interested in an increasingly daunting task. Even query-based searches for some specific fields return a large number of relevant articles that far exceed human processing capabilities [7], as scientific articles are generally much longer.

Researchers have proposed various application systems to summarize long document, including: writing section-structured [82], user-specific [45] or presentation-based [107] summaries for scientific papers, automating scientific reviewing [126], tends to streamline long documents into essentials in various forms.

These existing methods have yeilded promising progress towards the goald of long document summarization, however , they are not suitable to summarize scientific document for user to filter what they interested , for one or more of the following reasons, which are the parts  that information most easily lost during processing: 1) most of methods mentioned above can only process data with plain text as input, and cannot process PDF documents as input, which causes great inconvenience to users. 2）mjority of previous methods ignore the multimodal data contains in the scientific document (e.g. Mathematical formulas，tables, figures etc.), which  represent the most pertinent experimental results, ideas, or workflows[6]. 3) existing approaches are  either unable to deal with long document as they are mainly focus on the short document practice or hard to follow as merely concatenation of extract sentences.

Given these persisting issues, i.e. the challenges of PDF document parsing  ,utilization of  crucial multimedia information, and long document text summarization, we hereby present the "arXiv summarizer", a novel Scientific Automatic Summarization System (SASS), which offers a resolution to these dilemmas. Our system leverages the recent research development from Optical    Understanding to tackle the first two questions. In the issue of text summarization for  long document, as the 

 is built on Nougat[2], a pretrained model capable of converting a PDF to lightweight markup language, and large language models (LLMs) such as GPT-3.5, to generate summaries that strike a harmonious balance between fluency and precision. The primary contributions of our paper are as follows:

1. We introduce an interactive generation system that accepts input in PDF format while outputting precise and fluid summaries. These summaries can be further transferred  for academic papers, interpretive blogs, and broadcasts thereby filling up the gap between the summaries lacking conciseness and fluency generated by traditional summarization methods, which can be challenging for readers to follow, and those gold summaries crafted by human experts in summarizations of scientific articles.
2. we introduce a pipeline to process long text documents into groups and merge them into seamlessly and understandable summaries.





Given these persisting issues, i.e. the challenges of preserving crucial multimedia information, parsering pdf document without loss and the rigidity of fine-tuning pretrained models, another significant concern lies in the problem of long document summarization—the existing state-of-the-art summarization systems (e.g., pre-trained models) are often cornered by their limitation to handle 512 to 1024 lexical tokens only [26-27],due to the memory complexity issues and hardware limitations and the time costing pretraining process.,long document summarization approach has been lacking [28-32], we hereby present the "arXiv summarizer", a novel Scientific Automatic Summarization System (SASS), which offers a resolution to these dilemmas. Our system is built on Nougat[2], a pretrained model capable of converting a PDF to lightweight markup language, and large language models (LLMs) such as GPT-3.5, to generate summaries that strike a harmonious balance between fluency and precision. The primary contributions of our paper are as follows:

1. We introduce an interactive generation system that accepts input in PDF format while outputting precise and fluid summaries. These summaries can be further transferred  for academic papers, interpretive blogs, and broadcasts thereby filling up the gap between the summaries lacking conciseness and fluency generated by traditional summarization methods, which can be challenging for readers to follow, and those gold summaries crafted by human experts in summarizations of scientific articles.
2. we introduce a pipeline to process long text documents into groups and merge them into seamlessly and understandable summaries.
3. We provide an evaluation of the scientific automatic summarization system against established benchmarks.







## related work

### 2.1 Issue 1: Difficulty in extracting original information from PDFs

The dawn of the digital age has led to a rapid surge in the transmission of academic articles online, making the retrieval of specific scientific documents an increasingly daunting task. Most of these academic resources are in the Portable Document Format (PDF), which comprises 2.4% of the common crawl on the internet, making it the second most prevalent data format online [1]. Despite this ubiquity, extracting information embedded in PDF files into other formats poses a significant challenge, particularly for specialized documents like scientific research papers. This complexity is further compounded when dealing with mathematical expressions, whose semantic information is often lost [2].

Past technologies such as Optical Character Recognition (OCR) engines, exemplified by Tesseract OCR [3], have proven effective in extracting individual characters and words from images, even in intricate environments. Nonetheless, these engines fall short in preserving the relative positional relationship among different formats, especially mathematical expressions and tables due to their line-by-line approach. This limitation underscores a severe shortfall in information capture, hampering the generation of accurate summaries even for human readers who might misconstrue the intended meanings without the original context.

### 2.2 Issue 2: Summarizing scientific articles

The growth in summarizing generic texts since the 1950s has been commendable [4]. Yet, summarizing scientific articles presents a unique challenge, despite they have a more structured nature, generally comprising sections like introduction, methodology, experiments, and conclusions. As pointed out by Teufel and Moens (2002) [5], there are two main reasons that make scientific articles notably different: 1) scientific articles are typically lengthier than generic texts, and 2) the objectives of the summary are multifaceted, as various researchers may be interested in different aspects, such as new contributions, findings, and solutions proposed.

Bhatia and Mitra (2012) [6] reinforced another issue, positing that conventional text methodologies fall short in creating summaries for academic research due to most of the  information value embodied in 'document elements. These elements, separate from the running text, either augment or summarize the text’s content.Figures, tables, mathematical expressions, and pseudocodes for algorithms often constitute these elements, representing the most pertinent experimental results, ideas, or workflows.

The abstract is a plausible substitute for a summary. However, several issues arise: 1) Abstracts typically do not delve into the specifics of the full text [7], 2) they frequently represent the author's subjective and potentially biased viewpoint of distinctive features [8], 3) a single abstract does not necessarily meet every user’s requirements [9], and 4) an abstract may not encompass all the paper’s impacts and contributions but highlights what the author desires to emphasize (Elkiss et al., 2008) [10]. As such, the ideal summary from a Scientific Automatic Summarization System should capture the essence and maintain objectivity while being revealing.

### related work

Current prevailing methods for scientific article summarization are primarily categorized into two classes:abstract generation-based approaches and citation-oriented approaches. Their classification is contingent upon whether the abstract is considered the reference summary or we called gold summary.

The abstract generation-based approaches aim to automatically generate an abstract for a research article, implying that the abstract is deemed the gold summary, with the remaining text utilised as input. Noteworthy research in this area includes the work of Yang et al. (2016)[11], who designed a system for generating an extended abstract that describes the most important aspects of a scientific article employing a data-weighted reconstruction approach. This process comprises two stages: weight learning and salient sentence selection. Similarly, Slamet et al. (2018)[12] proposed a system to generate article abstracts automatically for the Indonesian language. Both employ intricate mechanisms for sentence weight calculation. The finalized abstract, compiled from the top ranked sentences, may subsequently undergo a reconstruction process.

Following the advent of BERT [13], numerous large-scale models, each with distinct pre-training tasks, have been introduced. However, due to hardware constraints, the input token length is limited to 512 tokens. To address this limitation, Cui and Hu [14] proposed a memory network that incorporates graph attention networks and gated recurrent units to dynamically select important sentences through sliding a window along the entire source document. By confining its use within each window—where the window size is set to be less than or equal to 512 tokens—this approach effectively employs the pre-trained BERT model to tackle long document summarization tasks.



Contrastingly, citation-oriented approaches operate under the assumption that citation sentences typically encapsulate the essential information from the cited article. Thus, citation sentences towards a target paper could be viewed as a succinct summary penned by the cited researchers. Citation-oriented approaches can potentially outperformabstract generation-based approaches as citation summaries reflect the viewpoints of multiple researchers, while the abstract signifies only the authors' perspectives.

In the landscape of citation-oriented methodologies, the crux of most proposals centers on sentence ranking, augmentation, classification, and summarization. This is exemplified in the work of Lauscher et al. (2017)[15] who developed a system leveraging their Learning to Rank (L2R) model. This model ranks sentences from the target paper based on their similarity. Following this initial stage, they engage in clustering and sentence selection, guided by the TextRank score (Mihalcea and Tarau, 2004)[16].Furthermore, Agrawal et al. (2019)[17] proposed a semi-supervised method to tackle scientific article summarization.

As summarization with the abstractive approach is naturally a sequence-to-sequencetask,where the source and target reside in different spaces due to variations in length, redundancy, and other metrics. Thus, a pre-trained model with a sequence-to-sequence objective task would be more fitting than using an encoder-only (e.g., BERT/RoBERTa) or a decoder-only (e.g., GPT-2/GPT-3) pre-trained model. Within the domain of summarization, Bidirectional and Auto-Regressive Transformers (BART) [18] and Text-to-Text Transfer Transformers (T5) [19] are the two preeminent sequence-to-sequence pre-trained models.

BART is pre-trained on a self-supervised task of reconstructing arbitrarily corrupted text while T5 is pre-trained on both unsupervised and supervised objectives, such as token masking, as well as translation and summarization.

Despite the demonstrable effectiveness of T5 in the domain of short document summarization [20], no supervised transformer models within the long document summarization domain - let alone the scientific article domain - have incorporated a summarizer equipped with a T5 pre-training task. While, BART, pre-trained on short documents, adheres to an input limitation of 1,024 tokens.

Summarily, the aforementioned methods can reasonably be characterized as models that train on extensive summarization datasets  tailored to specific domains. However, these models require fine-tuning when transitioning to unfamiliar data distributions. Notably,most of  them are extractive approach which often extract key sentences from the original text and integrate them. The prevalence of extractive approaches can be attributed to scientific papers falling under the category of domain-specific articles, which tend to feature more complex formulas and terminologies. Nonetheless, this indicates that a model that merely extracts lexical fragments from the original text of a long document can still generate a summary closer in resemblance to the reference summary.  Given that abstractive summarization models have recently been found to contain factual inconsistencies in up to 30% of the summary outputs in the short document domain [21-22] while extractive summarization model will faithfully preserve the original content, this insight encourages for the development of long document extractive models in the real-world production level settings.However, extractive approaches are inadequate to summarize a long scientific article  into a succinct and fluent summary, particularly when the salient contents are thinly scattered[23] and may overlap with each other, despite may subsequent attempts at reconstruction.

The recent success of large language models, such as GPT-3, has prompted a paradigm shift in Natural Language Processing research. Large language models (LLMs) provide a novel methodology for generating summaries. They leverage natural language task instructions and exhibit zero or few-shot performances in the prompt without the requirement for weight updates. LLMs demonstrate overwhelming preference by humans when compared to their fine-tuning pretrained model counterparts and remain unaffected by common dataset-specific problems like insufficient factual accuracy in news summarization.[24]and cross-lingual summarization[25].





## method

### overview

Existing methodologies and applications like, user-specific summaries [33] or presentation-based [34] summaries for scientific papers have not sufficiently addressed the trio of issues outlined earlier - namely, 1) extraction information loss  from scientific articles, 2) prevalent extractive approaches lacking in conciseness and fluency and 3) the incapacity to summarize lengthy scientific documents. 

Figure 1 illustrates the main workflow of the Arxiv summarizer.

<!-- TODO: Figure -->

![image-20231224180248031](img/image-20231224180248031.png)



### Setup

To contend with the challenges of PDF extraction, we elected to use Nougat as a useful util , given its proven superiority against other approaches and its outstanding performance across all metrics.[2] Specifically, we have opted for the Nougat small version due to its demonstrated equal efficacy compared to the larger base model, thereby optimizing the generation speed.

Furthermore, considering the inflexibility of large-scale fine-tuning pre-trained models like BERT, BART, and their variants requiring adaptation on target datasets, we have selected ChatGPT, particularly the black-box model GPT-3.5-turbo-16k-0613 [35], and GPT-4, OpenAI's latest and most advanced model [36].They are well-established Language Learning Models (LLMs) benchmarks offering comprehensive capabilities [37] and they have been shown to produce zero-shot summaries comparable to those crafted by freelance writers. [38].



<!-- TODO: Experiment of Basic indicators of papers in statistical data sets -->

==实验的论文统计，长度统计，不同section平均长度统计，方差等以及不同类型，例如cs，math等==

### Section-level Summarization

Nevertheless, when addressing long documents where the full text may even surpasses the maximum token limit of GPT-3.5-Turbo-16k, it becomes imperative to tactically partition the text. An intuition approach may involve trimming texts to ensure it falls within the maximum token limit or merely retaining the forepart and conclusion while diminishing the main body proportionately to meet the prescribed token limit.

Kryściński et al. [39] found substantial **layout bias** in the source text  where approximately 60% of critical sentences are found within the opening 30% of source articles, and argued that such layout bias does not apply to the other domains. This may suggest not every word in the raw content matters. However, [40] argues that the salient content is dispersed more uniformly throughout long documents than short ones. Hence, unlike short document summarization, where models often benefit from layout biases [41-43], long document models employing a truncation strategy for only a small subset of the leading content may experience considerable performance degradation.

Taking inspiration from FacetSum[44],a Faceted Summarization Dataset where each article is with a structured abstract that summarizes the article from distinct aspects including purpose, method, findings, and value, we also paragraph the article based on its subtitles(even based on the triple hash symbol if Double hash symbol is too long)  through keyword matching within the section titles. However, we have evolved from their methodology of adhering to four defined sections to segregating nearly every section. Each section (e.g. abstract, introduction, etc.) is then equipped with a matching prompt from our pre-prepared set, facilitating key point generation as a generic structure can often be discerned in scientific articles [41] and the primary function of every section is predominantly fixed . Sections having titles not included in the prompt set are assigned a generic prompt alternatively.

Moreover, as displayed in [40], the average token-level and sentence-level compression ratio of long-document summarization datasets surpasses their short-document counterparts by factors of 1.4 and 2.2 respectively. This discrepancy is attributable to the information loss, sparsely distributed salient content, and redundant information in source documents. Consequently, the generated summary inevitably neglects information that is considered important by some readers, which supports the efforts in controllable summarization. As the final summaries align with the reader's needs and expectancy [33,45], therefore, it is imperative to filter out sections of lesser interest (e.g., references or acknowledgements) in advance to speed up inference and eliminate unimportant interference from the summary.

<!-- TODO: Experiment of performance between section summary and full text input -->

==experiment needed==

Our experiment underscored the importance of section-level Summarization, as content selection mechanism is one of  the two most notable long document mechanisms[40]. The content selection mechanism requires a separate retriever to extract salient content from the source, where in our method, we use a simple way to retains most words the source document with almost all the section input except for the subtitle that contains user be indifferent about. This minimizes information loss during extraction and allows for a more fine-grained filtration by the GPT model to pinpoint key narratives.

The notable performance enhancement may also be attributed to the interesting observation connected with the hypothesize of "lost in the middle" phenomenon [46]. This results in LLMs exhibiting a 'U-shaped' performance curve. Specifically, LLMs are better at utilizing relevant information that occurs at the beginning or end of its input context window.



### prompt design

As evidenced in [46], model's performance remains robust towards any positional changes of relevant information within the input context when input length is below a specific threshold(i.e. the length of training-time context window), while longer input contexts result in a greater performance degradation when placing relevant information in the middle of the input context. These may stem from the current models still falling short when it involves reasoning over very long texts [47-48] and also encountering memory complexity issues and hardware limitations [49-50]. It infers that Transformer-based models are better optimized towards short document language tasks rather than long documents.

Considering that the average paragraph length of the sampled articles in our dataset is approximately 2048 tokens, the inclusion of more input text (for instance, inserting the full text) will significantly undermine summarization performance, perhaps due to performance issues when retrieving information located in the middle methoned above.

To shorten prompt and make the division of labor between different roles clearer, we divide prompts into three segments:

1. **Task Description**: This is the prompt of the "system" role and provides an overview of the task, with demands or background.
2. **Current Input**: In this part, the "user" role inputs reference text derived from the raw document content or previous generated summary, or even both, as external knowledge to better comprehend the entire text.
3. **Output Indicator**: Involves a specific workflow, mandating a guideline for GPT to follow.. As Chain of thought(CoT) has the advantage of robustness and a large margin outperform standard baseline[51] and Chain of Density(CoD) control of information density due to a summary is uninformative if it contains insufficient detail while if it contains too much information, however, it can become difficult to follow without having to increase the overall length. In the part, We combine these two methods organically. When making section summaries or generating documents in different formats (such as broadcasts, blogs, etc.), we use CoT because this helps GPT "think" better with Self-examination and improvement, and when merging section summaries we use CoD to aviod missing important information entity and increase the infomation density .

To ensure that important information is located either at the beginning or the end of a long input text, or to merely keep the prompt short for superior performance, we formulate concise and clear prompts in a well-structured manner, as illustrated examples of section summary, integrate summary, and answer enhancement in Figures -2  :

<!-- TODO: Figure  -->

```
section summary:
	[
	"system":[Task Description] + [Output Indicator]
		You excel in crafting academic paper abstracts...
	"user": [Current Input]
		Article fragment:{Article fragment}
	]
```

```
integrate summary:
	[
	"system":[Task Description]
		You excel at proficiently integrating multiple abstract sections within the same paper ...
	"system":[Output Indicator]
		To optimize the quality and structure of the abstract after integration, follow the three-step process outlined below ...
	"user": [Current Input]
		Summary Chunk:{{Titles} + {authors} + {summary chunk}}
	]
```



```
answer enhancement：
	[
	"system":[Task Description]
		`usage`: regenerate\ blog generation\ speech generation
		Based on the generated summary and original content,you are required to create {`usage`} ...
	"system":[Output Indicator]
		Please follow the steps below to improve the quality and clarity of {`usage`} ...
	"user": [Current Input]
		Raw Summary:{result of section summary}
		Generated Summary:{generated summary}
	]
```

Noted that summary chunk is essentially the result of section-level summarization, wherein all section-level summaries are concatenated.The generated summary constitutes either the result of integrated summary or a regeneration based on the integrated result.

The message content associated with each role strictly encompasses minimal length while maintaining distinct responsibilities, as opposed to fusing all functional aspects within a single role, e.g. includes the implementation of a rigid workflow in the portion of output indicator  with the current input, which references the text from the target document. Unfortunately, this causes the reference text to be positioned in the middle, a less than ideal situation that could potentially lead to content being overlooked by the model, thereby deteriorating its performance. It's noteworthy to mention that the inclusion of the title, authors, and affiliation information (filtered through NER technology) within the reference text contributes towards aligning the synthesized summary more closely to human-created ones.



## experiment

The efficient ROUGE metric [52] has been historically used as a standard measure to compare the performance of summarization models. By analysing the lexical overlap between the reference and candidate summaries, including unigram (ROUGE-1), bigram (ROUGE-2), and the longest common subsequence (ROUGE-L), it provides valuable insight. However, as it's reliant on exact token matches, its limitations include overlooking overlaps between synonymous tokens or phrases [53-56]. Such restrictions penalize models for 1) innovating distinctive wordings and phrases that don't mirror those found in the reference summary, 2) fails to consider the factual consistency plot between the model output and the source document. 3) does not account for the fluency and conciseness integral to a summary.

Notably, experiments by [40] testing various adaptations of a BERT textual entailment model for summarizing arXiv candidates found it ineffective in gauging a summary's factual consistency with the source. In contrast, utilizing GPT-4 as an evaluator has shown promising correlation with human judgments [57-58], and in certain instances, it has even outperformed group-sourced workers on specific annotation tasks [60]. Thus, we model our approach after [59], eliciting GPT-4 to rate CoD summaries (1-5) along 5 dimensions: Informative, Quality, Coherence, Attributable, and Overall with the following template:

<!-- TODO: Figure needed -->

```
Article: {{Article}} 
Summary: {{Summary}}
Please rate the summary (1=worst to 5=best) with respect to {{Dimension}}. 
{{Definition}}

• Informative: An informative summary captures the important information in the article and presents it accurately and concisely. 
• Quality: A high quality summary is comprehensible and understandable. 
• Coherence: A coherent summary is wellstructured and well-organized. 
• Attributable: Is all the information in the summary fully attributable to the Article? 
• Overall Preference: A good summary should convey the main ideas in the Article in a concise, logical, and coherent fashion.
```

their result demonstrates, unsurprisingly, that a prompt designed to capture overall summary rating has the highest summary-level Pearson correlation to overall preferences (0.311) . Other measured metrics have Pearson correlation values ranging from 0.120 to 0.245,yet overall correlations are still low.[59]. Thus,we utilize GPT-4 to evaluate summary quality metrics, as it can serve as a proxy for trending human preferences.

<!-- TODO: Experiment of summaries prefered by GPT4 with raw input pdf + GPT-4 & Arxiv summarizer + pdf input -->

<!-- TODO: metrics required: preference + entity density -->

As illustrated in Figure -3, the average entity density of summaries generated by our system is ==xxx==, which is comparable to manual summary results.As demonstrated in [59], a preferred entity density can be roughly infered of ∼ 0.15 across the CoD candidates which aligns with human-written summaries (0.151)

## conclusions & future

We have introduced 'ArXiv Summarizer', an innovative automatic summarization system based on Nougat and section-level summary, based on Nougat and section-level summary. This system successfully summarizes scientific articles in a PDF format, facilitating an interactive way to reduce time spent on full reviews and efficiently capturing the core ideas within. We've provided comprehensive analyses and results to explore the ArXiv Summarizer's distinct features and primary workflows. Looking ahead, our pursuit of system optimization will revolve around the following focal points:

1. **Incorporation of External Knowledge and User Interaction**: As an article's significance may vary over time, the summarization system should accommodate the perspectives of other researchers (citations)  and user interactive operations(e.g, user-generated questions about the article's congruity with similar studies) as external knowledge , as well as the major points underscored by the articles' authors in the abstract [61].
2. **Addressing the Problem of Infactualness**: At the current stage of development, abstract summaries generated by state-of-the-art models frequently contain content that bears factual inconsistency with the source document, posing a barrier for commercial applications [21~22].
3. **Absence of Progression**: A relative lack of research focusing on evaluation metrics, compared to practices in short-document contexts,  within the context of long document summarization may potentially hinder future advancements in this domain.
4. **Exclusive Dependence on Large Models**: Though chain-of-thought prompting generates larger performance gains for more complicated problems, it does not positively impact performance for smaller models.
5. **Sectional Intercommunication Shortfall**：Section-level summarization exhibits a lack of interaction between sections, thereby potentially hindering a holistic comprehension of the relationship among paragraphs. Future optimization could adopt a mechanism similar to that proposed by Transformer-XL[62], Longformer[63], or Talking-Head[64] to fuse information efficiently.



## reference

[1] Sebastian Spiegler. Statistics of the Common Crawl Corpus 2012, June 2013. URL https://docs.google.com/file/d/ 1 9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb uL5N9.

[2]Blecher L, Cucurull G, Scialom T, et al. Nougat: Neural optical understanding for academic documents[J]. arXiv preprint arXiv:2308.13418, 2023.

[3] R. Smith. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629–633, Curitiba, Parana, Brazil, September 2007. IEEE. ISBN 978-0-7695-2822-9. doi: 10.1109/ICDAR.2007.4376991. URL http://ieeexplore.ieee.org/document/4376991/. ISSN: 1520-5363.

[4] El-Kassas W S, Salama C R, Rafea A A, et al. Automatic text summarization: A comprehensive survey[J]. Expert systems with applications, 2021, 165: 113679.

[5] Teufel, Simone, Moens, Marc, 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Comput. Linguist. 28 (4), 409–445.

[6] Bhatia, Sumit, Mitra, Prasenjit, 2012. Summarizing figures, tables, and algorithms in scientific publications to augment search results. ACM Trans. Information Syst. (TOIS) 30 (1), 3.

[7] Altmami N I, Menai M E B. Automatic summarization of scientific articles: A survey[J]. Journal of King Saud University-Computer and Information Sciences, 2022, 34(4): 1011-1028.

[8] Yang, Shansong et al., 2016. Amplifying scientific paper’s abstract by leveraging data-weighted reconstruction. Inf. Process. Manage. 52 (4), 698–719.

[9] Reeve, Lawrence H., Han, Hyoil, Brooks, Ari D., 2007. The use of domain-specific concepts in biomedical text summarization. Inf. Process. Manage. 43 (6), 17651776.

[10] Elkiss, Aaron et al., 2008. Blind men and elephants: what do citation summar

[11] Yang, Shansong et al., 2016. Amplifying scientific paper’s abstract by leveraging data-weighted reconstruction. Inf. Process. Manage. 52 (4), 698–719.

[12] Slamet, Cepi et al., 2018. Automated Text Summarization for Indonesian Article Using Vector Space Model. In: IOP Conference Series: Materials Science and Engineering. IOP Publishing, p. 012037.

[13] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

[14]  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1–67.

[15] Lauscher, Anne, Glavaš, Goran, Eckert, Kai, 2017. University of Mannheim@ CLSciSumm-17: citation-based summarization of scientific articles using semantic textual similarity. In: CEUR Workshop Proceedings, pp. 33–42.

[16] Mihalcea, Ra.da., Tarau, Paul, 2004. Textrank: bringing order into text. Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.

[17] Agrawal, Kritika, Mittal, Aakash, Pudi, Vikram, 2019. Scalable, semi-supervised extraction of structured information from scientific literature. In: Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pp. 11–20.

[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

[19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1–67.

[20] Sascha Rothe, Joshua Maynez, and Shashi Narayan. 2021. A Thorough Evaluation of Task-Specific Pretraining for Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 140–145.

[21] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.

[22] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.

[23]  Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, and Dragomir Radev. 2021. An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings. (2021).

[24]Goyal T, Li J J, Durrett G. News summarization and evaluation in the era of gpt-3[J]. arXiv preprint arXiv:2209.12356, 2022.

[25] Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Crosslingual summarization via chatgpt. arXiv preprint arXiv:2302.14229.

[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).4171–4186.

[27] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328–11339.

[28] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

[29] Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 conference on empirical methods in natural language processing. 128–137.

[30] Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What Have We Achieved on Text Summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 446–469.

[31] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 4812–4829.

[32] Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 5093–5100.

[33] Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards generic controllable text summarization. arXiv preprint arXiv:2012.04281 (2020).

[34] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. 2021. D2S: Document-to-Slide Generation Via Query-Based Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1405–1418.

[35] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.

[36] OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.

[37] Min Z, Wang J. Exploring the integration of large language models into automatic speech recognition systems: An empirical study[C]//International Conference on Neural Information Processing. Singapore: Springer Nature Singapore, 2023: 69-84.

[38] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.

[39] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

[40] Koh H Y, Ju J, Liu M, et al. An empirical survey on long document summarization: Datasets, models, and metrics[J]. ACM computing surveys, 2022, 55(8): 1-35.

[41] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.

[42] Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A Deep Reinforced Model for Abstractive Summarization. In International Conference on Learning Representations.

[43] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1073–1083.

[44] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

[45] Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable Abstractive Dialogue Summarization with Sketch Supervision. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 5108–5122.

[46] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

[47] Potsawee Manakul and Mark Gales. 2021. Long-span summarization via local attention and content selection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 6026–6041.

[48] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

[49] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1419–1436.

[50] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.

[51] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.

[52] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74–81.

[53] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

[54] Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 643–653.

[55] Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying Human and Statistical Evaluation for Natural Language Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association or Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 1689–1701.

[56] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

[57] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.

[58] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

[59] Adams G, Fabbri A, Ladhak F, et al. From sparse to dense: GPT-4 summarization with chain of density prompting[J]. arXiv preprint arXiv:2309.04269, 2023.

[60] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056.

[61] Yasunaga, Michihiro et al., 2019. Scisummnet: a large annotated corpus and content-impact models for scientific paper summarization with citation networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7386–7393.

[62]  Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.

[63] Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.

[64] Shazeer N, Lan Z, Cheng Y, et al. Talking-heads attention[J]. arXiv preprint arXiv:2003.02436, 2020.



----



## old

[13] Lauscher, Anne, Glavaš, Goran, Eckert, Kai, 2017. University of Mannheim@ CLSciSumm-17: citation-based summarization of scientific articles using semantic textual similarity. In: CEUR Workshop Proceedings, pp. 33–42.

[14] Mihalcea, Ra.da., Tarau, Paul, 2004. Textrank: bringing order into text. Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.

[15] Agrawal, Kritika, Mittal, Aakash, Pudi, Vikram, 2019. Scalable, semi-supervised extraction of structured information from scientific literature. In: Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pp. 11–20.

[16] Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, and Dragomir Radev. 2021. An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings. (2021).

[17]Goyal T, Li J J, Durrett G. News summarization and evaluation in the era of gpt-3[J]. arXiv preprint arXiv:2209.12356, 2022.

[18] Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Crosslingual summarization via chatgpt. arXiv preprint arXiv:2302.14229.

[19]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).4171–4186.

[20] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328–11339.

[21] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

[22] Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 conference on empirical methods in natural language processing. 128–137.

[23] Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What Have We Achieved on Text Summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 446–469.

[24] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 4812–4829.

[25] Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 5093–5100.

## isold

is[1] :Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.

is[2] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.

is[3]: Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards generic controllable text summarization. arXiv preprint arXiv:2012.04281 (2020).

is[4] :Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable Abstractive Dialogue Summarization with Sketch Supervision. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 5108–5122. 

is[5] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

is[6] :Koh H Y, Ju J, Liu M, et al. An empirical survey on long document summarization: Datasets, models, and metrics[J]. ACM computing surveys, 2022, 55(8): 1-35.

is[7] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.

is[8] Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A Deep Reinforced Model for Abstractive Summarization. In International Conference on Learning Representations.

is[9] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1073–1083.

is[10] Yasunaga, Michihiro et al., 2019. Scisummnet: a large annotated corpus and content-impact models for scientific paper summarization with citation networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7386–7393.

is[11] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.

is[12] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.

is[13] Potsawee Manakul and Mark Gales. 2021. Long-span summarization via local attention and content selection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 6026–6041.

is[14] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

is[15] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1419–1436.

is[16] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.

is[17] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

is[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1–67.

is[19] Sascha Rothe, Joshua Maynez, and Shashi Narayan. 2021. A Thorough Evaluation of Task-Specific Pretraining for Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 140–145.

is[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).4171–4186.

is[21] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

is[22] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74–81.

is[23] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

is[24] Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 643–653.

Is[25] Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying Human and Statistical Evaluation for Natural Language Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association or Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 1689–1701.

Is[26] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

is[27] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.

is[28] Maxime Peyrard. 2019. A Simple Theoretical Model of Importance for Summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 1059–1073.

is[29] Priyam Tejaswin, Dhruv Naik, and Pengfei Liu. 2021. How well do you know your summarization datasets? arXiv preprint arXiv:2106.11388 (2021).

is[30] Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021. Improving Factual Consistency of Abstractive Summarization via Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online.

is[31] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.

is[32] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

is[33] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.

is[34] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

is[35] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056.

is[36] Adams G, Fabbri A, Ladhak F, et al. From sparse to dense: GPT-4 summarization with chain of density prompting[J]. arXiv preprint arXiv:2309.04269, 2023.

Is[37] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

is[38] Min Z, Wang J. Exploring the integration of large language models into automatic speech recognition systems: An empirical study[C]//International Conference on Neural Information Processing. Singapore: Springer Nature Singapore, 2023: 69-84.

is[39] OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.

is[40] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.

is[41] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.

is[42] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. 2021. D2S: Document-to-Slide Generation Via Query-Based Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1405–1418.

is[43] Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.

is[44] Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.

is[45] Shazeer N, Lan Z, Cheng Y, et al. Talking-heads attention[J]. arXiv preprint arXiv:2003.02436, 2020.



----

## draft

 Sebastian Spiegler. Statistics of the Common Crawl Corpus 2012, June 2013. URL https://docs.google.com/file/d/ 1 9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb uL5N9.

Blecher L, Cucurull G, Scialom T, et al. Nougat: Neural optical understanding for academic documents[J]. arXiv preprint arXiv:2308.13418, 2023.

 R. Smith. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629–633, Curitiba, Parana, Brazil, September 2007. IEEE. ISBN 978-0-7695-2822-9. doi: 10.1109/ICDAR.2007.4376991. URL http://ieeexplore.ieee.org/document/4376991/. ISSN: 1520-5363.

 El-Kassas W S, Salama C R, Rafea A A, et al. Automatic text summarization: A comprehensive survey[J]. Expert systems with applications, 2021, 165: 113679.

 Teufel, Simone, Moens, Marc, 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Comput. Linguist. 28 (4), 409–445.

 Bhatia, Sumit, Mitra, Prasenjit, 2012. Summarizing figures, tables, and algorithms in scientific publications to augment search results. ACM Trans. Information Syst. (TOIS) 30 (1), 3.

 Altmami N I, Menai M E B. Automatic summarization of scientific articles: A survey[J]. Journal of King Saud University-Computer and Information Sciences, 2022, 34(4): 1011-1028.

 Yang, Shansong et al., 2016. Amplifying scientific paper’s abstract by leveraging data-weighted reconstruction. Inf. Process. Manage. 52 (4), 698–719.

 Reeve, Lawrence H., Han, Hyoil, Brooks, Ari D., 2007. The use of domain-specific concepts in biomedical text summarization. Inf. Process. Manage. 43 (6), 17651776.

 Elkiss, Aaron et al., 2008. Blind men and elephants: what do citation summar

 Yang, Shansong et al., 2016. Amplifying scientific paper’s abstract by leveraging data-weighted reconstruction. Inf. Process. Manage. 52 (4), 698–719.

 Slamet, Cepi et al., 2018. Automated Text Summarization for Indonesian Article Using Vector Space Model. In: IOP Conference Series: Materials Science and Engineering. IOP Publishing, p. 012037.

 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1–67.

 Lauscher, Anne, Glavaš, Goran, Eckert, Kai, 2017. University of Mannheim@ CLSciSumm-17: citation-based summarization of scientific articles using semantic textual similarity. In: CEUR Workshop Proceedings, pp. 33–42.

 Mihalcea, Ra.da., Tarau, Paul, 2004. Textrank: bringing order into text. Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.

 Agrawal, Kritika, Mittal, Aakash, Pudi, Vikram, 2019. Scalable, semi-supervised extraction of structured information from scientific literature. In: Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pp. 11–20.

 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.

 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1–67.

 Sascha Rothe, Joshua Maynez, and Shashi Narayan. 2021. A Thorough Evaluation of Task-Specific Pretraining for Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 140–145.

 Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.

 Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.

  Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, and Dragomir Radev. 2021. An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings. (2021).

Goyal T, Li J J, Durrett G. News summarization and evaluation in the era of gpt-3[J]. arXiv preprint arXiv:2209.12356, 2022.

 Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Crosslingual summarization via chatgpt. arXiv preprint arXiv:2302.14229.

 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).4171–4186.

 Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328–11339.

 Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

 Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 conference on empirical methods in natural language processing. 128–137.

 Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What Have We Achieved on Text Summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 446–469.

 Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 4812–4829.

 Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 5093–5100.

 Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards generic controllable text summarization. arXiv preprint arXiv:2012.04281 (2020).

 Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. 2021. D2S: Document-to-Slide Generation Via Query-Based Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1405–1418.

 OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.

 OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.

 Min Z, Wang J. Exploring the integration of large language models into automatic speech recognition systems: An empirical study[C]//International Conference on Neural Information Processing. Singapore: Springer Nature Singapore, 2023: 69-84.

 Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.

 Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

 Koh H Y, Ju J, Liu M, et al. An empirical survey on long document summarization: Datasets, models, and metrics[J]. ACM computing surveys, 2022, 55(8): 1-35.

 Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.

 Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A Deep Reinforced Model for Abstractive Summarization. In International Conference on Learning Representations.

 Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1073–1083.

 Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

 Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable Abstractive Dialogue Summarization with Sketch Supervision. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 5108–5122.

 Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

 Potsawee Manakul and Mark Gales. 2021. Long-span summarization via local attention and content selection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 6026–6041.

 Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.

 Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1419–1436.

 Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.

 Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.

 Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74–81.

 Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).

 Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 643–653.

 Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying Human and Statistical Evaluation for Natural Language Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association or Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 1689–1701.

 Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.

 Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.

 Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

 Adams G, Fabbri A, Ladhak F, et al. From sparse to dense: GPT-4 summarization with chain of density prompting[J]. arXiv preprint arXiv:2309.04269, 2023.

 Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056.

 Yasunaga, Michihiro et al., 2019. Scisummnet: a large annotated corpus and content-impact models for scientific paper summarization with citation networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7386–7393.

  Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.

 Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.

 Shazeer N, Lan Z, Cheng Y, et al. Talking-heads attention[J]. arXiv preprint arXiv:2003.02436, 2020.