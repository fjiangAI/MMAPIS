

# progress

## 服务器

```
ssh clouduser@61.241.103.32 -p 20022
```

password：

```
C7#jH9u%##FreeAI@GZW%LLMzoo
```

```
cd workspace/arxiv_summarizer/proj
conda activate code_arxiv_summarizer
```

转移文件：

```bash
$ scp -P 20022  /d/linux/EasyChatGPT.py clouduser@61.241.103.32:/home/clouduser/workspace/arxiv_summarizer/proj/gpt_test.py
```

```python
$ scp -r -P 20022  /d/linux/EasyChatGPT.py clouduser@61.241.103.32:/home/clouduser/workspace/arxiv_summarizer/proj/gpt_test.py
```

使用清华园镜像快速下载：

```bash
$ python3 -m spacy download en_core_web_sm
-i https://pypi.tuna.tsinghua.edu.cn/simple
```



## 后端：

```
uvicorn backend:app --reload --port 8000
```

```
uvicorn backend:app --host 0.0.0.0 --port 8080 --reload
```

```
https://61.241.103.32/
```

可以

- [ ] API 多线程请求

  单线程

  | 线程数       | 时间/s                   | max_cpu |
  | ------------ | ------------------------ | ------- |
  | 1            | time: 57.278584480285645 | 16      |
  | 2(url)       | 62.95418906211853        | 16      |
  | 2(openai 库) | 59.546823501586914       | 16      |
  | 4(url)       | 50.7413694858551         | 16      |
  | 4(openai 库) | 58.5442636013031         | 16      |
  
  ```python
      def Multi_Chat_Processing(self,article_texts:Union[List[str],str],response_only:bool = True,resest_messages:bool = True):
          with multiprocessing.Pool(processes=self.num_processes) as pool:
              chat_func = partial(self.chat_with_openai,reset_messages=resest_messages,response_only=response_only)
              article_texts = [article_texts] if isinstance(article_texts,str) else article_texts
              results = [
                  pool.apply_async(chat_func, args=(article_text,))
                  for article_text in article_texts
              ]
              pool.close()
              pool.join()
              results = [p.get() for p in results]
          return results
  
  
      def chat_with_openai(self,
                           user_input:str,
                           reset_messages:bool = True,
                           response_only:bool = True):
         # input user prompt
          self.messages.append({'role': 'user', 'content': user_input})
          input_tokens = num_tokens_from_messages(self.messages, model=self.model)
          if input_tokens > self.max_tokens* self.prompt_factor:
             logging.warning(f'input tokens {input_tokens} is larger than max tokens {self.max_tokens* self.prompt_factor}, will cut the input')
             diff = int(input_tokens - self.max_tokens * self.prompt_factor)
             self.messages[-1]['content'] = self.messages[-1]['content'][:-diff]
         # use openai api
          response = openai.ChatCompletion.create(
              model= self.model,
              messages=self.messages,
              temperature= self.temperature,
              max_tokens= self.max_tokens-input_tokens,
              top_p= self.top_p,
              frequency_penalty= self.frequency_penalty,
              presence_penalty= self.presence_penalty
          )
            
            
      @staticmethod
      def init_openai_connect(host:str, api_key:str):
          os.environ["http_proxy"] = host
          os.environ["https_proxy"] = host
          openai.api_key = api_key
  ```

  推测原因是，使用多线程时,由于每个线程都有自己的变量空间，所以每个线程中的这些变量是相互独立的,不会共享主线程中的变量。api_key是在主线程中设置的,但在每个子线程中是没有定义的。所以当子线程试图调用OpenAI API时,没有找到api_key,因此报错。
  
  ```
  AuthenticationError: No API key provided. You can set your API key in code 
  using 'openai.api_key = <API-KEY>', or you can set the environment variable 
  OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point 
  the openai module at it with 'openai.api_key_path = <PATH>'. You can generate 
  API keys in the OpenAI web interface. See 
  https://platform.openai.com/account/api-keys for details.
  ```
  
  `https://api.chatanywhere.cn/v1/chat/completions`:
  
  ```
  ProxyError: HTTPSConnectionPool(host='api.chatanywhere.cn', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))
  ```
  
  因此暂时baseurl目前使用`https://api.openai.com/v1/chat/completions`



## 打包依赖库



```
pipreqs ./ --savepath ./requirements.txt  --encoding=utf8 
```

出现问题：

```
    return compile(source, filename, mode, flags,
  File "<unknown>", line 159
    parser.add_argument("--root", type=Path, default=None)``
```

传到服务器上，运行

```
(code_arxiv_summarizer) clouduser@clouduser:~/workspace/arxiv_summarizer$ pipreqs /home/clouduser/workspace/arxiv_summarizer/proj --force
```

也出现这个问题：

```

ERROR: Failed on file: 
/home/clouduser/workspace/arxiv_summarizer/proj/submodule/nougat_main/nougat/dataset/create_index.py
Traceback (most recent call last):
  File "/home/clouduser/.local/bin/pipreqs", line 8, in <module>
    sys.exit(main())
  File "/home/clouduser/.local/lib/python3.8/site-packages/pipreqs/pipreqs.py", line 528, in main
    init(args)
  File "/home/clouduser/.local/lib/python3.8/site-packages/pipreqs/pipreqs.py", line 455, in init
    candidates = get_all_imports(input_path,
  File "/home/clouduser/.local/lib/python3.8/site-packages/pipreqs/pipreqs.py", line 131, in get_all_imports
    raise exc
  File "/home/clouduser/.local/lib/python3.8/site-packages/pipreqs/pipreqs.py", line 117, in get_all_imports
    tree = ast.parse(contents)
  File "/usr/lib/python3.8/ast.py", line 47, in parse
    return compile(source, filename, mode, flags,
  File "<unknown>", line 159
    parser.add_argument("--root", type=Path, default=None)``
```

解决：

此前以为是库没装好，但是发现仍存在问题，`create_index.py`有问题，在`    parser.add_argument("--root", type=Path, default=None)`` `最后有连个`  `` `,导致出现问题

---

使用`pipreqs  .  --encoding=utf8 --force`发现导出的库不完整，但是使用`pip freeze`导出整个虚拟环境的库，太多

```
(pytorch) D:\Github\code_arxiv_summarizer(local)\code_arxiv_summarizer>pipreqs  .  --encoding=utf8 --force            
WARNING: Import named "fastapi" not found locally. Trying to resolve it at the PyPI server.
WARNING: Import named "fastapi" was resolved to "fastapi:0.104.0" package (https://pypi.org/project/fastapi/).
Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.
WARNING: Import named "fitz" not found locally. Trying to resolve it at the PyPI server.
WARNING: Import named "fitz" was resolved to "fitz:0.0.1.dev2" package (https://pypi.org/project/fitz/).
Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.
WARNING: Import named "fuzzysearch" not found locally. Trying to resolve it at the PyPI server.
WARNING: Import named "fuzzysearch" was resolved to "fuzzysearch:0.7.3" package (https://pypi.org/project/fuzzysearch/).

```

---

部分代码win跟linux不兼容，例如`winreg`库，用于获取windows基本信息，例如proxy，但是linux不兼容



## 环境适应

- [x] 部分代码需要代理翻墙访问api才能运行

  例如计算目前所使用token数量的函数：

  ```python
  encoding = tiktoken.encoding_for_model(model)
  ```

  以便决定messages在summary时所需的max_token以及当message所使用的token过长时，进行截断

  ```python
    File "/home/clouduser/.local/lib/python3.8/site-packages/requests/adapters.py", line 513, in send
      raise ProxyError(e, request=request)
  requests.exceptions.ProxyError: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b2d372460>: Failed to establish a new connection: [Errno 111] Connection refused')))
  ```

  - [x] 解决办法：

    手动下载所依赖的包，移至指定位置

- [ ] proxy，headers等都需要去掉，发现requests中有proxy就会访问不到，可以将proxies源头改为None

  

- [ ] 

  

  ## 编码方式

  [openai-cookbook/examples/How_to_count_tokens_with_tiktoken.ipynb at main · openai/openai-cookbook (github.com)](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)


    编码方式规定了如何将文本转换成token。不同的模型使用不同的编码方式。
    
    `tiktoken`支持OpenAI模型使用的三种编码方式：
    
    | 编码名称           | OpenAI模型                                       |
    |-------------------------|-----------------------------------------------------|
    | `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |
    | `p50k_base`             | Codex模型, `text-davinci-002`, `text-davinci-003`|
    | `r50k_base` (或 `gpt2`) | 像 `davinci` 这样的GPT-3模型                         |

- [x] 部分proxy，header等超参数是否需要调整？

proxy，header不需要，通过连接华佗链接



# problems

- [ ] 多文件下载

- [x] abstract的标题级数不为6的情况

- [x] save成mmd时的格式问题：

  <u>当用的是原论文中的subtitle时（markdown格式，会带#）在summary.mmd中会出现## ## subtitle的情况，而使用section summary中的key时，与## 结合，没有多余的#</u>）：

  ```python
  for i,chunk in enumerate(chunks):
       subtitle,summary_prompt = assgin_prompts(prompts,chunk[0])
       input_prompt = summary_prompt + chunk[1]
       resp = chat_with_openai(input_prompt,messages,model,temperature,max_tokens,top_p,
                               frequency_penalty,presence_penalty,response_only=True,prompt_factor=prompt_factor)
  
       respons.append('\n## ' + subtitle + ':\n' + resp)
  ```

- [ ] streamlit服务器问题：

  当服务器闲置较久时，有些时候会出现：

  ```
  AxiosError: Request failed with status code 403
  ```

- [ ] prompts问题

  修改prompt方向：

  - 1-shot / few-shot
  - 复杂的prompt
    - 任务描述
    - 标准制定
    - 操作对象输入

- [ ] 鲁棒性问题：

  使用my_utils中的decorator以解决报错直接中止的问题，并想出给用户的提示方案

- [ ] 技术架构，思路（流程图（内部，时间流）/架构图（宏观，模块联系））

- [ ] github完善（多余代码删除，使用文档攥写）

- [ ] 团队项目，了解

- [ ] GPT --> Instruct GPT（turbo - instruct-16k）,请求 --> 要求



cur:

- [ ] github --> org
- [ ] prompt  优化 COT
  - 参考链接：
    1. [efficient prompts](https://mp.weixin.qq.com/s/t4q_DC9CgZhyA5MKOT8ptA)
    2. [一文总结提示工程框架，除了CoT还有ToT、GoT、AoT、SoT、PoT](https://mp.weixin.qq.com/s/7XunMdZNHkII-6sRgetegw)
    3. [4k窗口长度就能读长文](https://mp.weixin.qq.com/s/4YLN6tL61iIorzUsUMALQQ)
    4. [哈工大发布大模型思维链推理综述](https://mp.weixin.qq.com/s/tDTJjRpLsPVnEi_LaPPrxQ)
- [ ] FastAPI部署测试，解耦
- [ ] 代码整理，前端优化
- [ ] instrcut GPT 模型修改
- [ ] api返回体再学习一下
