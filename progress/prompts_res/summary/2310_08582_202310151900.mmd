
## abstract:
The main objective of the paper is to address the inefficiencies of the prevalent paradigm of close-loop task planning using Large Language Models (LLMs). The paper proposes a new approach called Tree-Planner, which consists of three phases: plan sampling, action tree construction, and grounded deciding. In the plan sampling phase, an LLM generates a set of potential plans. These plans are then aggregated to form an action tree in the action tree construction phase. Finally, in the grounded deciding phase, the LLM makes decisions on the action tree based on real-time environmental information. 

The research findings show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, the token consumption is reduced by 92.2% compared to the previous best-performing model. Additionally, the flexibility of the correction process is improved, resulting in a 40.5% decrease in error corrections. The proposed approach opens up opportunities for large-scale testing and applications.
## intro:
The main motivation of the authors is to address the token and correction inefficiencies in the existing Iterative-Planner approach for task planning using Large Language Models (LLMs). They propose a new approach called Tree-Planner that divides the queries to an LLM into two parts: plan sampling and grounded deciding. This allows for more efficient token usage and logical correction. The authors demonstrate the effectiveness of Tree-Planner in the VirtualHome environment, achieving state-of-the-art results in terms of success rate and showing significant improvements in token and correction efficiency compared to existing methods. They also conduct formal verification, ablation studies, and error analysis to further evaluate and improve the proposed model.
## ## 2 Preliminary:
In this section, the authors introduce Task and Motion Planning (TAMP), which is the process of generating a sequence of actions and robot motions to achieve a desired goal in a given environment. The authors explain that a high-level task description can be decomposed into several mid-level actions. They assume the existence of a low-level controller that can execute these mid-level actions. Task planning can be categorized into closed-loop task planning and open-loop task planning. Closed-loop task planning involves adjusting planning strategies through perception and feedback mechanisms to adapt to environmental changes and uncertainties during execution. The authors focus on closed-loop task planning, which is more suitable for task execution in dynamic and complex environments.

The authors formulate the closed-loop task planning problem as a partially observable Markov decision process (POMDP), which includes sets of states, observations, and actions. The transition model represents the probability of transitioning to the next state given the current state and action. In a POMDP setting, the observation represents a subset of the underlying state. The authors mention that the optimal policy must take into account not only the current observation but also the entire history of actions.
## model:
The authors of this research propose a theoretical model called Tree-Planner for task planning in robots. The model integrates large language models (LLMs) into task planning by prompting the LLM to generate prospective task plans before executing them in a specific environment. The authors chose this method to extract commonsense knowledge from the LLM through plan sampling, which serves as prior knowledge for task planning.

The process of plan sampling involves using a prompt consisting of instruction, global information, initial observation, and in-context examples to generate a set of plan candidates. Each plan candidate is a sequence of actions, and the number of sampled plans is determined by a hyper-parameter. 

To select an optimal plan from the potential plans, the authors designed a structured representation called Action Tree. The Action Tree aggregates the shared parts of plans into a single branch while representing the differing actions as divergent paths. This reduces the execution time in the environment by converting the filtering of plans into a search at the action level.

During grounded deciding, the LLM functions as a policy and chooses an action from the child nodes of the current node on the Action Tree. The LLM is provided with an instruction, observation, and history as prompts to simulate human decision-making. If a chosen action fails to execute, the model performs error correction by marking invalid nodes, tracing back on the Action Tree to find valid fork nodes, and executing the inverse process of previously executed actions to recover the agent's state. Error correction with grounded deciding is more effective and efficient compared to other methods.

The authors chose this approach because it leverages the power of LLMs to encode commonsense knowledge and provides a structured representation that reduces execution time and allows for error correction. The effectiveness of the approach is analyzed quantitatively in the research.
## result:
The main results of the research are as follows:

1. The Tree-Planner method outperforms the listed baseline systems, achieving an absolute improvement of 11.2% and 7.04% in executability, 6.71% and 7.29% in goal conditions recall (GCR), and 1.29% and 3.65% in success rate (SR) under both experimental settings, respectively. This shows that reframing the language model-based (LLM-based) planning pipeline does not compromise its performance.

2. Tree-Planner demonstrates a significant advantage in token efficiency. In the "without correction" setting, it reduces the cost of the Iterative-Planner method by 53.29%. In the "with correction" setting, it further reduces the token cost by 74.36% and 92.24% compared to the Local Replan and Global Replan methods, respectively.

3. Tree-Planner also shows high correction efficiency, resulting in a reduction of the number of action-retry times for Local Replan and Global Replan by 37.99% and 40.52%, respectively. This decrease in corrections contributes to a decrease in token consumption.

Additionally, it is noted that Tree-Planner significantly outperforms Zero-shot Planner and ProgPrompt in terms of performance, achieving a 27.26% and 15.79% improvement in SR, respectively. The impact of increasing the hyper-parameter \(N\) is also discussed in Section 5.2.
## ## 5 Analysis:
In this section, titled "Analysis," the paper discusses token efficiency, plan sampling, grounded deciding, and error analysis. 

Regarding token efficiency, the paper compares the token consumption of Tree-Planner and Iterative-Planner. It explains the formula for calculating the number of tokens required for each method and determines the boundary conditions for the hyperparameter N that satisfy the inequality where Tree-Planner consumes fewer tokens than Iterative-Planner. 

The paper then moves on to discuss plan sampling, which is an important aspect of Tree-Planner. It introduces two metrics to study the upper limit of plan sampling: the maximum GCR and the average GCR. The paper analyzes the results and draws conclusions about the effectiveness of plan sampling and the impact of the value of N on the performance of Tree-Planner. 

Next, the paper focuses on grounded deciding and conducts ablation experiments to investigate its effectiveness. It shows that incorporating the gold plan into the construction of the action tree significantly improves performance and reduces the number of error corrections. The paper also discusses the impact of the number of plans on performance.

Finally, the paper presents an error analysis, categorizing error types into missing correct plans, deciding errors, and false negatives. It discusses the percentage of errors attributed to each type and suggests potential future improvements to address these errors.

Overall, this section provides a detailed analysis of token efficiency, plan sampling, grounded deciding, and error analysis in the context of Tree-Planner.
## related work:
Previous research in the task planning domain has been categorized into search-based methods and generate-based methods. Some search-based methods include the works of Jiang et al. (2018) and Garrett et al. (2018), while generate-based methods include the works of Song et al. (2023), Wu et al. (2023), Ding et al. (2023), and Mu et al. (2023).

LLMs trained on large-scale corpora have provided a wealth of commonsense knowledge for task planning, leading to an increased focus on generate-based methods. Some works directly generate the entire plan without executing it in the environment, such as the models proposed by Singh et al. (2022), Liang et al. (2023), Wu et al. (2023), Zeng et al. (2023), Lin et al. (2023), and Yang et al. (2023). However, these models lack the ability to dynamically modify the plan in the face of errors.

Another line of research involves generating actions iteratively, allowing for more flexibility in error correction, human interaction, and environment grounding. Works like Vemprala et al. (2023), Yao et al. (2022), Huang et al. (2022), and Shinn et al. (2023) adopt this paradigm.

Some studies, such as Carta et al. (2023), Huang et al. (2023), and Ahn et al. (2022), use implicit representations of LLMs. In contrast, this paper focuses on Black-box LLMs, which are commonly used in research and industry and provide only input and output without additional information.

In the area of tree-based modeling, Yao et al. (2023) and Long (2023) propose the use of "tree-of-thought" for problem-solving but focus on reasoning tasks rather than the interaction between inner steps and the environment. Cao and Lee (2023) leverage LLMs for automatic behavior-tree-based task generation in the context of robotics. Zhao et al. (2023) and Hao et al. (2023) propose using LLMs as world models to assist planning algorithms like Monte Carlo Tree Search (MCTS). However, the approach presented in this paper, called Tree-Planner, samples diverse paths and aggregates them into an action tree, offering advantages in efficiency.

The paper follows the "generate then select" paradigm, where a prediction is selected from the sampled potential plans. Some models use external controllers or incorporate environmental state information to re-rank generated plans. In contrast, Tree-Planner performs fine-grained selection at the action level using action trees.

Efficient inference with LLMs has been achieved through various modifications to transformer architectures or decoding strategies. Some studies propose batch prompting methods or incorporate small LMs fine-tuned on oracle trajectories. Tree-Planner differs from these studies by reframing the planning process of LLMs to reduce repeated token consumption without additional training.
## conclusion:
The main conclusions and findings mentioned in the conclusion section of the paper are:
1. Tree-Planner, a novel framework for task planning with LLMs, addresses the inefficiencies of the commonly adopted paradigm while achieving high performance.
2. Through experiments in the VirtualHome environment, Tree-Planner outperforms other strong baselines and achieves state-of-the-art performance.
3. The framework is highly efficient in terms of token consumption and error correction.
4. Studies have been conducted to analyze the performance gains of Tree-Planner and identify potential bottlenecks.
5. Qualitative error analysis has been performed to identify areas where the model may fail.
The authors suggest further research and development of more efficient task-planning methods inspired by Tree-Planner.
## ## 8 Acknowledgements:
The section titled "Acknowledgements" expresses gratitude towards Tianbao Xie for providing helpful feedback on the work.