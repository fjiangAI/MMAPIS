
## abstract:
The main objective of the paper is to develop a domain-specific knowledge graph (KG) for cancer research, which can be used to assist in biomarker discovery and provide interactive question answering. The authors address the challenge of integrating data and knowledge from structured and unstructured sources to create a large-scale KG. They propose the use of a domain ontology called OncoNet Ontology (ONQ) for semantic reasoning in validating gene-disease relations. The KG is enriched with information extracted from scientific articles using BioBERT- and SciBERT-based information extraction methods. Additionally, the authors highlight the importance of incorporating up-to-date findings to prevent concept drift in AI systems. They achieve this by finetuning the KG using large language models based on recent articles and knowledge bases. The paper contributes to the fields of bioinformatics, ontology, knowledge graphs, machine learning, and large language models.
## intro:
The main motivation of the authors is to address the challenging scenario of providing improved diagnosis and treatment for cancer. They emphasize the importance of early detection, personalized interventions, and identifying biomarkers and therapeutic targets in cancer research. The authors identify machine learning techniques, particularly in the analysis of multimodal data, as valuable tools for cancer research. They also highlight the challenges in integrating and extracting knowledge from unstructured data sources, such as scientific literature, and the need for information extraction methods in natural language processing. The authors mention semantic heterogeneity and the use of Semantic Web technologies, such as knowledge graphs, to address data variety. They emphasize the importance of domain ontologies and knowledge bases in supporting multidisciplinary cancer research and clinical decision support. Finally, the authors discuss the challenges in exploring, processing, and analyzing large-scale knowledge graphs and highlight the importance of using up-to-date domain knowledge in AI systems.
## related work:
Previous research in the field of life sciences has focused on using semantic technologies such as knowledge bases (KBs) and ontologies to build structured networks of interconnected knowledge. Specifically, research efforts have been made to extract semantic knowledge about cancer from various types of data and construct large-scale knowledge graphs (KGs). Examples of prominent KGs in the field include Bio2RDF and PubMed KGs. Additionally, a biomedical KG has been developed by harmonizing gene ontology (GO), human phenotype ontology (HPO), and disease ontology (DO) and applying representation learning techniques.

Some specific examples of KGs developed for cancer research include a prototype KG based on the Louisiana Tumor Registry, which provides scenario-specific querying and data visualization capabilities but is limited in terms of comprehensive knowledge about cancer types. Another example is a large-scale breast cancer (BRCA) KG that integrates Dutch medical guidelines, genomic data, clinical trials, semantic annotations, and medical publications.

Despite these efforts, there is still a lack of comprehensive KGs targeting multiple cancer types. Additionally, the adoption of data-driven approaches in clinical settings has been hindered by the lack of scalable computational methods and the need for reliable and up-to-date information. The field of explainable artificial intelligence (XAI) aims to address the transparency and understandability of AI systems, especially in healthcare applications. Interpretable machine learning (ML) models have emerged to reveal the factors that impact their outcomes.

In terms of natural language processing (NLP), querying or information extraction from structured data is relatively straightforward, but the same tasks from unstructured data, such as text, may require domain-specific and efficient NLP methods. For example, identifying cancer-specific biomarkers or understanding the functionality of genes involved in cancer requires advanced NLP techniques.

Transformer language models (TLMs), such as BERT, have become the standard for representation learning in NLP. They utilize bidirectional attention mechanisms and large-scale unsupervised corpora to obtain context-sensitive representations of words. TLMs have shown promise in bio-entity extraction and other NLP tasks in the scientific domain. In contrast, traditional approaches such as conditional random fields (CRFs) or neural networks (DNNs) have lower generalization performance.

Finally, large language models (LLMs), such as GPT-4, LaMDA, and PaLM, are expensive to train or fine-tune but have the potential to learn knowledge from large text corpora and integrate that knowledge into downstream tasks through prompts.
## method:
In the methods section of the paper, several research methods and techniques are applied. The main steps of data collection and analysis, as described in the methods section, are outlined as follows:

1. Construction of ONO Ontology: The researchers first construct the ONO (Ontology of Neoplastic Diseases) ontology. This ontology serves as a framework for organizing and representing cancer-related knowledge.

2. Enrichment of Knowledge Graph (KG): The ONO ontology, along with additional controlled vocabularies and facts obtained from scientific articles, is used to enrich the KG. This process involves integrating and linking cancer-related knowledge from multiple sources.

3. KG Construction: The researchers construct the KG by integrating cancer-related knowledge from different sources. This step involves collecting and organizing data from various databases, articles, and other relevant sources.

4. Fine-tuning the KG using Language Models (LLMs): The KG is further enhanced and refined using LLMs. This process, as depicted in Figure 2 of the paper, involves leveraging the power of language models to improve the quality and accuracy of the KG.

Overall, the research methods and techniques applied include ontology construction, enrichment of the knowledge graph, integration of cancer-related knowledge, and the use of language models for fine-tuning the KG.
## model:
The authors of the research developed the Oncogenomics Knowledge Graph (ONO) by reusing existing ontologies that contain annotations about diseases, genes, concepts, and biological processes. The ONO is based on the Cancer, Biomarker, and Feature classes, which serve as controlled vocabularies and facts for the knowledge graph.

The Cancer class is a conceptual term that includes 33 types of cancer. The Biomarker class is a conceptual term that contains 660 genes responsible for different types of cancer. Genes can be responsible for single or multiple types of cancer, and they are characterized by types, evidence sources, significance levels, cross-responsibilities, and citations. The Feature class provides additional information about genes, such as biomarker types, significance levels, and evidence sources.

To inherit metadata about genes, genomes, and diseases, the authors used the ontology of genes and genomes (OGG) and the human disease ontology (DOID). They also incorporated semantic concepts from various biomedical ontologies to capture the connections between biomarkers and cancer types.

The authors performed instance mapping by assigning unique identifiers to entities in the knowledge graph. Biological entities, such as genes, proteins, and diseases, were treated as instances, while classes from the DOID were also treated as instances. The ontology structure included hierarchical relations between classes and subclasses, which allowed for ontology-based annotations.

The knowledge graph was enriched through the integration of additional facts from recent articles. Cancer-specific articles were collected from PubMed and processed using BERT-based information extraction. Named entity recognition (NER) and entity linking (EL) techniques were employed to extract and link named entities from the articles. BioBERT and SciBERT models were fine-tuned for NER using biomedical texts.

The extracted entities were linked to nodes in the knowledge graph, and entity disambiguation and multi-type normalization were performed. Relation extraction was also carried out, considering both binary and n-ary relation types between genes/proteins and diseases. The extracted triples, including information about significance, evidence type, and relationships, were integrated into the knowledge graph.

The authors utilized semantic lexicons and rules from the ontology to enrich the instance information and create RDF triples. The context of the terms in the source text was analyzed for appropriate disambiguation, and attribute-value pairs were identified to form connected components of sentences.

Overall, the authors chose this approach of developing the ONO by reusing existing ontologies and integrating information from various biomedical sources to create a comprehensive knowledge graph for oncogenomics. This approach allows for the representation of complex relationships between genes, biomarkers, diseases, and evidence sources, facilitating complex question-answering and reasoning tasks.
## ### Fine-tuning KG with LLMs:
In this section, the authors discuss the use of Language Models (LLMs) for fine-tuning Knowledge Graphs (KGs). They highlight the potential inconsistency or outdated information in KGs and propose using LLMs to extract up-to-date knowledge and identify inconsistencies. They employ the supervised fine-tuning approach, specifically the instruction-tuning approach, for training LLMs.

Instead of training LLMs from scratch, the authors leverage their capabilities in Information Extraction (IE) with in-context learning in prompts. They provide the LLM with the ontology, KG, and recent reference articles that were not used during the fine-tuning of other models like BioBERT or SciBERT. The goal is to generate triples, guided by the ontology, from input texts. These generated triples are then compared with existing triples in the KG.

For a given input prompt, the authors instruct the ChatGPT model to generate a response by optimizing the likelihood of the response given the input prompt. The response is represented as RDF triples for the KG. Finally, the authors perform automated validation of the facts using a semantic reasoner.

Footnote 14 refers to the full-text corpora with guided URLs and prefixes used in the process.
## ### Quality assessment of knowledge graph:
The assigned section focuses on the quality assessment of a knowledge graph (KG). When constructing a KG, it is common for the initial data to be incomplete and contain errors. This is especially true when multiple sources are used to construct the KG. After constructing the KG, it is important to assess the quality of the facts within it. The assessment involves evaluating the fitness of the facts based on dimensions like availability, completeness, conciseness, interlinking, performance, and relevancy. The SANSA-linked data quality assessment metrics are used for this assessment.
## result:
The main findings of the research are not mentioned in the given text. However, the discussion section may provide insights on how the authors interpret their results, how these results compare to previous research, and the implications they have.
## experiment:
The main experimental techniques or procedures employed in the research include:

1. Fine-tuning each _BERT_-based NER (Named Entity Recognition) model for 20 epochs with a maximum input length of 256 and shuffled training set for each epoch.
2. Using the Adam optimizer to optimize the loss with a scheduled learning rate, starting at \(2e^{-5}\) and applying gradient clipping.
3. Reporting results from random search and 5-fold cross-validation tests.
4. Comparing the domain-specific potentials of the transformer-based approach with CRF (Conditional Random Fields) and Bi-LSTM (Bidirectional Long Short-Term Memory) architectures for information extraction.
5. Providing benchmark queries in both human language (NLQ) and DLx query (DLQ) formats, where DLx rules are generated using Pellet, ELK, and HermiT reasoners.
6. Using Protege 5.5.0 for reasoning with DLx querying, but reporting rules generated with Pellet reasoner only.
7. Expressing inferred rules based on reasoning in DLx format.
8. Interpreting generated rules, followed by decision reasoning with the rules.
9. Using Figure 4 as an example prompt to generate triples from the given text and ontology as input to ChatGPT.
## ### Analysis of information extraction:
This section discusses the analysis of information extraction using exact-match evaluation and the performance of the BioBERT and SciBERT models in recognizing named entities in scientific articles. The evaluation is based on entity-level precision and recall, with precision measuring the accuracy of the NER system in presenting correct entities and recall measuring the ability of the model to recognize all entities in the corpus. The results, presented in table 1, include precision, recall, and F1 scores, with the highest scores in bold and the second-best scores underlined. The performance of the BERT and BiLSTM-CRF models is provided as baselines.

BioBERT achieved the highest F1-score in recognizing Genes/Proteins and diseases, surpassing SciBERT by about 2%. It also outperformed the BiLSTM-CRF model by 7.85% in terms of F1-score. SciBERT, being fine-tuned on domain-specific articles, outperformed the BiLSTM-CRF model by 5.39% on average. BERT, pre-trained on general domain corpus, was highly effective. On average, BioBERT and SciBERT outperformed BERT by 3.25% in terms of F1-score.
## ### Using the knowledge graph:
This section discusses the use of a knowledge graph (KG) to bridge the gap between humans and AI systems. Explanations are seen as a way for the AI to benefit from external knowledge and support domain experts in understanding the results of algorithms. The authors highlight the potential benefits for researchers and medical doctors to use the KG for interactive explanations and validation of diagnosis decisions. They provide an example of how a natural language query can be answered using the KG to retrieve information. The authors also mention the use of symbolic techniques for reasoning over the KG, which can help in question answering and logical reasoning. The section includes a footnote explaining the use of inference rules and an HL encoding IF-THEN-style consequences. Additionally, the authors discuss the integration of an ML model with a knowledge-based system, which would allow human operators to reason, ask questions, and validate predictions. They mention that a doctor with expertise, combined with information from the KG, could provide additional interpretation and explanation of the decision. The section ends with a footnote mentioning the concept of neuro-symbolic AI, which combines connectionist and symbolic AI paradigms.
## conclusion:
The main conclusions and findings mentioned in the conclusion section of the paper are as follows:

1. The authors constructed a domain knowledge graph (KG) for cancer-specific biomarker discovery and interactive question-answering (QA) purposes.
2. The domain ontology (ONO) was developed to enable semantic reasoning for the validation of gene-disease relations in symbolic contexts.
3. The KG was fine-tuned using language models (LLMs) based on more recent articles and knowledge bases, which helped extract new facts and knowledge from scientific literature and keep the KG up-to-date.

The authors also mention potential limitations of their approach and suggest further directions for research, including:

1. Testing the expressiveness of DLx against incompleteness and inconsistency.
2. Using the fully inferred, deductively closed KG for representation learning, such as embeddings of nodes and relations.
3. Developing a neuro-symbolic AI system that integrates both domain and human expertise, combining connectionist and symbolic AI paradigms for knowledge acquisition, exploration, reasoning, and explainability.
## ## Acknowledgments:
The Acknowledgments section of the paper mentions that the concepts discussed in the paper are based on the PhD thesis of the first author. It also states that the second author is being funded by the Federal Ministry of Education and Research under a specific grant.